


<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="shortcut icon" href="../img/radtorch_icon.ico">
      <meta name="generator" content="mkdocs-1.1, mkdocs-material-5.1.0">
    
    
      
        <title>radtorch.pipeline - RADTorch - API Documentation</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.89dc9fe3.min.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/palette.ecd4686e.min.css">
      
      
        
        
        <meta name="theme-color" content="">
      
    
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    
      
        
<link rel="preconnect dns-prefetch" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-116382803-2","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}),document.addEventListener("DOMContentSwitch",function(){ga("send","pageview")})</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
  
    
    
    <body dir="ltr" data-md-color-primary="black" data-md-color-accent="deep-orange">
  
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#pipeline-module-radtorchpipeline" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href=".." title="RADTorch - API Documentation" class="md-header-nav__button md-logo" aria-label="RADTorch - API Documentation">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18,22A2,2 0 0,0 20,20V4C20,2.89 19.1,2 18,2H12V9L9.5,7.5L7,9V2H6A2,2 0 0,0 4,4V20A2,2 0 0,0 6,22H18Z" /></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3,6H21V8H3V6M3,11H21V13H3V11M3,16H21V18H3V16Z" /></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            RADTorch - API Documentation
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              radtorch.pipeline
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5,3A6.5,6.5 0 0,1 16,9.5C16,11.11 15.41,12.59 14.44,13.73L14.71,14H15.5L20.5,19L19,20.5L14,15.5V14.71L13.73,14.44C12.59,15.41 11.11,16 9.5,16A6.5,6.5 0 0,1 3,9.5A6.5,6.5 0 0,1 9.5,3M9.5,5C7,5 5,7 5,9.5C5,12 7,14 9.5,14C12,14 14,12 14,9.5C14,7 12,5 9.5,5Z" /></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5,3A6.5,6.5 0 0,1 16,9.5C16,11.11 15.41,12.59 14.44,13.73L14.71,14H15.5L20.5,19L19,20.5L14,15.5V14.71L13.73,14.44C12.59,15.41 11.11,16 9.5,16A6.5,6.5 0 0,1 3,9.5A6.5,6.5 0 0,1 9.5,3M9.5,5C7,5 5,7 5,9.5C5,12 7,14 9.5,14C12,14 14,12 14,9.5C14,7 12,5 9.5,5Z" /></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19,6.41L17.59,5L12,10.59L6.41,5L5,6.41L10.59,12L5,17.59L6.41,19L12,13.41L17.59,19L19,17.59L13.41,12L19,6.41Z" /></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/radtorch/radtorch/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    radtorch/radtorch
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
        
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="RADTorch - API Documentation" class="md-nav__button md-logo" aria-label="RADTorch - API Documentation">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18,22A2,2 0 0,0 20,20V4C20,2.89 19.1,2 18,2H12V9L9.5,7.5L7,9V2H6A2,2 0 0,0 4,4V20A2,2 0 0,0 6,22H18Z" /></svg>

    </a>
    RADTorch - API Documentation
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/radtorch/radtorch/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    radtorch/radtorch
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../start/" title="Getting Started" class="md-nav__link">
      Getting Started
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../install/" title="Installation" class="md-nav__link">
      Installation
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../core/" title="radtorch.core" class="md-nav__link">
      radtorch.core
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        radtorch.pipeline
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3,9H17V7H3V9M3,13H17V11H3V13M3,17H17V15H3V17M19,17H21V15H19V17M19,7V9H21V7H19M19,13H21V11H19V13Z" /></svg>
        </span>
      </label>
    
    <a href="./" title="radtorch.pipeline" class="md-nav__link md-nav__link--active">
      radtorch.pipeline
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#image_classification" class="md-nav__link">
    Image_Classification
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gan" class="md-nav__link">
    GAN
  </a>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../involve/" title="Get Involved" class="md-nav__link">
      Get Involved
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../copyright/" title="Copyrights" class="md-nav__link">
      Copyrights
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../license/" title="License" class="md-nav__link">
      License
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#image_classification" class="md-nav__link">
    Image_Classification
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gan" class="md-nav__link">
    GAN
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  
                
                
                <h1 id="pipeline-module-radtorchpipeline">Pipeline Module <small> radtorch.pipeline </small></h1>
<p style='text-align: justify;'>
Pipelines are probably the most exciting feature of RADTorch Framework. With only few lines of code, the pipeline module allows you to run state-of-the-art machine learning algorithms and much more.
</p>

<pre><code>from radtorch import pipeline
</code></pre>
<p style='text-align: justify;'>
RADTorch follows principles of <b>object-oriented-programming</b> (OOP) in the sense that RADTorch pipelines are made of core building blocks and each of these blocks has specific functions/methods that can be accessed accordingly.
</p>

<p style='text-align: justify;'>

For example,
</p>

<pre><code>pipeline.Image_Classification.data_processor.dataset_info()
</code></pre>
<p style='text-align: justify;'>
can be used to access the dataset information for that particular Image Classification pipeline.
</p>

<h2 id="image_classification">Image_Classification</h2>
<div class="highlight"><pre><span></span><code>pipeline.Image_Classification(
              data_directory, is_dicom=False, table=None,
              image_path_column=&#39;IMAGE_PATH&#39;, image_label_column=&#39;IMAGE_LABEL&#39;,
              is_path=True, mode=&#39;RAW&#39;, wl=None, balance_class=False,
              balance_class_method=&#39;upsample&#39;, interaction_terms=False,
              normalize=((0,0,0), (1,1,1)), batch_size=16, num_workers=0,
              sampling=1.0, test_percent=0.2, valid_percent=0.2, custom_resize=False,
              model_arch=&#39;alexnet&#39;, pre_trained=True, unfreeze=False,
              type=&#39;nn_classifier&#39;, cv=True, stratified=True, num_splits=5, parameters={},
              learning_rate=0.0001, epochs=10, optimizer=&#39;Adam&#39;,
              loss_function=&#39;CrossEntropyLoss&#39;,lr_scheduler=None,
              custom_nn_classifier=None, loss_function_parameters={},
              optimizer_parameters={}, transformations=&#39;default&#39;,
              extra_transformations=None, device=&#39;auto&#39;,)
</code></pre></div>

<div class="admonition quote">
<p><strong>Description</strong></p>
<p>Complete end-to-end image classification pipeline.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p>data_directory (string, required): path to target data directory/folder.</p>
</li>
<li>
<p>is_dicom (bollean, optional): True if images are DICOM. default=False.</p>
</li>
<li>
<p>table (string or pandas dataframe, optional): path to label table csv or name of pandas data table. default=None.</p>
</li>
<li>
<p>image_path_column (string, optional): name of column that has image path/image file name. default='IMAGE_PATH'.</p>
</li>
<li>
<p>image_label_column (string, optional): name of column that has image label. default='IMAGE_LABEL'.</p>
</li>
<li>
<p>is_path (boolean, optional): True if file_path column in table is file path. If False, this assumes that the column contains file names only and will append the data_directory to all files. default=True.</p>
</li>
<li>
<p>mode (string, optional): mode of handling pixel values from DICOM to numpy array. Option={'RAW': raw pixel values, 'HU': converts pixel values to HU using slope and intercept, 'WIN':Applies a certain window/level to HU converted DICOM image, 'MWIN': converts DICOM image to 3 channel HU numpy array with each channel adjusted to certain window/level. default='RAW'.</p>
</li>
<li>
<p>wl (tuple or list of tuples, optional): value of Window/Levelto be used. If mode is set to 'WIN' then wl takes the format (level, window). If mode is set to 'MWIN' then wl takes the format [(level1, window1), (level2, window2), (level3, window3)]. default=None.</p>
</li>
<li>
<p>balance_class (bollean, optional): True to perform oversampling in the train dataset to solve class imbalance. default=False.</p>
</li>
<li>
<p>balance_class_method (string, optional): methodology used to balance classes. Options={'upsample', 'downsample'}. default='upsample'.</p>
</li>
<li>
<p>interaction_terms (boolean, optional): create interaction terms between different features and add them as new features to feature table. default=False.</p>
</li>
<li>
<p>normalize (bolean/False or Tuple, optional): Normalizes all datasets by a specified mean and standard deviation. Since most of the used CNN architectures assumes 3 channel input, this follows the following format ((mean, mean, mean), (std, std, std)). default=((0,0,0), (1,1,1)).</p>
</li>
<li>
<p>batch_size (integer, optional): Batch size for dataloader. defult=16.</p>
</li>
<li>
<p>num_workers (integer, optional): Number of CPU workers for dataloader. default=0.</p>
</li>
<li>
<p>sampling (float, optional): fraction of the whole dataset to be used. default=1.0.</p>
</li>
<li>
<p>test_percent (float, optional): percentage of data for testing.default=0.2.</p>
</li>
<li>
<p>valid_percent (float, optional): percentage of data for validation (ONLY with NN_Classifier) .default=0.2.</p>
</li>
<li>
<p>custom_resize (integer, optional): By default, the data processor resizes the image in dataset into the size expected bu the different CNN architectures. To override this and use a custom resize, set this to desired value. default=False.</p>
</li>
<li>
<p>transformations (list, optional): list of pytorch transformations to be applied to all datasets. By default, the images are resized, channels added up to 3 and greyscaled. default='default'.</p>
</li>
<li>
<p>extra_transformations (list, optional): list of pytorch transformations to be extra added to train dataset specifically. default=None.</p>
</li>
<li>
<p>model_arch (string, required): CNN model architecture that this data will be used for. Used to resize images as detailed above. default='alexnet' .</p>
</li>
<li>
<p>pre_trained (boolean, optional): Initialize with ImageNet pretrained weights or not. default=True.</p>
</li>
<li>
<p>unfreeze (boolean, required): Unfreeze all layers of network for future retraining. default=False.</p>
</li>
<li>
<p>type (string, required): type of classifier. For complete list refer to settings. default='logistic_regression'.</p>
</li>
</ul>
<p><strong><em>Classifier specific parameters</em></strong></p>
<ul>
<li>
<p>cv (boolean, required): True for cross validation. default=True.</p>
</li>
<li>
<p>stratified (boolean, required): True for stratified cross validation. default=True.</p>
</li>
<li>
<p>num_splits (integer, required): Number of K-fold cross validation splits. default=5.</p>
</li>
<li>
<p>parameters (dictionary, optional): optional parameters passed to the classifier. Please refer to sci-kit learn documentaion.</p>
</li>
</ul>
<p><strong><em>NN_Classifier specific parameters</em></strong></p>
<ul>
<li>
<p>learning_rate (float, required): Learning rate. default=0.0001.</p>
</li>
<li>
<p>epochs (integer, required): training epochs. default=10.</p>
</li>
<li>
<p>optimizer (string, required): neural network optimizer type. Please see radtorch.settings for list of approved optimizers. default='Adam'.</p>
</li>
<li>
<p>optimizer_parameters (dictionary, optional): optional extra parameters for optimizer as per pytorch documentation.</p>
</li>
<li>
<p>loss_function (string, required): neural network loss function. Please see radtorch.settings for list of approved loss functions. default='CrossEntropyLoss'.</p>
</li>
<li>
<p>loss_function_parameters (dictionary, optional): optional extra parameters for loss function as per pytorch documentation.</p>
</li>
<li>
<p>lr_scheduler (string, optional): learning rate scheduler - upcoming soon.</p>
</li>
<li>
<p>custom_nn_classifier (pytorch model, optional): Option to use a custom made neural network classifier that will be added after feature extracted layers. default=None.</p>
</li>
<li>
<p>device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'.</p>
</li>
</ul>
<p><strong>Methods</strong></p>
<ul>
<li>In addition to <a href="https://www.radtorch.com/documentation/core/">core component methods</a>, pipeline accessible methods include:
  <div class="highlight"><pre><span></span><code>.info()
</code></pre></div>
    Displays information of the image classification pipeline.
  <div class="highlight"><pre><span></span><code>.run()
</code></pre></div>
    Starts the image classification pipeline training.
  <div class="highlight"><pre><span></span><code>.metrics(figure_size=(700, 350))
</code></pre></div>
    Displays the training metrics of the image classification pipeline.
  <div class="highlight"><pre><span></span><code>.export(output_path):
</code></pre></div>
    Exports the pipeline to output path.</li>
</ul>
</div>
<h2 id="gan">GAN</h2>
<div class="highlight"><pre><span></span><code>core.GAN(
        data_directory, table=None, is_dicom=False, is_path=True,
        image_path_column=&#39;IMAGE_PATH&#39;, image_label_column=&#39;IMAGE_LABEL&#39;,
        mode=&#39;RAW&#39;, wl=None, batch_size=16, normalize=((0,0,0),(1,1,1)),
        num_workers=0, label_smooth=True, sampling=1.0, transformations=&#39;default&#39;,

        discriminator=&#39;dcgan&#39;, generator=&#39;dcgan&#39;,
        generator_noise_size=100, generator_noise_type=&#39;normal&#39;,
        discriminator_num_features=64, generator_num_features=64,
        image_size=128, image_channels=1,

        discrinimator_optimizer=&#39;Adam&#39;, generator_optimizer=&#39;Adam&#39;,
        discrinimator_optimizer_param={&#39;betas&#39;:(0.5,0.999)},
        generator_optimizer_param={&#39;betas&#39;:(0.5,0.999)},
        generator_learning_rate=0.0001, discriminator_learning_rate=0.0001,        

        epochs=10, device=&#39;auto&#39;)
</code></pre></div>

<div class="admonition quote">
<p><strong>Description</strong></p>
<p>Generative Advarsarial Networks Pipeline.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p>data_directory (string, required): path to target data directory/folder.</p>
</li>
<li>
<p>is_dicom (bollean, optional): True if images are DICOM. default=False.</p>
</li>
<li>
<p>table (string or pandas dataframe, optional): path to label table csv or name of pandas data table. default=None.</p>
</li>
<li>
<p>image_path_column (string, optional): name of column that has image path/image file name. default='IMAGE_PATH'.</p>
</li>
<li>
<p>image_label_column (string, optional): name of column that has image label. default='IMAGE_LABEL'.</p>
</li>
<li>
<p>is_path (boolean, optional): True if file_path column in table is file path. If False, this assumes that the column contains file names only and will append the data_directory to all files. default=True.</p>
</li>
<li>
<p>mode (string, optional): mode of handling pixel values from DICOM to numpy array. Option={'RAW': raw pixel values, 'HU': converts pixel values to HU using slope and intercept, 'WIN':Applies a certain window/level to HU converted DICOM image, 'MWIN': converts DICOM image to 3 channel HU numpy array with each channel adjusted to certain window/level. default='RAW'.</p>
</li>
<li>
<p>wl (tuple or list of tuples, optional): value of Window/Levelto be used. If mode is set to 'WIN' then wl takes the format (level, window). If mode is set to 'MWIN' then wl takes the format [(level1, window1), (level2, window2), (level3, window3)]. default=None.</p>
</li>
<li>
<p>batch_size (integer, optional): Batch size for dataloader. defult=16.</p>
</li>
<li>
<p>num_workers (integer, optional): Number of CPU workers for dataloader. default=0.</p>
</li>
<li>
<p>sampling (float, optional): fraction of the whole dataset to be used. default=1.0.</p>
</li>
<li>
<p>transformations (list, optional): list of pytorch transformations to be applied to all datasets. By default, the images are resized, channels added up to 3 and greyscaled. default='default'.</p>
</li>
<li>
<p>normalize (bolean/False or Tuple, optional): Normalizes all datasets by a specified mean and standard deviation. Since most of the used CNN architectures assumes 3 channel input, this follows the following format ((mean, mean, mean), (std, std, std)). default=((0,0,0),(1,1,1)).</p>
</li>
<li>
<p>label_smooth (boolean, optioanl): by default, labels for real images as assigned to 1. If label smoothing is set to True, lables of real images will be assigned to 0.9. default=True. (Source: <a href="https://github.com/soumith/ganhacks#6-use-soft-and-noisy-labels">https://github.com/soumith/ganhacks#6-use-soft-and-noisy-labels</a>)</p>
</li>
<li>
<p>epochs (integer, required): training epochs. default=10.</p>
</li>
<li>
<p>generator (string, required): type of generator network. Options = {'dcgan', 'vanilla'}. default='dcgan'</p>
</li>
<li>
<p>discriminator (string, required): type of discriminator network. Options = {'dcgan', 'vanilla'}. default='dcgan'</p>
</li>
<li>
<p>image_channels (integer, required): number of output channels for discriminator input and generator output. default=1</p>
</li>
<li>
<p>generator_noise_type (string, optional): shape of noise to sample from. Options={'normal', 'gaussian'}. default='normal'. (<a href="https://github.com/soumith/ganhacks#3-use-a-spherical-z">https://github.com/soumith/ganhacks#3-use-a-spherical-z</a>)</p>
</li>
<li>
<p>generator_noise_size (integer, required): size of the noise sample to be generated. default=100</p>
</li>
<li>
<p>generator_num_features (integer, required): number of features/convolutions for generator network. default=64</p>
</li>
<li>
<p>image_size (integer, required): iamge size for discriminator input and generator output.default=128</p>
</li>
<li>
<p>discriminator_num_features (integer, required): number of features/convolutions for discriminator network.default=64</p>
</li>
<li>
<p>generator_optimizer (string, required): generator network optimizer type. Please see radtorch.settings for list of approved optimizers. default='Adam'.</p>
</li>
<li>
<p>generator_optimizer_param (dictionary, optional): optional extra parameters for optimizer as per pytorch documentation. default={'betas':(0.5,0.999)} for Adam optimizer.</p>
</li>
<li>
<p>discrinimator_optimizer (string, required): discrinimator network optimizer type. Please see radtorch.settings for list of approved optimizers. default='Adam'.</p>
</li>
<li>
<p>discrinimator_optimizer_param (dictionary, optional): optional extra parameters for optimizer as per pytorch documentation. default={'betas':(0.5,0.999)} for Adam optimizer.</p>
</li>
<li>
<p>generator_learning_rate (float, required): generator network learning rate. default=0.0001.</p>
</li>
<li>
<p>discriminator_learning_rate (float, required): discrinimator network learning rate. default=0.0001.</p>
</li>
<li>
<p>device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'.</p>
</li>
</ul>
<p><strong>Methods</strong>
  <div class="highlight"><pre><span></span><code>.run(self, verbose=&#39;batch&#39;, show_images=True, figure_size=(10,10))
</code></pre></div></p>
<ul>
<li>
<p>Runs the GAN training.</p>
</li>
<li>
<p>Parameters:</p>
<ul>
<li>
<p>verbose (string, required): amount of data output. Options {'batch': display info after each batch, 'epoch': display info after each epoch}.default='batch'</p>
</li>
<li>
<p>show_images (boolean, optional): True to show sample of generatot generated images after each epoch.</p>
</li>
<li>
<p>figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(10,10)</p>
</li>
</ul>
</li>
</ul>
<div class="highlight"><pre><span></span><code>.sample(figure_size=(10,10), show_labels=True)
</code></pre></div>

<ul>
<li>
<p>Displays a sample of real data.</p>
</li>
<li>
<p>Parameters:</p>
<ul>
<li>
<p>figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(10,10).</p>
</li>
<li>
<p>show_labels (boolean, optional): show labels on top of images. default=True.</p>
</li>
</ul>
</li>
</ul>
<div class="highlight"><pre><span></span><code>.info()
</code></pre></div>

<ul>
<li>Displays different parameters of the generative adversarial network.</li>
</ul>
<div class="highlight"><pre><span></span><code>.metrics(figure_size=(700,350))
</code></pre></div>

<ul>
<li>
<p>Displays training metrics for the GAN.</p>
</li>
<li>
<p>Explanation of metrics:</p>
<ul>
<li>
<p><em>D_loss</em>: Total loss of discriminator network on both real and fake images.</p>
</li>
<li>
<p><em>G_loss</em>: Loss of discriminator network on detecting fake images as real.</p>
</li>
<li>
<p><em>d_loss_real</em>: Loss of discriminator network on detecting real images as real.</p>
</li>
<li>
<p><em>d_loss_fake</em>: Loss of discriminator network on detecting fake images as fake.</p>
</li>
</ul>
</li>
<li>
<p>Parameters:</p>
<ul>
<li>figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(700,350).</li>
</ul>
</li>
</ul>
</div>
<p><small> Documentation Update: 5/14/2020 </small></p>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="Footer">
        
          <a href="../core/" title="radtorch.core" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                radtorch.core
              </div>
            </div>
          </a>
        
        
          <a href="../involve/" title="Get Involved" class="md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Get Involved
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4,11V13H16L10.5,18.5L11.92,19.92L19.84,12L11.92,4.08L10.5,5.5L16,11H4Z" /></svg>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/vendor.36cbf620.min.js"></script>
      <script src="../assets/javascripts/bundle.00c583dd.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents"}</script>
      
      <script>
        app = initialize({
          base: "..",
          features: ["instant"],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.7f7c8775.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
    
  </body>
</html>