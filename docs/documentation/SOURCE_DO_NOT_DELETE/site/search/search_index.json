{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-116382803-2'); RADTorch The Medical Imaging Machine Learning Framework RADTorch provides a framework of higher level classes and functions that significantly reduce the time needed for implementation of different machine and deep learning algorithms on DICOM and non-DICOM medical images. RADTorch was built by radiologists for radiologists so they can build, test and implement state-of-the-art machine learning algorithms in minutes . RADTorch was developed and is currently maintained by Mohamed Elbanan, MD : a Radiology Resident at Yale New Haven Health System, Clinical Research Affiliate at Yale School of Medicine and Artificial Intelligence Advocate. RADTorch is built upon widely used machine learning and deep learning frameworks. These include: PyTorch for Deep Learning and Neural Networks. Scikit-learn for Data Management and Machine Learning Algorithms. PyDICOM for handling of DICOM data. Bokeh, Matplotlib and Seaborn for Data Visualization. Documentation Update: 08/01/2020","title":"Home"},{"location":"#radtorch-the-medical-imaging-machine-learning-framework","text":"RADTorch provides a framework of higher level classes and functions that significantly reduce the time needed for implementation of different machine and deep learning algorithms on DICOM and non-DICOM medical images. RADTorch was built by radiologists for radiologists so they can build, test and implement state-of-the-art machine learning algorithms in minutes . RADTorch was developed and is currently maintained by Mohamed Elbanan, MD : a Radiology Resident at Yale New Haven Health System, Clinical Research Affiliate at Yale School of Medicine and Artificial Intelligence Advocate. RADTorch is built upon widely used machine learning and deep learning frameworks. These include: PyTorch for Deep Learning and Neural Networks. Scikit-learn for Data Management and Machine Learning Algorithms. PyDICOM for handling of DICOM data. Bokeh, Matplotlib and Seaborn for Data Visualization. Documentation Update: 08/01/2020","title":"RADTorch   The Medical Imaging Machine Learning Framework "},{"location":"copyright/","text":"Copyrights Copyrights are reserved to authors of all used open source packages and snippets of code. PyTorch https://github.com/pytorch/pytorch/blob/master/LICENSE From PyTorch Copyright \u00a9 2016- Facebook, Inc (Adam Paszke) Copyright \u00a9 2014- Facebook, Inc (Soumith Chintala) Copyright \u00a9 2011-2014 Idiap Research Institute (Ronan Collobert) Copyright \u00a9 2012-2014 Deepmind Technologies (Koray Kavukcuoglu) Copyright \u00a9 2011-2012 NEC Laboratories America (Koray Kavukcuoglu) Copyright \u00a9 2011-2013 NYU (Clement Farabet) Copyright \u00a9 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston) Copyright \u00a9 2006 Idiap Research Institute (Samy Bengio) Copyright \u00a9 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz) From Caffe2: Copyright \u00a9 2016-present, Facebook Inc. All rights reserved. All contributions by Facebook: Copyright \u00a9 2016 Facebook Inc. All contributions by Google: Copyright \u00a9 2015 Google Inc. All rights reserved. All contributions by Yangqing Jia: Copyright \u00a9 2015 Yangqing Jia All rights reserved. All contributions from Caffe: Copyright\u00a9 2013, 2014, 2015, the respective contributors All rights reserved. All other contributions: Copyright\u00a9 2015, 2016 the respective contributors All rights reserved. Caffe2 uses a copyright model similar to Caffe: each contributor holds copyright over their contributions to Caffe2. The project versioning records all such contribution and copyright details. If a contributor wants to further mark their specific copyright on a particular contribution, they should indicate their copyright solely in the commit message of the change when it is committed. Sklearn https://scikit-learn.org/stable/about.html#citing-scikit-learn Buitinck, L., Louppe, G., Blondel, M., Pedregosa, F., Mueller, A., Grisel, O., Niculae, V., Prettenhofer, P., Gramfort, A., Grobler, J. and Layton, R., 2013. API design for machine learning software: experiences from the scikit-learn project. arXiv preprint arXiv:1309.0238. Pydicom https://github.com/pydicom/pydicom/blob/master/LICENSE License file for pydicom, a pure-python DICOM library Copyright \u00a9 2008-2020 Darcy Mason and pydicom contributors Except for portions outlined below, pydicom is released under an MIT license: Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. Portions of pydicom (private dictionary file(s)) were generated from the private dictionary of the GDCM library, released under the following license: Program: GDCM (Grassroots DICOM). A DICOM library Module: http://gdcm.sourceforge.net/Copyright.html Copyright \u00a9 2006-2010 Mathieu Malaterre Copyright \u00a9 1993-2005 CREATIS (CREATIS = Centre de Recherche et d'Applications en Traitement de l'Image) All rights reserved. Matplotlib https://matplotlib.org/users/license.html Matplotlib only uses BSD compatible code, and its license is based on the PSF license. See the Open Source Initiative licenses page for details on individual licenses. Non-BSD compatible licenses (e.g., LGPL) are acceptable in matplotlib toolkits. For a discussion of the motivations behind the licensing choice, see Licenses. Bokeh https://github.com/bokeh/demo.bokeh.org/blob/master/LICENSE.txt Copyright \u00a9 2012 - 2018, Anaconda, Inc., and Bokeh Contributors All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of Anaconda nor the names of any contributors may be used to endorse or promote products derived from this software without specific prior written permission. Confusion Matrix Matplotlib Code snippet adapted with modification from : https://www.kaggle.com/grfiv4/plot-a-confusion-matrix Class Activation Maps Code snippet adapted with permission and modification from : https://github.com/yiskw713/ScoreCAM Documentation Update: 08/01/2020","title":"Copyrights"},{"location":"copyright/#copyrights","text":"Copyrights are reserved to authors of all used open source packages and snippets of code.","title":"Copyrights"},{"location":"copyright/#pytorch","text":"https://github.com/pytorch/pytorch/blob/master/LICENSE From PyTorch Copyright \u00a9 2016- Facebook, Inc (Adam Paszke) Copyright \u00a9 2014- Facebook, Inc (Soumith Chintala) Copyright \u00a9 2011-2014 Idiap Research Institute (Ronan Collobert) Copyright \u00a9 2012-2014 Deepmind Technologies (Koray Kavukcuoglu) Copyright \u00a9 2011-2012 NEC Laboratories America (Koray Kavukcuoglu) Copyright \u00a9 2011-2013 NYU (Clement Farabet) Copyright \u00a9 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston) Copyright \u00a9 2006 Idiap Research Institute (Samy Bengio) Copyright \u00a9 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz) From Caffe2: Copyright \u00a9 2016-present, Facebook Inc. All rights reserved. All contributions by Facebook: Copyright \u00a9 2016 Facebook Inc. All contributions by Google: Copyright \u00a9 2015 Google Inc. All rights reserved. All contributions by Yangqing Jia: Copyright \u00a9 2015 Yangqing Jia All rights reserved. All contributions from Caffe: Copyright\u00a9 2013, 2014, 2015, the respective contributors All rights reserved. All other contributions: Copyright\u00a9 2015, 2016 the respective contributors All rights reserved. Caffe2 uses a copyright model similar to Caffe: each contributor holds copyright over their contributions to Caffe2. The project versioning records all such contribution and copyright details. If a contributor wants to further mark their specific copyright on a particular contribution, they should indicate their copyright solely in the commit message of the change when it is committed.","title":"PyTorch"},{"location":"copyright/#sklearn","text":"https://scikit-learn.org/stable/about.html#citing-scikit-learn Buitinck, L., Louppe, G., Blondel, M., Pedregosa, F., Mueller, A., Grisel, O., Niculae, V., Prettenhofer, P., Gramfort, A., Grobler, J. and Layton, R., 2013. API design for machine learning software: experiences from the scikit-learn project. arXiv preprint arXiv:1309.0238.","title":"Sklearn"},{"location":"copyright/#pydicom","text":"https://github.com/pydicom/pydicom/blob/master/LICENSE License file for pydicom, a pure-python DICOM library Copyright \u00a9 2008-2020 Darcy Mason and pydicom contributors Except for portions outlined below, pydicom is released under an MIT license: Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. Portions of pydicom (private dictionary file(s)) were generated from the private dictionary of the GDCM library, released under the following license: Program: GDCM (Grassroots DICOM). A DICOM library Module: http://gdcm.sourceforge.net/Copyright.html Copyright \u00a9 2006-2010 Mathieu Malaterre Copyright \u00a9 1993-2005 CREATIS (CREATIS = Centre de Recherche et d'Applications en Traitement de l'Image) All rights reserved.","title":"Pydicom"},{"location":"copyright/#matplotlib","text":"https://matplotlib.org/users/license.html Matplotlib only uses BSD compatible code, and its license is based on the PSF license. See the Open Source Initiative licenses page for details on individual licenses. Non-BSD compatible licenses (e.g., LGPL) are acceptable in matplotlib toolkits. For a discussion of the motivations behind the licensing choice, see Licenses.","title":"Matplotlib"},{"location":"copyright/#bokeh","text":"https://github.com/bokeh/demo.bokeh.org/blob/master/LICENSE.txt Copyright \u00a9 2012 - 2018, Anaconda, Inc., and Bokeh Contributors All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of Anaconda nor the names of any contributors may be used to endorse or promote products derived from this software without specific prior written permission.","title":"Bokeh"},{"location":"copyright/#confusion-matrix-matplotlib","text":"Code snippet adapted with modification from : https://www.kaggle.com/grfiv4/plot-a-confusion-matrix","title":"Confusion Matrix Matplotlib"},{"location":"copyright/#class-activation-maps","text":"Code snippet adapted with permission and modification from : https://github.com/yiskw713/ScoreCAM Documentation Update: 08/01/2020","title":"Class Activation Maps"},{"location":"core/","text":"Core Module radtorch.core Core Module Documentation is Out of Date Core Modules documentations is out of date. Please check again later. from radtorch import core The core module has all the core functionalities of RADTorch framework. These include: RADTorch_Dataset Data_Processor Feature_Extractor Classifier NN_Classifier DCGAN_Discriminator DCGAN_Generator GAN_Discriminator GAN_Generator Feature_Selector (Coming Soon) Image classification RADTorch_Dataset core.RADTorch_Dataset(data_directory,transformations, table=None,is_dicom=False, mode='RAW', wl=None, image_path_column='IMAGE_PATH', image_label_column='IMAGE_LABEL', is_path=True, sampling=1.0) Description Core class for dataset. This is an extension of Pytorch dataset class with modifications. Parameters data_directory (string, required): path to target data directory/folder. is_dicom (bollean, optional): True if images are DICOM. default=False. table (string or pandas dataframe, optional): path to label table csv or name of pandas data table. default=None. image_path_column (string, optional): name of column that has image path/image file name. default='IMAGE_PATH'. image_label_column (string, optional): name of column that has image label. default='IMAGE_LABEL'. is_path (boolean, optional): True if file_path column in table is file path. If False, this assumes that the column contains file names only and will append the data_directory to all files. default=True. mode (string, optional): mode of handling pixel values from DICOM to numpy array. Option={'RAW': raw pixel values, 'HU': converts pixel values to HU using slope and intercept, 'WIN':Applies a certain window/level to HU converted DICOM image, 'MWIN': converts DICOM image to 3 channel HU numpy array with each channel adjusted to certain window/level. default='RAW'. wl (tuple or list of tuples, optional): value of Window/Levelto be used. If mode is set to 'WIN' then wl takes the format (level, window). If mode is set to 'MWIN' then wl takes the format [(level1, window1), (level2, window2), (level3, window3)]. default=None. sampling (float, optional): fraction of the whole dataset to be used. default=1.0. transformations (list, optional): list of pytorch transformations to be applied to all datasets. By default, the images are resized, channels added up to 3 and greyscaled. default='default'. Returns RADTorch dataset object. Methods info() Returns information of the dataset. .classes() Returns list of classes in dataset. .class_to_idx() Returns mapping of classes to class id (dictionary). .parameters() Returns all the parameter names of the dataset. .balance(method='upsample') Returns a balanced dataset. methods={'upsample', 'downsample'} .mean_std() calculates mean and standard deviation of dataset. Returns tuple of (mean, std) .normalize() Returns a normalized dataset with either mean/std of the dataset or a user specified mean/std in the form of ((mean, mean, mean), (std, std, std)). Data_Processor core.Data_Processor(data_directory,is_dicom=False,table=None, image_path_column='IMAGE_PATH', image_label_column='IMAGE_LABEL', is_path=True, mode='RAW', wl=None, balance_class=False, balance_class_method='upsample', normalize=((0,0,0), (1,1,1)), batch_size=16, num_workers=0, sampling=1.0, custom_resize=False, model_arch='alexnet', type='nn_classifier', transformations='default', extra_transformations=None, test_percent=0.2, valid_percent=0.2, device='auto') Description Class Data Processor. The core class for data preparation before feature extraction and classification. This class performs dataset creation, data splitting, sampling, balancing, normalization and transformations. Parameters data_directory (string, required): path to target data directory/folder. is_dicom (bollean, optional): True if images are DICOM. default=False. table (string or pandas dataframe, optional): path to label table csv or name of pandas data table. default=None. None means the Data_Processor will create the datasets and labels from folder structure as shown here . image_path_column (string, optional): name of column that has image path/image file name. default='IMAGE_PATH'. image_label_column (string, optional): name of column that has image label. default='IMAGE_LABEL'. is_path (boolean, optional): True if file_path column in table is file path. If False, this assumes that the column contains file names only and will append the data_directory to all files. default=False. mode (string, optional): mode of handling pixel values from DICOM to numpy array. Option={'RAW': raw pixel values, 'HU': converts pixel values to HU using slope and intercept, 'WIN':Applies a certain window/level to HU converted DICOM image, 'MWIN': converts DICOM image to 3 channel HU numpy array with each channel adjusted to certain window/level. default='RAW'. wl (tuple or list of tuples, optional): value of Window/Levelto be used. If mode is set to 'WIN' then wl takes the format (level, window). If mode is set to 'MWIN' then wl takes the format [(level1, window1), (level2, window2), (level3, window3)]. default=None. balance_class (bollean, optional): True to perform oversampling in the train dataset to solve class imbalance. default=False. balance_class_method (string, optional): methodology used to balance classes. Options={'upsample', 'downsample'}. default='upsample'. normalize (bollean, optional): Normalizes all datasets by a specified mean and standard deviation. Since most of the used CNN architectures assumes 3 channel input, this follows the following format ((mean, mean, mean), (std, std, std)). default=False. batch_size (integer, optional): Batch size for dataloader. defult=16. num_workers (integer, optional): Number of CPU workers for dataloader. default=0. sampling (float, optional): fraction of the whole dataset to be used. default=1.0. test_percent (float, optional): percentage of data for testing.default=0.2. valid_percent (float, optional): percentage of data for validation (ONLY with NN_Classifier) .default=0.2. custom_resize (integer, optional): By default, the data processor resizes the image in dataset into the size expected bu the different CNN architectures. To override this and use a custom resize, set this to desired value. default=False. model_arch (string, required): CNN model architecture that this data will be used for. Used to resize images as detailed above. default='alexnet' . type (string, required): type of classifier that will be used. please refer to classifier object type. default='nn_classifier'. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. transformations (list, optional): list of pytorch transformations to be applied to all datasets. By default, the images are resized, channels added up to 3 and greyscaled. default='default'. extra_transformations (list, optional): list of pytorch transformations to be extra added to train dataset specifically. default=None. Methods .classes() Returns dictionary of classes/class_idx in data. .info() Returns full information of the data processor object. .dataset_info(plot=True, figure_size=(500,300)) Displays information of the data and class breakdown. Parameters: plot (boolean, optional): True to display data as graph. False to display in table format. default=True figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(500,300) .sample(figure_size=(10,10), show_labels=True, show_file_name=False) Displays a sample from the training dataset. Number of images displayed is the same as batch size. Parameters: figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(10,10) show_label (boolean, optional): show labels above images. default=True show_file_names (boolean, optional): show file path above image. default=False .check_leak(show_file=False) Checks possible overlap between train and test dataset files. Parameters: show_file (boolean, optional): display table of leaked/common files between train and test. default=False. .export(output_path) Exports the Dtaprocessor object for future use. Parameters: output_path (string, required): output file path. Feature_Extractor core.Feature_Extractor(model_arch, dataloader,pre_trained=True, unfreeze=False, device='auto',) Creates a feature extractor neural network using one of the famous CNN architectures and the data provided as dataloader from Data_Processor. Parameters model_arch (string, required): CNN architecture to be utilized. To see list of supported architectures see settings. pre_trained (boolean, optional): Initialize with ImageNet pretrained weights or not. default=True. unfreeze (boolean, required): Unfreeze all layers of network for future retraining. default=False. dataloader (pytorch dataloader object, required): the dataloader that will be used to supply data for feature extraction. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. Model Architecture Default Input Image Size Output Features vgg11 224 x 224 4096 vgg13 224 x 224 4096 vgg16 224 x 224 4096 vgg19 224 x 224 4096 vgg11_bn 224 x 224 4096 vgg13_bn 224 x 224 4096 vgg16_bn 224 x 224 4096 vgg19_bn 224 x 224 4096 resnet18 224 x 224 512 resnet34 224 x 224 512 resnet50 224 x 224 2048 resnet101 224 x 224 2048 resnet152 224 x 224 2048 wide_resnet50_2 224 x 224 2048 wide_resnet101_2 224 x 224 2048 alexnet 256 x 256 4096 Returns Pandas dataframe with extracted features. Methods .num_features() Returns the number of features to be extracted. .run() Runs the feature extraction process. Returns tuple of feature_table (dataframe which contains all features, labels and image file path), features (dataframe which contains features only), feature_names(list of feature names) .export_features(csv_path) Exports extracted features into csv file. Parameters: csv_path (string, required): path to csv output. .plot_extracted_features(num_features=100, num_images=100,image_path_column='IMAGE_PATH', image_label_column='IMAGE_LABEL') Plots Extracted Features in Heatmap Parameters: num_features (integer, optional): number of features to display. default=100 num_images (integer, optional): number of images to display features for. default=100 image_path_column (string, required): name of column that has image names/path. default='IMAGE_PATH' image_label_column (string, required): name of column that has image labels. default='IMAGE_LABEL' .export(output_path) Exports the Feature Extractor object for future use. Parameters: output_path (string, required): output file path. Classifier core.Classifier(extracted_feature_dictionary, feature_table=None, image_label_column=None, image_path_column=None, test_percent=None, type='logistic_regression', interaction_terms=False, parameters={}, cv=True, stratified=True, num_splits=5) Description Image Classification Class. Performs Binary/Multiclass classification using features extracted via Feature Extractor or Supplied by user. Parameters extracted_feature_dictionary (dictionary, required): Dictionary of features/labels datasets to be used for classification. This follows the following format : { 'train': {'features':dataframe, 'feature_names':list, 'labels': list}}, 'test': {'features':dataframe, 'feature_names':list, 'labels': list}}, } feature_table (string, optional): path to csv table with user selected image paths, labels and features. default=None. image_label_column (string, required if using feature_table): name of the column with images labels.default=None. image_path_column (string, requried if using feature_table): name of column with images paths.default=None. test_percent (float, required if using feature_table): percentage of data for testing.default=None. type (string, required): type of classifier. For complete list refer to settings. default='logistic_regression'. interaction_terms (boolean, optional): create interaction terms between different features and add them as new features to feature table. default=False. cv (boolean, required): True for cross validation. default=True. stratified (boolean, required): True for stratified cross validation. default=True. num_splits (integer, required): Number of K-fold cross validation splits. default=5. parameters (dictionary, optional): optional parameters passed to the classifier. Please refer to sci-kit learn documentation. Methods .info() Returns table of different classifier parameters/properties. .run() Runs Image Classifier. .average_cv_accuracy() Returns average cross validation accuracy. .test_accuracy() Returns accuracy of trained classifier on test dataset. .confusion_matrix(title='Confusion Matrix',cmap=None,normalize=False,figure_size=(8,6)) Displays confusion matrix using trained classifier and test dataset. Parameters: title (string, optional): name to be displayed over confusion matrix. cmap (string, optional): colormap of the displayed confusion matrix. This follows matplot color palletes. default=None. normalize (boolean, optional): normalize values. default=False. figure_size (tuple, optional): size of the figure as width, height. default=(8,6) .roc() Display ROC and AUC of trained classifier and test dataset. .predict(input_image_path, all_predictions=False) Returns label prediction of a target image using a trained classifier. This works as part of pipeline only for now. Parameters: input_image_path (string, required): path of target image. all_predictions (boolean, optional): return a table of all predictions for all possible labels. .export() Exports the Classifier object for future use. output_path (string, required): output file path. .export_trained_classifier() Exports only the trained classifier for future use. output_path (string, required): output file path. NN_Classifier core.NN_Classifier(feature_extractor, data_processor, unfreeze=False, learning_rate=0.0001, epochs=10, optimizer='Adam', loss_function='CrossEntropyLoss', lr_scheduler=None, batch_size=16, device='auto', custom_nn_classifier=None, loss_function_parameters={}, optimizer_parameters={},) Description Neural Network Classifier. This serves as extension of pytorch neural network modules e.g. VGG16, for fine tuning or transfer learning. Parameters data_processor (radtorch.core.data_processor, required): data processor object from radtorch.core.Data_Processor. feature_extractor (radtorch.core.feature_extractor, required): feature_extractor object from radtorch.core.Feature_Extractor. unfreeze (boolean, optional): True to unfreeze the weights of all layers in the neural network model for model finetuning. False to just use unfreezed final layers for transfer learning. default=False. learning_rate (float, required): Learning rate. default=0.0001. epochs (integer, required): training epochs. default=10. optimizer (string, required): neural network optimizer type. Please see radtorch.settings for list of approved optimizers. default='Adam'. optimizer_parameters (dictionary, optional): optional extra parameters for optimizer as per pytorch documentation. loss_function (string, required): neural network loss function. Please see radtorch.settings for list of approved loss functions. default='CrossEntropyLoss'. loss_function_parameters (dictionary, optional): optional extra parameters for loss function as per pytorch documentation. lr_scheduler (string, optional): learning rate scheduler - upcoming soon. batch_size (integer, required): batch size. default=16 custom_nn_classifier (pytorch model, optional): Option to use a custom made neural network classifier that will be added after feature extracted layers. default=None. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. Methods .info() Returns table of different classifier parameters/properties. .run() Runs Image Classifier. .confusion_matrix(target_dataset=None, figure_size=(8,6), cmap=None) Displays confusion matrix for trained nn_classifier on test dataset. Parameters: target_dataset (pytorch dataset, optional): this option can be used to test the trained model on an external test dataset. If set to None, the confusion matrix is generated using the test dataset initially specified in the data_processor. default=None. figure_size (tuple, optional): size of the figure as width, height. default=(8,6) .roc() Display ROC and AUC of trained model and test dataset. .metrics(figure_size=(700,400)) Displays graphical representation of train/validation loss /accuracy. .predict(input_image_path, all_predictions=True) Displays classs prediction for a target image using a trained classifier. Parameters: input_image_path (string, required): path to target image. all_predictions (boolean, optional): True to display prediction percentage accuracies for all prediction classes. default=True. .misclassified(num_of_images=4, figure_size=(5,5), table=False) Displays sample of images misclassified by the classifier from test dataset. Parameters: num_of_images (integer, optional): number of images to be displayed. default=4. figure_size (tuple, optional): size of the figure as width, height. default=(5,5). table (boolean, optional): True to display a table of all misclassified images including image path, true label and predicted label. Generative Adversarial Networks DCGAN_Discriminator core.DCGAN_Discriminator(num_input_channels, kernel_size, num_discriminator_features, input_image_size, device='auto') Description Core Deep Convolutional GAN Discriminator Network. Parameters kernel_size (integer, required): size of kernel/filter to be used for convolution. num_discriminator_features (integer, required): number of features/convolutions for discriminator network. num_input_channels (integer, required): number of channels for input image. input_image_size (integer, required): size of input image. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. DCGAN_Generator core.DCGAN_Generator(noise_size, num_generator_features, num_output_channels, target_image_size, device='auto') Description Core Deep Convolutional GAN Generator Network. Parameters noise_size (integer, required): size of the noise sample to be generated. num_generator_features (integer, required): number of features/convolutions for generator network. num_output_channels (integer, required): number of channels for output image. target_image_size (integer, required): size of output image. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. GAN_Discriminator core.GAN_Discriminator(input_image_size, intput_num_channels, device='auto') Description Core Vanilla GAN Discriminator Network. Parameters num_input_channels (integer, required): number of channels for input image. input_image_size (integer, required): size of input image. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. GAN_Generator core.GAN_Generator(noise_size, target_image_size, output_num_channels, device='auto') Description Core Vanilla Convolutional GAN Generator Network. Parameters noise_size (integer, required): size of the noise sample to be generated. num_output_channels (integer, required): number of channels for output image. target_image_size (integer, required): size of output image. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. Documentation Update: 5/14/2020","title":"Core"},{"location":"core/#core-module-radtorchcore","text":"Core Module Documentation is Out of Date Core Modules documentations is out of date. Please check again later. from radtorch import core The core module has all the core functionalities of RADTorch framework. These include: RADTorch_Dataset Data_Processor Feature_Extractor Classifier NN_Classifier DCGAN_Discriminator DCGAN_Generator GAN_Discriminator GAN_Generator Feature_Selector (Coming Soon)","title":"Core Module  radtorch.core "},{"location":"core/#image-classification","text":"","title":"Image classification"},{"location":"core/#radtorch_dataset","text":"core.RADTorch_Dataset(data_directory,transformations, table=None,is_dicom=False, mode='RAW', wl=None, image_path_column='IMAGE_PATH', image_label_column='IMAGE_LABEL', is_path=True, sampling=1.0) Description Core class for dataset. This is an extension of Pytorch dataset class with modifications. Parameters data_directory (string, required): path to target data directory/folder. is_dicom (bollean, optional): True if images are DICOM. default=False. table (string or pandas dataframe, optional): path to label table csv or name of pandas data table. default=None. image_path_column (string, optional): name of column that has image path/image file name. default='IMAGE_PATH'. image_label_column (string, optional): name of column that has image label. default='IMAGE_LABEL'. is_path (boolean, optional): True if file_path column in table is file path. If False, this assumes that the column contains file names only and will append the data_directory to all files. default=True. mode (string, optional): mode of handling pixel values from DICOM to numpy array. Option={'RAW': raw pixel values, 'HU': converts pixel values to HU using slope and intercept, 'WIN':Applies a certain window/level to HU converted DICOM image, 'MWIN': converts DICOM image to 3 channel HU numpy array with each channel adjusted to certain window/level. default='RAW'. wl (tuple or list of tuples, optional): value of Window/Levelto be used. If mode is set to 'WIN' then wl takes the format (level, window). If mode is set to 'MWIN' then wl takes the format [(level1, window1), (level2, window2), (level3, window3)]. default=None. sampling (float, optional): fraction of the whole dataset to be used. default=1.0. transformations (list, optional): list of pytorch transformations to be applied to all datasets. By default, the images are resized, channels added up to 3 and greyscaled. default='default'. Returns RADTorch dataset object. Methods info() Returns information of the dataset. .classes() Returns list of classes in dataset. .class_to_idx() Returns mapping of classes to class id (dictionary). .parameters() Returns all the parameter names of the dataset. .balance(method='upsample') Returns a balanced dataset. methods={'upsample', 'downsample'} .mean_std() calculates mean and standard deviation of dataset. Returns tuple of (mean, std) .normalize() Returns a normalized dataset with either mean/std of the dataset or a user specified mean/std in the form of ((mean, mean, mean), (std, std, std)).","title":"RADTorch_Dataset"},{"location":"core/#data_processor","text":"core.Data_Processor(data_directory,is_dicom=False,table=None, image_path_column='IMAGE_PATH', image_label_column='IMAGE_LABEL', is_path=True, mode='RAW', wl=None, balance_class=False, balance_class_method='upsample', normalize=((0,0,0), (1,1,1)), batch_size=16, num_workers=0, sampling=1.0, custom_resize=False, model_arch='alexnet', type='nn_classifier', transformations='default', extra_transformations=None, test_percent=0.2, valid_percent=0.2, device='auto') Description Class Data Processor. The core class for data preparation before feature extraction and classification. This class performs dataset creation, data splitting, sampling, balancing, normalization and transformations. Parameters data_directory (string, required): path to target data directory/folder. is_dicom (bollean, optional): True if images are DICOM. default=False. table (string or pandas dataframe, optional): path to label table csv or name of pandas data table. default=None. None means the Data_Processor will create the datasets and labels from folder structure as shown here . image_path_column (string, optional): name of column that has image path/image file name. default='IMAGE_PATH'. image_label_column (string, optional): name of column that has image label. default='IMAGE_LABEL'. is_path (boolean, optional): True if file_path column in table is file path. If False, this assumes that the column contains file names only and will append the data_directory to all files. default=False. mode (string, optional): mode of handling pixel values from DICOM to numpy array. Option={'RAW': raw pixel values, 'HU': converts pixel values to HU using slope and intercept, 'WIN':Applies a certain window/level to HU converted DICOM image, 'MWIN': converts DICOM image to 3 channel HU numpy array with each channel adjusted to certain window/level. default='RAW'. wl (tuple or list of tuples, optional): value of Window/Levelto be used. If mode is set to 'WIN' then wl takes the format (level, window). If mode is set to 'MWIN' then wl takes the format [(level1, window1), (level2, window2), (level3, window3)]. default=None. balance_class (bollean, optional): True to perform oversampling in the train dataset to solve class imbalance. default=False. balance_class_method (string, optional): methodology used to balance classes. Options={'upsample', 'downsample'}. default='upsample'. normalize (bollean, optional): Normalizes all datasets by a specified mean and standard deviation. Since most of the used CNN architectures assumes 3 channel input, this follows the following format ((mean, mean, mean), (std, std, std)). default=False. batch_size (integer, optional): Batch size for dataloader. defult=16. num_workers (integer, optional): Number of CPU workers for dataloader. default=0. sampling (float, optional): fraction of the whole dataset to be used. default=1.0. test_percent (float, optional): percentage of data for testing.default=0.2. valid_percent (float, optional): percentage of data for validation (ONLY with NN_Classifier) .default=0.2. custom_resize (integer, optional): By default, the data processor resizes the image in dataset into the size expected bu the different CNN architectures. To override this and use a custom resize, set this to desired value. default=False. model_arch (string, required): CNN model architecture that this data will be used for. Used to resize images as detailed above. default='alexnet' . type (string, required): type of classifier that will be used. please refer to classifier object type. default='nn_classifier'. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. transformations (list, optional): list of pytorch transformations to be applied to all datasets. By default, the images are resized, channels added up to 3 and greyscaled. default='default'. extra_transformations (list, optional): list of pytorch transformations to be extra added to train dataset specifically. default=None. Methods .classes() Returns dictionary of classes/class_idx in data. .info() Returns full information of the data processor object. .dataset_info(plot=True, figure_size=(500,300)) Displays information of the data and class breakdown. Parameters: plot (boolean, optional): True to display data as graph. False to display in table format. default=True figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(500,300) .sample(figure_size=(10,10), show_labels=True, show_file_name=False) Displays a sample from the training dataset. Number of images displayed is the same as batch size. Parameters: figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(10,10) show_label (boolean, optional): show labels above images. default=True show_file_names (boolean, optional): show file path above image. default=False .check_leak(show_file=False) Checks possible overlap between train and test dataset files. Parameters: show_file (boolean, optional): display table of leaked/common files between train and test. default=False. .export(output_path) Exports the Dtaprocessor object for future use. Parameters: output_path (string, required): output file path.","title":"Data_Processor"},{"location":"core/#feature_extractor","text":"core.Feature_Extractor(model_arch, dataloader,pre_trained=True, unfreeze=False, device='auto',) Creates a feature extractor neural network using one of the famous CNN architectures and the data provided as dataloader from Data_Processor. Parameters model_arch (string, required): CNN architecture to be utilized. To see list of supported architectures see settings. pre_trained (boolean, optional): Initialize with ImageNet pretrained weights or not. default=True. unfreeze (boolean, required): Unfreeze all layers of network for future retraining. default=False. dataloader (pytorch dataloader object, required): the dataloader that will be used to supply data for feature extraction. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. Model Architecture Default Input Image Size Output Features vgg11 224 x 224 4096 vgg13 224 x 224 4096 vgg16 224 x 224 4096 vgg19 224 x 224 4096 vgg11_bn 224 x 224 4096 vgg13_bn 224 x 224 4096 vgg16_bn 224 x 224 4096 vgg19_bn 224 x 224 4096 resnet18 224 x 224 512 resnet34 224 x 224 512 resnet50 224 x 224 2048 resnet101 224 x 224 2048 resnet152 224 x 224 2048 wide_resnet50_2 224 x 224 2048 wide_resnet101_2 224 x 224 2048 alexnet 256 x 256 4096 Returns Pandas dataframe with extracted features. Methods .num_features() Returns the number of features to be extracted. .run() Runs the feature extraction process. Returns tuple of feature_table (dataframe which contains all features, labels and image file path), features (dataframe which contains features only), feature_names(list of feature names) .export_features(csv_path) Exports extracted features into csv file. Parameters: csv_path (string, required): path to csv output. .plot_extracted_features(num_features=100, num_images=100,image_path_column='IMAGE_PATH', image_label_column='IMAGE_LABEL') Plots Extracted Features in Heatmap Parameters: num_features (integer, optional): number of features to display. default=100 num_images (integer, optional): number of images to display features for. default=100 image_path_column (string, required): name of column that has image names/path. default='IMAGE_PATH' image_label_column (string, required): name of column that has image labels. default='IMAGE_LABEL' .export(output_path) Exports the Feature Extractor object for future use. Parameters: output_path (string, required): output file path.","title":"Feature_Extractor"},{"location":"core/#classifier","text":"core.Classifier(extracted_feature_dictionary, feature_table=None, image_label_column=None, image_path_column=None, test_percent=None, type='logistic_regression', interaction_terms=False, parameters={}, cv=True, stratified=True, num_splits=5) Description Image Classification Class. Performs Binary/Multiclass classification using features extracted via Feature Extractor or Supplied by user. Parameters extracted_feature_dictionary (dictionary, required): Dictionary of features/labels datasets to be used for classification. This follows the following format : { 'train': {'features':dataframe, 'feature_names':list, 'labels': list}}, 'test': {'features':dataframe, 'feature_names':list, 'labels': list}}, } feature_table (string, optional): path to csv table with user selected image paths, labels and features. default=None. image_label_column (string, required if using feature_table): name of the column with images labels.default=None. image_path_column (string, requried if using feature_table): name of column with images paths.default=None. test_percent (float, required if using feature_table): percentage of data for testing.default=None. type (string, required): type of classifier. For complete list refer to settings. default='logistic_regression'. interaction_terms (boolean, optional): create interaction terms between different features and add them as new features to feature table. default=False. cv (boolean, required): True for cross validation. default=True. stratified (boolean, required): True for stratified cross validation. default=True. num_splits (integer, required): Number of K-fold cross validation splits. default=5. parameters (dictionary, optional): optional parameters passed to the classifier. Please refer to sci-kit learn documentation. Methods .info() Returns table of different classifier parameters/properties. .run() Runs Image Classifier. .average_cv_accuracy() Returns average cross validation accuracy. .test_accuracy() Returns accuracy of trained classifier on test dataset. .confusion_matrix(title='Confusion Matrix',cmap=None,normalize=False,figure_size=(8,6)) Displays confusion matrix using trained classifier and test dataset. Parameters: title (string, optional): name to be displayed over confusion matrix. cmap (string, optional): colormap of the displayed confusion matrix. This follows matplot color palletes. default=None. normalize (boolean, optional): normalize values. default=False. figure_size (tuple, optional): size of the figure as width, height. default=(8,6) .roc() Display ROC and AUC of trained classifier and test dataset. .predict(input_image_path, all_predictions=False) Returns label prediction of a target image using a trained classifier. This works as part of pipeline only for now. Parameters: input_image_path (string, required): path of target image. all_predictions (boolean, optional): return a table of all predictions for all possible labels. .export() Exports the Classifier object for future use. output_path (string, required): output file path. .export_trained_classifier() Exports only the trained classifier for future use. output_path (string, required): output file path.","title":"Classifier"},{"location":"core/#nn_classifier","text":"core.NN_Classifier(feature_extractor, data_processor, unfreeze=False, learning_rate=0.0001, epochs=10, optimizer='Adam', loss_function='CrossEntropyLoss', lr_scheduler=None, batch_size=16, device='auto', custom_nn_classifier=None, loss_function_parameters={}, optimizer_parameters={},) Description Neural Network Classifier. This serves as extension of pytorch neural network modules e.g. VGG16, for fine tuning or transfer learning. Parameters data_processor (radtorch.core.data_processor, required): data processor object from radtorch.core.Data_Processor. feature_extractor (radtorch.core.feature_extractor, required): feature_extractor object from radtorch.core.Feature_Extractor. unfreeze (boolean, optional): True to unfreeze the weights of all layers in the neural network model for model finetuning. False to just use unfreezed final layers for transfer learning. default=False. learning_rate (float, required): Learning rate. default=0.0001. epochs (integer, required): training epochs. default=10. optimizer (string, required): neural network optimizer type. Please see radtorch.settings for list of approved optimizers. default='Adam'. optimizer_parameters (dictionary, optional): optional extra parameters for optimizer as per pytorch documentation. loss_function (string, required): neural network loss function. Please see radtorch.settings for list of approved loss functions. default='CrossEntropyLoss'. loss_function_parameters (dictionary, optional): optional extra parameters for loss function as per pytorch documentation. lr_scheduler (string, optional): learning rate scheduler - upcoming soon. batch_size (integer, required): batch size. default=16 custom_nn_classifier (pytorch model, optional): Option to use a custom made neural network classifier that will be added after feature extracted layers. default=None. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. Methods .info() Returns table of different classifier parameters/properties. .run() Runs Image Classifier. .confusion_matrix(target_dataset=None, figure_size=(8,6), cmap=None) Displays confusion matrix for trained nn_classifier on test dataset. Parameters: target_dataset (pytorch dataset, optional): this option can be used to test the trained model on an external test dataset. If set to None, the confusion matrix is generated using the test dataset initially specified in the data_processor. default=None. figure_size (tuple, optional): size of the figure as width, height. default=(8,6) .roc() Display ROC and AUC of trained model and test dataset. .metrics(figure_size=(700,400)) Displays graphical representation of train/validation loss /accuracy. .predict(input_image_path, all_predictions=True) Displays classs prediction for a target image using a trained classifier. Parameters: input_image_path (string, required): path to target image. all_predictions (boolean, optional): True to display prediction percentage accuracies for all prediction classes. default=True. .misclassified(num_of_images=4, figure_size=(5,5), table=False) Displays sample of images misclassified by the classifier from test dataset. Parameters: num_of_images (integer, optional): number of images to be displayed. default=4. figure_size (tuple, optional): size of the figure as width, height. default=(5,5). table (boolean, optional): True to display a table of all misclassified images including image path, true label and predicted label.","title":"NN_Classifier"},{"location":"core/#generative-adversarial-networks","text":"","title":"Generative Adversarial Networks"},{"location":"core/#dcgan_discriminator","text":"core.DCGAN_Discriminator(num_input_channels, kernel_size, num_discriminator_features, input_image_size, device='auto') Description Core Deep Convolutional GAN Discriminator Network. Parameters kernel_size (integer, required): size of kernel/filter to be used for convolution. num_discriminator_features (integer, required): number of features/convolutions for discriminator network. num_input_channels (integer, required): number of channels for input image. input_image_size (integer, required): size of input image. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'.","title":"DCGAN_Discriminator"},{"location":"core/#dcgan_generator","text":"core.DCGAN_Generator(noise_size, num_generator_features, num_output_channels, target_image_size, device='auto') Description Core Deep Convolutional GAN Generator Network. Parameters noise_size (integer, required): size of the noise sample to be generated. num_generator_features (integer, required): number of features/convolutions for generator network. num_output_channels (integer, required): number of channels for output image. target_image_size (integer, required): size of output image. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'.","title":"DCGAN_Generator"},{"location":"core/#gan_discriminator","text":"core.GAN_Discriminator(input_image_size, intput_num_channels, device='auto') Description Core Vanilla GAN Discriminator Network. Parameters num_input_channels (integer, required): number of channels for input image. input_image_size (integer, required): size of input image. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'.","title":"GAN_Discriminator"},{"location":"core/#gan_generator","text":"core.GAN_Generator(noise_size, target_image_size, output_num_channels, device='auto') Description Core Vanilla Convolutional GAN Generator Network. Parameters noise_size (integer, required): size of the noise sample to be generated. num_output_channels (integer, required): number of channels for output image. target_image_size (integer, required): size of output image. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. Documentation Update: 5/14/2020","title":"GAN_Generator"},{"location":"install/","text":"Installation Stable Release pip3 install git+https://download.radtorch.com/ Beta Version !git clone -b beta https://github.com/radtorch/radtorch/ -q !pip3 install /radtorch/. -q Uninstall pip3 uninstall radtorch Requirements Python 3.5 or later. Internet connection is required for installation and to download trained model weights. Dependencies All RADTorch dependencies and packages are installed automatically during the installation process. Documentation Update: 08/01/2020","title":"Installation"},{"location":"install/#installation","text":"","title":"Installation"},{"location":"install/#stable-release","text":"pip3 install git+https://download.radtorch.com/","title":"Stable Release"},{"location":"install/#beta-version","text":"!git clone -b beta https://github.com/radtorch/radtorch/ -q !pip3 install /radtorch/. -q","title":"Beta Version"},{"location":"install/#uninstall","text":"pip3 uninstall radtorch","title":"Uninstall"},{"location":"install/#requirements","text":"Python 3.5 or later. Internet connection is required for installation and to download trained model weights.","title":"Requirements"},{"location":"install/#dependencies","text":"All RADTorch dependencies and packages are installed automatically during the installation process. Documentation Update: 08/01/2020","title":"Dependencies"},{"location":"involve/","text":"Feature Requests Feature requests are more than welcomed on our discussion board HERE Contributing RadTorch is on GitHub . Bug reports and pull requests are welcomed. Documentation Update: 08/01/2020","title":"Get Involved"},{"location":"involve/#feature-requests","text":"Feature requests are more than welcomed on our discussion board HERE","title":"Feature Requests"},{"location":"involve/#contributing","text":"RadTorch is on GitHub . Bug reports and pull requests are welcomed. Documentation Update: 08/01/2020","title":"Contributing"},{"location":"license/","text":"GNU Affero General Public License v3.0 License Copyright \u00a9 2020 RADTorch, Mohamed Elbanan M.D. This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program. If not, see https://www.gnu.org/licenses/. Documentation Update: 08/01/2020","title":"License"},{"location":"license/#gnu-affero-general-public-license-v30-license","text":"Copyright \u00a9 2020 RADTorch, Mohamed Elbanan M.D. This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program. If not, see https://www.gnu.org/licenses/. Documentation Update: 08/01/2020","title":"GNU Affero General Public License v3.0 License"},{"location":"pipeline/","text":"Pipeline Module (radtorch.pipeline) Pipelines are the most exciting feature of RADTorch Framework. With only few lines of code, pipelines allow you to run state-of-the-art machine learning algorithms and much more. RADTorch follows principles of object-oriented-programming (OOP) in the sense that RADTorch pipelines are made of core building blocks and each of these blocks has specific functions/methods that can be accessed accordingly. For example, pipeline.Image_Classification.data_processor.dataset_info() can be used to access the dataset information for that particular Image Classification pipeline. Import from radtorch import pipeline Image Classification pipeline.Image_Classification( data_directory, name=None, table=None, image_path_column='IMAGE_PATH', image_label_column='IMAGE_LABEL',is_path=True, is_dicom=False, mode='RAW', wl=None, custom_resize=False, balance_class=False, balance_class_method='upsample', interaction_terms=False, normalize=((0,0,0), (1,1,1)), batch_size=16, num_workers=0, sampling=1.0, test_percent=0.2, valid_percent=0.2, model_arch='resnet50', pre_trained=True, unfreeze=False, type='nn_classifier', custom_nn_classifier=None, cv=True, stratified=True, num_splits=5, parameters={}, learning_rate=0.0001, epochs=10, lr_scheduler=None, optimizer='Adam', loss_function='CrossEntropyLoss', loss_function_parameters={}, optimizer_parameters={}, transformations='default', extra_transformations=None, device='auto',auto_save=False) Description Complete end-to-end image classification pipeline. Parameters Parameter Type Description Default Value General Parameters name string, optional name to be given to this classifier pipeline data_directory string, required path to target data directory/folder is_dicom boolean, optional True if images are DICOM False table string or pandas dataframe, optional path to label table csv or name of pandas data table None image_path_column string, optional name of column that has image path/image file name. 'IMAGE_PATH' image_label_column string, optional name of column that has image label. 'IMAGE_LABEL' is_path boolean, optional True if file_path column in table is file path. If False, this assumes that the column contains file names only and will append the data_directory to all files True mode string, optional mode of handling pixel values from DICOM to numpy array. Option={'RAW': raw pixel values, 'HU': converts pixel values to HU using slope and intercept, 'WIN':Applies a certain window/level to HU converted DICOM image, 'MWIN': converts DICOM image to 3 channel HU numpy array with each channel adjusted to certain window/level 'RAW' wl tuple or list of tuples, optional value of Window/Level to be used. If mode is set to 'WIN' then wl takes the format (level, window). If mode is set to 'MWIN' then wl takes the format [(level1, window1), (level2, window2), (level3, window3)] balance_class boolean, optional True to perform oversampling in the train dataset to solve class imbalance True balance_class_method string, optional methodology used to balance classes. Options={'upsample', 'downsample'} 'upsample' interaction_terms boolean, optional create interaction terms between different features and add them as new features to feature table False normalize boolean/False or Tuple, optional normalizes all datasets by a specified mean and standard deviation. Since most of the used CNN architectures assumes 3 channel input, this follows the following format ((mean, mean, mean), (std, std, std)) ((0,0,0), (1,1,1)) batch_size integer, optional batch size for dataloader 16 num_workers integer, optional number of CPU workers for dataloader 0 sampling float, optional fraction of the whole dataset to be used 1.0 test_percent float, optional percentage of data for testing 0.2 valid_percent float, optional percentage of data for validation 0.2 custom_resize integer, optional by default, the data processor resizes the image in dataset into the size expected bu the different CNN architectures. To override this and use a custom resize, set this to desired value False transformations list, optional list of pytorch transformations to be applied to all datasets. By default, the images are resized, channels added up to 3 and greyscaled 'default' extra_transformations list, optional list of pytorch transformations to be extra added to train dataset specifically None model_arch string, required CNN model architecture that this data will be used for default image resize, feature extraction and model training (model training only if type is nn_classifier) 'resnet50' pre_trained boolean, optional initialize CNN with ImageNet pretrained weights or not True unfreeze boolean, required unfreeze all layers of network for retraining False device string, optional device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu} 'auto' type string, required type of classifier. For complete list refer to settings 'logistic_regression' Classifier specific parameters cv boolean, required True for cross validation True stratified boolean, required True for stratified cross validation True num_splits integer, required number of K-fold cross validation splits 5 parameters dictionary, optional optional user-specified parameters passed to the classifier. Please refer to sci-kit-learn documentation. NN_Classifier specific parameters learning_rate float, required CNN learning rate 0.0001 epochs integer, required number of training epochs 10 optimizer string, required neural network optimizer type. Please see radtorch.settings for list of approved optimizers 'Adam' optimizer_parameters dictionary, optional optional user-specific extra parameters for optimizer as per pytorch documentation. loss_function string, required neural network loss function. Please see radtorch.settings for list of approved loss functions 'CrossEntropyLoss' loss_function_parameters dictionary, optional optional user-specific extra parameters for loss function as per pytorch documentation. custom_nn_classifier pytorch model, optional Option to use a custom made neural network classifier that will be added after feature extracted layers None Beta lr_scheduler string, optional in progress auto_save boolean, optional in progress Methods In addition to core component methods , image classification pipeline specific methods include: Method Description Parameters Default Value .info() show information of the image classification pipeline. .run() starts the image classification pipeline training. .metrics(figure_size=(700, 350) displays the training metrics of the image classification pipeline. figure_size: Size of display figure (700,350) .export(output_path) exports the pipeline to output path. output_path: path to exported pipeline file .cam(target_image_path, target_layer, type='scorecam', figure_size=(10,5), cmap='jet', alpha=0.5) diplays class activation maps from a specific layer of the trained model on a target image target_image_path: path to target image target_layer: target layer in trained model type: cam type . Options = 'cam', 'gradcam', 'gradcampp', 'smoothgradcampp', 'scorecam' figure_size: size of display figure cmap: color map alpha: overlay alpha 'scorecam' (10,5) 'jet' 0.5 Beta .deploy in progress Visualize Class Activatin Maps (CAM) for a trained image classification pipeline Requirements: Trained image classification pipeline. Select a target image. Identify a target layer in the trained model. This is done using the show_model_layers method of the nn_classifier core module . Use the .cam method. Example Assume that your trained image classifier pipeline is clf and the target image is '/image/test.png'. To select the target layer use: clf.classifier.show_model_layers() Then use the following method to show CAM : clf.cam(target_image_path='/image/test.png', target_layer=clf.trained_model.layer4[2].conv3) Hybrid Image Classification pipeline.Hybrid_Image_Classification( data_directory, name=None, table=None, image_path_column='IMAGE_PATH', image_label_column='IMAGE_LABEL',is_path=True, is_dicom=False, mode='RAW', wl=None, custom_resize=False, balance_class=False, balance_class_method='upsample', interaction_terms=False, normalize=((0,0,0), (1,1,1)), batch_size=16, num_workers=0, sampling=1.0, test_percent=0.2, valid_percent=0.2, model_arch='resnet50', pre_trained=True, unfreeze=False, type='xgboost' cv=True, stratified=True, num_splits=5, parameters={}, transformations='default', extra_transformations=None, device='auto',auto_save=False) Description Complete end-to-end image classification pipeline that uses combination of automatically generated imaging features and user provided clinical/laboratory features. Parameters The hybrid image classification pipeline uses the same parameters as the Image_Classification pipeline. Format of provided data table for hybrid model The Hybrid Image Classification pipeline expects the data table to have the following headers : Column for Image Path (same as the one specified in 'image_label_path') Column for Image Label (same as the one specified in 'image_label_column') Columns for clinical/laboratory features. Supported classifiers The Hybrid Image Classification pipeline uses CNNs for automatic imaging feature extraction. Training of CNNs (nn_classifier) using hybrid model is not yet supported. Handling Categorical and Numerical Clinical/Laboratory Data The hybrid image classification pipeline is capable of automatically handling categorical and numerical clinical data provided by the user. For example, if the data provided looks like that : the pipeline will automatically detect types of variables and modify accordingly to get this: Methods Hybrid image classification pipeline follows the same methods as Image_Classification pipeline. Feature Extraction pipeline.Feature_Extraction( name=None, data_directory, table=None, is_dicom=True, normalize=((0, 0, 0), (1, 1, 1)), batch_size=16, num_workers=0, model_arch='resnet50', custom_resize=False, pre_trained=True, unfreeze=False, label_column='IMAGE_LABEL') Description Image Feature Extraction Pipeline. Parameters Parameter Type Description Default Value name string, optional name to be given to this classifier pipeline data_directory string, required path to target data directory/folder table string or pandas dataframe, required path to label table csv or name of pandas data table None is_dicom boolean, required True if images are DICOM False normalize boolean/False or Tuple, optional normalizes all datasets by a specified mean and standard deviation. Since most of the used CNN architectures assumes 3 channel input, this follows the following format ((mean, mean, mean), (std, std, std)) ((0,0,0), (1,1,1)) batch_size integer, optional batch size for dataloader 16 num_workers integer, optional number of CPU workers for dataloader 0 model_arch string, required CNN model architecture that this data will be used for default image resize, feature extraction and model training (model training only if type is nn_classifier) 'resnet50' custom_resize integer, optional by default, the data processor resizes the image in dataset into the size expected bu the different CNN architectures. To override this and use a custom resize, set this to desired value False pre_trained boolean, optional initialize CNN with ImageNet pretrained weights or not True unfreeze boolean, required unfreeze all layers of network for retraining False label_column string, required name of column that has image label. 'IMAGE_LABEL' Methods In addition to core component methods , feature extraction pipeline specific methods include: Method Description Parameters .info() show information of the pipeline. .run() starts the feature extraction pipeline. .export(output_path) exports the pipeline to output path. output_path: path to exported pipeline file Visualize Extracted Features You can use the plot_extracted_features method of the feature_extractor core module to visualize the extracted features as below: Assume that your feature extraction pipeline is called X, X.feature_extractor.plot_extracted_features(num_features=100, num_images=100) Generative Adversarial Networks core.GAN( data_directory, table=None, is_dicom=False, is_path=True, image_path_column='IMAGE_PATH', image_label_column='IMAGE_LABEL', mode='RAW', wl=None, batch_size=16, normalize=((0,0,0),(1,1,1)), num_workers=0, label_smooth=True, sampling=1.0, transformations='default', discriminator='dcgan', generator='dcgan', generator_noise_size=100, generator_noise_type='normal', discriminator_num_features=64, generator_num_features=64, image_size=128, image_channels=1, discrinimator_optimizer='Adam', generator_optimizer='Adam', discrinimator_optimizer_param={'betas':(0.5,0.999)}, generator_optimizer_param={'betas':(0.5,0.999)}, generator_learning_rate=0.0001, discriminator_learning_rate=0.0001, epochs=10, device='auto') Description Generative Advarsarial Networks Pipeline. Parameters Parameter Type Description Default Value - - - - General Parameters data_directory string, required path to target data directory/folder is_dicom boolean, optional True if images are DICOM False table string or pandas dataframe, optional path to label table csv or name of pandas data table None image_path_column string, optional name of column that has image path/image file name. 'IMAGE_PATH' image_label_column string, optional name of column that has image label. 'IMAGE_LABEL' is_path boolean, optional True if file_path column in table is file path. If False, this assumes that the column contains file names only and will append the data_directory to all file names. True mode string, optional mode of handling pixel values from DICOM to numpy array. Option={'RAW': raw pixel values, 'HU':converts pixel values to HU using slope and intercept, 'WIN':Applies a certain window/level to HU converted DICOM image, 'MWIN': converts DICOM image to 3 channel HU numpy array with each channel adjusted to certain window/level 'RAW' wl tuple or list of tuples, optional value of Window/Level to be used. If mode is set to 'WIN' then wl takes the format (level, window). If mode is set to 'MWIN' then wl takes the format [(level1, window1), (level2, window2), (level3, window3)] batch_size integer, optional batch size for dataloader 16 num_workers integer, optional number of CPU workers for dataloader 0 sampling float, optional fraction of the whole dataset to be used 1.0 transformations list, optional list of pytorch transformations to be applied to all datasets. By default, the images are resized, channels added up to 3 and greyscaled 'default' normalize boolean/False or Tuple, optional normalizes all datasets by a specified mean and standard deviation. Since most of the used CNN architectures assumes 3 channel input, this follows the following format ((mean, mean, mean), (std, std, std)) ((0,0,0), (1,1,1)) epochs integer, required number of training epochs 10 image_channels integer, required number of channels for discriminator input and generator output 1 image_size integer, required image size for discriminator input and generator output 128 device string, optional device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu} 'auto' Discriminator Parameters discriminator string, required type of discriminator network. Options = {'dcgan', 'vanilla', 'wgan'} 'dcgan' discriminator_num_features integer, required number of features/convolutions for discriminator network 64 label_smooth boolean, optioanl by default, labels for real images as assigned to 1. If label smoothing is set to True, lables of real images will be assigned to 0.9. (Source: https://github.com/soumith/ganhacks#6-use-soft-and-noisy-labels ) True discrinimator_optimizer string, required discriminator network optimizer type. Please see radtorch.settings for list of approved optimizers 'Adam' discrinimator_optimizer_param dictionary, optional optional extra parameters for optimizer as per pytorch documentation. {'betas':(0.5,0.999)} for Adam optimizer. discriminator_learning_rate float, required discrinimator network learning rate 0.0001 Generator Parameters generator string, required type of generator network. Options = {'dcgan', 'vanilla', 'wgan'} 'dcgan' generator_noise_type string, optional shape of noise to sample from. Options={'normal', 'gaussian'} (Source: ( https://github.com/soumith/ganhacks#3-use-a-spherical-z ) 'normal' generator_noise_size integer, required size of the noise sample to be generated 100 generator_num_features integer, required number of features/convolutions for generator network 64 generator_optimizer string, required generator network optimizer type. Please see radtorch.settings for list of approved optimizers 'Adam' generator_optimizer_param dictionary, optional optional extra parameters for optimizer as per pytorch documentation {'betas':(0.5,0.999)} for Adam optimizer. generator_learning_rate float, required generator network learning rate 0.0001 Methods .run(self, verbose='batch', show_images=True, figure_size=(10,10)) Runs the GAN training. Parameters: verbose (string, required): amount of data output. Options {'batch': display info after each batch, 'epoch': display info after each epoch}.default='batch' show_images (boolean, optional): True to show sample of generatot generated images after each epoch. figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(10,10) .sample(figure_size=(10,10), show_labels=True) Displays a sample of real data. Parameters: figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(10,10). show_labels (boolean, optional): show labels on top of images. default=True. .info() Displays different parameters of the generative adversarial network. .metrics(figure_size=(700,350)) Displays training metrics for the GAN. Explanation of metrics: D_loss : Total loss of discriminator network on both real and fake images. G_loss : Loss of discriminator network on detecting fake images as real. d_loss_real : Loss of discriminator network on detecting real images as real. d_loss_fake : Loss of discriminator network on detecting fake images as fake. Parameters: figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(700,350). Documentation Update: 08/01/2020","title":"Pipeline"},{"location":"pipeline/#pipeline-module-radtorchpipeline","text":"Pipelines are the most exciting feature of RADTorch Framework. With only few lines of code, pipelines allow you to run state-of-the-art machine learning algorithms and much more. RADTorch follows principles of object-oriented-programming (OOP) in the sense that RADTorch pipelines are made of core building blocks and each of these blocks has specific functions/methods that can be accessed accordingly. For example, pipeline.Image_Classification.data_processor.dataset_info() can be used to access the dataset information for that particular Image Classification pipeline.","title":"Pipeline Module  (radtorch.pipeline) "},{"location":"pipeline/#import","text":"from radtorch import pipeline","title":"Import"},{"location":"pipeline/#image-classification","text":"pipeline.Image_Classification( data_directory, name=None, table=None, image_path_column='IMAGE_PATH', image_label_column='IMAGE_LABEL',is_path=True, is_dicom=False, mode='RAW', wl=None, custom_resize=False, balance_class=False, balance_class_method='upsample', interaction_terms=False, normalize=((0,0,0), (1,1,1)), batch_size=16, num_workers=0, sampling=1.0, test_percent=0.2, valid_percent=0.2, model_arch='resnet50', pre_trained=True, unfreeze=False, type='nn_classifier', custom_nn_classifier=None, cv=True, stratified=True, num_splits=5, parameters={}, learning_rate=0.0001, epochs=10, lr_scheduler=None, optimizer='Adam', loss_function='CrossEntropyLoss', loss_function_parameters={}, optimizer_parameters={}, transformations='default', extra_transformations=None, device='auto',auto_save=False) Description Complete end-to-end image classification pipeline. Parameters Parameter Type Description Default Value General Parameters name string, optional name to be given to this classifier pipeline data_directory string, required path to target data directory/folder is_dicom boolean, optional True if images are DICOM False table string or pandas dataframe, optional path to label table csv or name of pandas data table None image_path_column string, optional name of column that has image path/image file name. 'IMAGE_PATH' image_label_column string, optional name of column that has image label. 'IMAGE_LABEL' is_path boolean, optional True if file_path column in table is file path. If False, this assumes that the column contains file names only and will append the data_directory to all files True mode string, optional mode of handling pixel values from DICOM to numpy array. Option={'RAW': raw pixel values, 'HU': converts pixel values to HU using slope and intercept, 'WIN':Applies a certain window/level to HU converted DICOM image, 'MWIN': converts DICOM image to 3 channel HU numpy array with each channel adjusted to certain window/level 'RAW' wl tuple or list of tuples, optional value of Window/Level to be used. If mode is set to 'WIN' then wl takes the format (level, window). If mode is set to 'MWIN' then wl takes the format [(level1, window1), (level2, window2), (level3, window3)] balance_class boolean, optional True to perform oversampling in the train dataset to solve class imbalance True balance_class_method string, optional methodology used to balance classes. Options={'upsample', 'downsample'} 'upsample' interaction_terms boolean, optional create interaction terms between different features and add them as new features to feature table False normalize boolean/False or Tuple, optional normalizes all datasets by a specified mean and standard deviation. Since most of the used CNN architectures assumes 3 channel input, this follows the following format ((mean, mean, mean), (std, std, std)) ((0,0,0), (1,1,1)) batch_size integer, optional batch size for dataloader 16 num_workers integer, optional number of CPU workers for dataloader 0 sampling float, optional fraction of the whole dataset to be used 1.0 test_percent float, optional percentage of data for testing 0.2 valid_percent float, optional percentage of data for validation 0.2 custom_resize integer, optional by default, the data processor resizes the image in dataset into the size expected bu the different CNN architectures. To override this and use a custom resize, set this to desired value False transformations list, optional list of pytorch transformations to be applied to all datasets. By default, the images are resized, channels added up to 3 and greyscaled 'default' extra_transformations list, optional list of pytorch transformations to be extra added to train dataset specifically None model_arch string, required CNN model architecture that this data will be used for default image resize, feature extraction and model training (model training only if type is nn_classifier) 'resnet50' pre_trained boolean, optional initialize CNN with ImageNet pretrained weights or not True unfreeze boolean, required unfreeze all layers of network for retraining False device string, optional device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu} 'auto' type string, required type of classifier. For complete list refer to settings 'logistic_regression' Classifier specific parameters cv boolean, required True for cross validation True stratified boolean, required True for stratified cross validation True num_splits integer, required number of K-fold cross validation splits 5 parameters dictionary, optional optional user-specified parameters passed to the classifier. Please refer to sci-kit-learn documentation. NN_Classifier specific parameters learning_rate float, required CNN learning rate 0.0001 epochs integer, required number of training epochs 10 optimizer string, required neural network optimizer type. Please see radtorch.settings for list of approved optimizers 'Adam' optimizer_parameters dictionary, optional optional user-specific extra parameters for optimizer as per pytorch documentation. loss_function string, required neural network loss function. Please see radtorch.settings for list of approved loss functions 'CrossEntropyLoss' loss_function_parameters dictionary, optional optional user-specific extra parameters for loss function as per pytorch documentation. custom_nn_classifier pytorch model, optional Option to use a custom made neural network classifier that will be added after feature extracted layers None Beta lr_scheduler string, optional in progress auto_save boolean, optional in progress Methods In addition to core component methods , image classification pipeline specific methods include: Method Description Parameters Default Value .info() show information of the image classification pipeline. .run() starts the image classification pipeline training. .metrics(figure_size=(700, 350) displays the training metrics of the image classification pipeline. figure_size: Size of display figure (700,350) .export(output_path) exports the pipeline to output path. output_path: path to exported pipeline file .cam(target_image_path, target_layer, type='scorecam', figure_size=(10,5), cmap='jet', alpha=0.5) diplays class activation maps from a specific layer of the trained model on a target image target_image_path: path to target image target_layer: target layer in trained model type: cam type . Options = 'cam', 'gradcam', 'gradcampp', 'smoothgradcampp', 'scorecam' figure_size: size of display figure cmap: color map alpha: overlay alpha 'scorecam' (10,5) 'jet' 0.5 Beta .deploy in progress Visualize Class Activatin Maps (CAM) for a trained image classification pipeline Requirements: Trained image classification pipeline. Select a target image. Identify a target layer in the trained model. This is done using the show_model_layers method of the nn_classifier core module . Use the .cam method. Example Assume that your trained image classifier pipeline is clf and the target image is '/image/test.png'. To select the target layer use: clf.classifier.show_model_layers() Then use the following method to show CAM : clf.cam(target_image_path='/image/test.png', target_layer=clf.trained_model.layer4[2].conv3)","title":"Image Classification"},{"location":"pipeline/#hybrid-image-classification","text":"pipeline.Hybrid_Image_Classification( data_directory, name=None, table=None, image_path_column='IMAGE_PATH', image_label_column='IMAGE_LABEL',is_path=True, is_dicom=False, mode='RAW', wl=None, custom_resize=False, balance_class=False, balance_class_method='upsample', interaction_terms=False, normalize=((0,0,0), (1,1,1)), batch_size=16, num_workers=0, sampling=1.0, test_percent=0.2, valid_percent=0.2, model_arch='resnet50', pre_trained=True, unfreeze=False, type='xgboost' cv=True, stratified=True, num_splits=5, parameters={}, transformations='default', extra_transformations=None, device='auto',auto_save=False) Description Complete end-to-end image classification pipeline that uses combination of automatically generated imaging features and user provided clinical/laboratory features. Parameters The hybrid image classification pipeline uses the same parameters as the Image_Classification pipeline. Format of provided data table for hybrid model The Hybrid Image Classification pipeline expects the data table to have the following headers : Column for Image Path (same as the one specified in 'image_label_path') Column for Image Label (same as the one specified in 'image_label_column') Columns for clinical/laboratory features. Supported classifiers The Hybrid Image Classification pipeline uses CNNs for automatic imaging feature extraction. Training of CNNs (nn_classifier) using hybrid model is not yet supported. Handling Categorical and Numerical Clinical/Laboratory Data The hybrid image classification pipeline is capable of automatically handling categorical and numerical clinical data provided by the user. For example, if the data provided looks like that : the pipeline will automatically detect types of variables and modify accordingly to get this: Methods Hybrid image classification pipeline follows the same methods as Image_Classification pipeline.","title":"Hybrid Image Classification"},{"location":"pipeline/#feature-extraction","text":"pipeline.Feature_Extraction( name=None, data_directory, table=None, is_dicom=True, normalize=((0, 0, 0), (1, 1, 1)), batch_size=16, num_workers=0, model_arch='resnet50', custom_resize=False, pre_trained=True, unfreeze=False, label_column='IMAGE_LABEL') Description Image Feature Extraction Pipeline. Parameters Parameter Type Description Default Value name string, optional name to be given to this classifier pipeline data_directory string, required path to target data directory/folder table string or pandas dataframe, required path to label table csv or name of pandas data table None is_dicom boolean, required True if images are DICOM False normalize boolean/False or Tuple, optional normalizes all datasets by a specified mean and standard deviation. Since most of the used CNN architectures assumes 3 channel input, this follows the following format ((mean, mean, mean), (std, std, std)) ((0,0,0), (1,1,1)) batch_size integer, optional batch size for dataloader 16 num_workers integer, optional number of CPU workers for dataloader 0 model_arch string, required CNN model architecture that this data will be used for default image resize, feature extraction and model training (model training only if type is nn_classifier) 'resnet50' custom_resize integer, optional by default, the data processor resizes the image in dataset into the size expected bu the different CNN architectures. To override this and use a custom resize, set this to desired value False pre_trained boolean, optional initialize CNN with ImageNet pretrained weights or not True unfreeze boolean, required unfreeze all layers of network for retraining False label_column string, required name of column that has image label. 'IMAGE_LABEL' Methods In addition to core component methods , feature extraction pipeline specific methods include: Method Description Parameters .info() show information of the pipeline. .run() starts the feature extraction pipeline. .export(output_path) exports the pipeline to output path. output_path: path to exported pipeline file Visualize Extracted Features You can use the plot_extracted_features method of the feature_extractor core module to visualize the extracted features as below: Assume that your feature extraction pipeline is called X, X.feature_extractor.plot_extracted_features(num_features=100, num_images=100)","title":"Feature Extraction"},{"location":"pipeline/#generative-adversarial-networks","text":"core.GAN( data_directory, table=None, is_dicom=False, is_path=True, image_path_column='IMAGE_PATH', image_label_column='IMAGE_LABEL', mode='RAW', wl=None, batch_size=16, normalize=((0,0,0),(1,1,1)), num_workers=0, label_smooth=True, sampling=1.0, transformations='default', discriminator='dcgan', generator='dcgan', generator_noise_size=100, generator_noise_type='normal', discriminator_num_features=64, generator_num_features=64, image_size=128, image_channels=1, discrinimator_optimizer='Adam', generator_optimizer='Adam', discrinimator_optimizer_param={'betas':(0.5,0.999)}, generator_optimizer_param={'betas':(0.5,0.999)}, generator_learning_rate=0.0001, discriminator_learning_rate=0.0001, epochs=10, device='auto') Description Generative Advarsarial Networks Pipeline. Parameters Parameter Type Description Default Value - - - - General Parameters data_directory string, required path to target data directory/folder is_dicom boolean, optional True if images are DICOM False table string or pandas dataframe, optional path to label table csv or name of pandas data table None image_path_column string, optional name of column that has image path/image file name. 'IMAGE_PATH' image_label_column string, optional name of column that has image label. 'IMAGE_LABEL' is_path boolean, optional True if file_path column in table is file path. If False, this assumes that the column contains file names only and will append the data_directory to all file names. True mode string, optional mode of handling pixel values from DICOM to numpy array. Option={'RAW': raw pixel values, 'HU':converts pixel values to HU using slope and intercept, 'WIN':Applies a certain window/level to HU converted DICOM image, 'MWIN': converts DICOM image to 3 channel HU numpy array with each channel adjusted to certain window/level 'RAW' wl tuple or list of tuples, optional value of Window/Level to be used. If mode is set to 'WIN' then wl takes the format (level, window). If mode is set to 'MWIN' then wl takes the format [(level1, window1), (level2, window2), (level3, window3)] batch_size integer, optional batch size for dataloader 16 num_workers integer, optional number of CPU workers for dataloader 0 sampling float, optional fraction of the whole dataset to be used 1.0 transformations list, optional list of pytorch transformations to be applied to all datasets. By default, the images are resized, channels added up to 3 and greyscaled 'default' normalize boolean/False or Tuple, optional normalizes all datasets by a specified mean and standard deviation. Since most of the used CNN architectures assumes 3 channel input, this follows the following format ((mean, mean, mean), (std, std, std)) ((0,0,0), (1,1,1)) epochs integer, required number of training epochs 10 image_channels integer, required number of channels for discriminator input and generator output 1 image_size integer, required image size for discriminator input and generator output 128 device string, optional device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu} 'auto' Discriminator Parameters discriminator string, required type of discriminator network. Options = {'dcgan', 'vanilla', 'wgan'} 'dcgan' discriminator_num_features integer, required number of features/convolutions for discriminator network 64 label_smooth boolean, optioanl by default, labels for real images as assigned to 1. If label smoothing is set to True, lables of real images will be assigned to 0.9. (Source: https://github.com/soumith/ganhacks#6-use-soft-and-noisy-labels ) True discrinimator_optimizer string, required discriminator network optimizer type. Please see radtorch.settings for list of approved optimizers 'Adam' discrinimator_optimizer_param dictionary, optional optional extra parameters for optimizer as per pytorch documentation. {'betas':(0.5,0.999)} for Adam optimizer. discriminator_learning_rate float, required discrinimator network learning rate 0.0001 Generator Parameters generator string, required type of generator network. Options = {'dcgan', 'vanilla', 'wgan'} 'dcgan' generator_noise_type string, optional shape of noise to sample from. Options={'normal', 'gaussian'} (Source: ( https://github.com/soumith/ganhacks#3-use-a-spherical-z ) 'normal' generator_noise_size integer, required size of the noise sample to be generated 100 generator_num_features integer, required number of features/convolutions for generator network 64 generator_optimizer string, required generator network optimizer type. Please see radtorch.settings for list of approved optimizers 'Adam' generator_optimizer_param dictionary, optional optional extra parameters for optimizer as per pytorch documentation {'betas':(0.5,0.999)} for Adam optimizer. generator_learning_rate float, required generator network learning rate 0.0001 Methods .run(self, verbose='batch', show_images=True, figure_size=(10,10)) Runs the GAN training. Parameters: verbose (string, required): amount of data output. Options {'batch': display info after each batch, 'epoch': display info after each epoch}.default='batch' show_images (boolean, optional): True to show sample of generatot generated images after each epoch. figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(10,10) .sample(figure_size=(10,10), show_labels=True) Displays a sample of real data. Parameters: figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(10,10). show_labels (boolean, optional): show labels on top of images. default=True. .info() Displays different parameters of the generative adversarial network. .metrics(figure_size=(700,350)) Displays training metrics for the GAN. Explanation of metrics: D_loss : Total loss of discriminator network on both real and fake images. G_loss : Loss of discriminator network on detecting fake images as real. d_loss_real : Loss of discriminator network on detecting real images as real. d_loss_fake : Loss of discriminator network on detecting fake images as fake. Parameters: figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(700,350). Documentation Update: 08/01/2020","title":"Generative Adversarial Networks"},{"location":"start/","text":"Starting Training a state-of-the-art DICOM image classifier can be done using the Image Classification Pipeline via: from radtorch import pipeline classifier = pipeline.Image_Classification(data_directory='path to data') classifier.run() The above 3 lines of code will run an image classifier pipeline using ResNet50 architecture with ImageNet pre-trained weights. Google Colab Playground RADTorch playground for testing is provided on Google Colab . Documentation Update: 08/01/2020","title":"Getting Started"},{"location":"start/#starting","text":"Training a state-of-the-art DICOM image classifier can be done using the Image Classification Pipeline via: from radtorch import pipeline classifier = pipeline.Image_Classification(data_directory='path to data') classifier.run() The above 3 lines of code will run an image classifier pipeline using ResNet50 architecture with ImageNet pre-trained weights.","title":"Starting"},{"location":"start/#google-colab-playground","text":"RADTorch playground for testing is provided on Google Colab . Documentation Update: 08/01/2020","title":"Google Colab Playground"}]}