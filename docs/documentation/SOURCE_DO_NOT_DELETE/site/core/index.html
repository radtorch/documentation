


<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="shortcut icon" href="../img/radtorch_icon.ico">
      <meta name="generator" content="mkdocs-1.1, mkdocs-material-5.1.0">
    
    
      
        <title>radtorch.core - RADTorch - API Documentation</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.89dc9fe3.min.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/palette.ecd4686e.min.css">
      
      
        
        
        <meta name="theme-color" content="">
      
    
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    
      
        
<link rel="preconnect dns-prefetch" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-116382803-2","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}),document.addEventListener("DOMContentSwitch",function(){ga("send","pageview")})</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
  
    
    
    <body dir="ltr" data-md-color-primary="black" data-md-color-accent="deep-orange">
  
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#core-module-radtorchcore" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href=".." title="RADTorch - API Documentation" class="md-header-nav__button md-logo" aria-label="RADTorch - API Documentation">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18,22A2,2 0 0,0 20,20V4C20,2.89 19.1,2 18,2H12V9L9.5,7.5L7,9V2H6A2,2 0 0,0 4,4V20A2,2 0 0,0 6,22H18Z" /></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3,6H21V8H3V6M3,11H21V13H3V11M3,16H21V18H3V16Z" /></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            RADTorch - API Documentation
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              radtorch.core
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5,3A6.5,6.5 0 0,1 16,9.5C16,11.11 15.41,12.59 14.44,13.73L14.71,14H15.5L20.5,19L19,20.5L14,15.5V14.71L13.73,14.44C12.59,15.41 11.11,16 9.5,16A6.5,6.5 0 0,1 3,9.5A6.5,6.5 0 0,1 9.5,3M9.5,5C7,5 5,7 5,9.5C5,12 7,14 9.5,14C12,14 14,12 14,9.5C14,7 12,5 9.5,5Z" /></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5,3A6.5,6.5 0 0,1 16,9.5C16,11.11 15.41,12.59 14.44,13.73L14.71,14H15.5L20.5,19L19,20.5L14,15.5V14.71L13.73,14.44C12.59,15.41 11.11,16 9.5,16A6.5,6.5 0 0,1 3,9.5A6.5,6.5 0 0,1 9.5,3M9.5,5C7,5 5,7 5,9.5C5,12 7,14 9.5,14C12,14 14,12 14,9.5C14,7 12,5 9.5,5Z" /></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19,6.41L17.59,5L12,10.59L6.41,5L5,6.41L10.59,12L5,17.59L6.41,19L12,13.41L17.59,19L19,17.59L13.41,12L19,6.41Z" /></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/radtorch/radtorch/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    radtorch/radtorch
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
        
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="RADTorch - API Documentation" class="md-nav__button md-logo" aria-label="RADTorch - API Documentation">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18,22A2,2 0 0,0 20,20V4C20,2.89 19.1,2 18,2H12V9L9.5,7.5L7,9V2H6A2,2 0 0,0 4,4V20A2,2 0 0,0 6,22H18Z" /></svg>

    </a>
    RADTorch - API Documentation
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/radtorch/radtorch/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    radtorch/radtorch
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../start/" title="Getting Started" class="md-nav__link">
      Getting Started
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../install/" title="Installation" class="md-nav__link">
      Installation
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        radtorch.core
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3,9H17V7H3V9M3,13H17V11H3V13M3,17H17V15H3V17M19,17H21V15H19V17M19,7V9H21V7H19M19,13H21V11H19V13Z" /></svg>
        </span>
      </label>
    
    <a href="./" title="radtorch.core" class="md-nav__link md-nav__link--active">
      radtorch.core
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#image-classification" class="md-nav__link">
    Image classification
  </a>
  
    <nav class="md-nav" aria-label="Image classification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#radtorch_dataset" class="md-nav__link">
    RADTorch_Dataset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data_processor" class="md-nav__link">
    Data_Processor
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#feature_extractor" class="md-nav__link">
    Feature_Extractor
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#classifier" class="md-nav__link">
    Classifier
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nn_classifier" class="md-nav__link">
    NN_Classifier
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generative-adversarial-networks" class="md-nav__link">
    Generative Adversarial Networks
  </a>
  
    <nav class="md-nav" aria-label="Generative Adversarial Networks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dcgan_discriminator" class="md-nav__link">
    DCGAN_Discriminator
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dcgan_generator" class="md-nav__link">
    DCGAN_Generator
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gan_discriminator" class="md-nav__link">
    GAN_Discriminator
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gan_generator" class="md-nav__link">
    GAN_Generator
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../pipeline/" title="radtorch.pipeline" class="md-nav__link">
      radtorch.pipeline
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../involve/" title="Get Involved" class="md-nav__link">
      Get Involved
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../copyright/" title="Copyrights" class="md-nav__link">
      Copyrights
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../license/" title="License" class="md-nav__link">
      License
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#image-classification" class="md-nav__link">
    Image classification
  </a>
  
    <nav class="md-nav" aria-label="Image classification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#radtorch_dataset" class="md-nav__link">
    RADTorch_Dataset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data_processor" class="md-nav__link">
    Data_Processor
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#feature_extractor" class="md-nav__link">
    Feature_Extractor
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#classifier" class="md-nav__link">
    Classifier
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nn_classifier" class="md-nav__link">
    NN_Classifier
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generative-adversarial-networks" class="md-nav__link">
    Generative Adversarial Networks
  </a>
  
    <nav class="md-nav" aria-label="Generative Adversarial Networks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dcgan_discriminator" class="md-nav__link">
    DCGAN_Discriminator
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dcgan_generator" class="md-nav__link">
    DCGAN_Generator
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gan_discriminator" class="md-nav__link">
    GAN_Discriminator
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gan_generator" class="md-nav__link">
    GAN_Generator
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  
                
                
                <h1 id="core-module-radtorchcore">Core Module <small> radtorch.core </small></h1>
<p><div class="highlight"><pre><span></span><code>from radtorch import core
</code></pre></div>
The core module has all the core functionalities of RADTorch framework. These include:</p>
<ol>
<li>
<p>RADTorch_Dataset</p>
</li>
<li>
<p>Data_Processor</p>
</li>
<li>
<p>Feature_Extractor</p>
</li>
<li>
<p>Classifier</p>
</li>
<li>
<p>NN_Classifier</p>
</li>
<li>
<p>DCGAN_Discriminator</p>
</li>
<li>
<p>DCGAN_Generator</p>
</li>
<li>
<p>GAN_Discriminator</p>
</li>
<li>
<p>GAN_Generator</p>
</li>
<li>
<p>Feature_Selector (Coming Soon)</p>
</li>
</ol>
<h2 id="image-classification">Image classification</h2>
<h3 id="radtorch_dataset">RADTorch_Dataset</h3>
<pre><code>core.RADTorch_Dataset(data_directory,transformations,
                      table=None,is_dicom=False,
                      mode='RAW', wl=None,
                      image_path_column='IMAGE_PATH',
                      image_label_column='IMAGE_LABEL',
                      is_path=True, sampling=1.0)
</code></pre>
<div class="admonition quote">
<p><strong>Description</strong></p>
<p>Core class for dataset. This is an extension of Pytorch dataset class with modifications.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p>data_directory (string, required): path to target data directory/folder.</p>
</li>
<li>
<p>is_dicom (bollean, optional): True if images are DICOM. default=False.</p>
</li>
<li>
<p>table (string or pandas dataframe, optional): path to label table csv or name of pandas data table. default=None.</p>
</li>
<li>
<p>image_path_column (string, optional): name of column that has image path/image file name. default='IMAGE_PATH'.</p>
</li>
<li>
<p>image_label_column (string, optional): name of column that has image label. default='IMAGE_LABEL'.</p>
</li>
<li>
<p>is_path (boolean, optional): True if file_path column in table is file path. If False, this assumes that the column contains file names only and will append the data_directory to all files. default=True.</p>
</li>
<li>
<p>mode (string, optional): mode of handling pixel values from DICOM to numpy array. Option={'RAW': raw pixel values, 'HU': converts pixel values to HU using slope and intercept, 'WIN':Applies a certain window/level to HU converted DICOM image, 'MWIN': converts DICOM image to 3 channel HU numpy array with each channel adjusted to certain window/level. default='RAW'.</p>
</li>
<li>
<p>wl (tuple or list of tuples, optional): value of Window/Levelto be used. If mode is set to 'WIN' then wl takes the format (level, window). If mode is set to 'MWIN' then wl takes the format [(level1, window1), (level2, window2), (level3, window3)]. default=None.</p>
</li>
<li>
<p>sampling (float, optional): fraction of the whole dataset to be used. default=1.0.</p>
</li>
<li>
<p>transformations (list, optional): list of pytorch transformations to be applied to all datasets. By default, the images are resized, channels added up to 3 and greyscaled. default='default'.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<p>RADTorch dataset object.</p>
<p><strong>Methods</strong></p>
<p><em>info()</em></p>
<ul>
<li>Returns information of the dataset.</li>
</ul>
<p><em>.classes()</em></p>
<ul>
<li>Returns list of classes in dataset.</li>
</ul>
<p><em>.class_to_idx()</em></p>
<ul>
<li>Returns mapping of classes to class id (dictionary).</li>
</ul>
<p><em>.parameters()</em></p>
<ul>
<li>Returns all the parameter names of the dataset.</li>
</ul>
<p><em>.balance(method='upsample')</em></p>
<ul>
<li>Returns a balanced dataset. methods={'upsample', 'downsample'}</li>
</ul>
<p><em>.mean_std()</em></p>
<ul>
<li>calculates mean and standard deviation of dataset. Returns tuple of (mean, std)</li>
</ul>
<p><em>.normalize()</em></p>
<ul>
<li>Returns a normalized dataset with either mean/std of the dataset or a user specified mean/std in the form of ((mean, mean, mean), (std, std, std)).</li>
</ul>
</div>
<h3 id="data_processor">Data_Processor</h3>
<pre><code>core.Data_Processor(data_directory,is_dicom=False,table=None,
                    image_path_column='IMAGE_PATH',
                    image_label_column='IMAGE_LABEL',
                    is_path=True, mode='RAW', wl=None,
                    balance_class=False, balance_class_method='upsample',
                    normalize=((0,0,0), (1,1,1)), batch_size=16,
                    num_workers=0, sampling=1.0, custom_resize=False,
                    model_arch='alexnet', type='nn_classifier',
                    transformations='default',
                    extra_transformations=None,
                    test_percent=0.2, valid_percent=0.2, device='auto')
</code></pre>
<div class="admonition quote">
<p><strong>Description</strong></p>
<p>Class Data Processor. The core class for data preparation before feature extraction and classification. This class performs dataset creation, data splitting, sampling, balancing, normalization and transformations.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p>data_directory (string, required): path to target data directory/folder.</p>
</li>
<li>
<p>is_dicom (bollean, optional): True if images are DICOM. default=False.</p>
</li>
<li>
<p>table (string or pandas dataframe, optional): path to label table csv or name of pandas data table. default=None. <strong>None</strong> means the Data_Processor will create the datasets and labels from folder structure as shown <a href="https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder">here</a>.</p>
</li>
<li>
<p>image_path_column (string, optional): name of column that has image path/image file name. default='IMAGE_PATH'.</p>
</li>
<li>
<p>image_label_column (string, optional): name of column that has image label. default='IMAGE_LABEL'.</p>
</li>
<li>
<p>is_path (boolean, optional): True if file_path column in table is file path. If False, this assumes that the column contains file names only and will append the data_directory to all files. default=False.</p>
</li>
<li>
<p>mode (string, optional): mode of handling pixel values from DICOM to numpy array. Option={'RAW': raw pixel values, 'HU': converts pixel values to HU using slope and intercept, 'WIN':Applies a certain window/level to HU converted DICOM image, 'MWIN': converts DICOM image to 3 channel HU numpy array with each channel adjusted to certain window/level. default='RAW'.</p>
</li>
<li>
<p>wl (tuple or list of tuples, optional): value of Window/Levelto be used. If mode is set to 'WIN' then wl takes the format (level, window). If mode is set to 'MWIN' then wl takes the format [(level1, window1), (level2, window2), (level3, window3)]. default=None.</p>
</li>
<li>
<p>balance_class (bollean, optional): True to perform oversampling in the train dataset to solve class imbalance. default=False.</p>
</li>
<li>
<p>balance_class_method (string, optional): methodology used to balance classes. Options={'upsample', 'downsample'}. default='upsample'.</p>
</li>
<li>
<p>normalize (bollean, optional): Normalizes all datasets by a specified mean and standard deviation. Since most of the used CNN architectures assumes 3 channel input, this follows the following format ((mean, mean, mean), (std, std, std)). default=False.</p>
</li>
<li>
<p>batch_size (integer, optional): Batch size for dataloader. defult=16.</p>
</li>
<li>
<p>num_workers (integer, optional): Number of CPU workers for dataloader. default=0.</p>
</li>
<li>
<p>sampling (float, optional): fraction of the whole dataset to be used. default=1.0.</p>
</li>
<li>
<p>test_percent (float, optional): percentage of data for testing.default=0.2.</p>
</li>
<li>
<p>valid_percent (float, optional): percentage of data for validation (ONLY with NN_Classifier) .default=0.2.</p>
</li>
<li>
<p>custom_resize (integer, optional): By default, the data processor resizes the image in dataset into the size expected bu the different CNN architectures. To override this and use a custom resize, set this to desired value. default=False.</p>
</li>
<li>
<p>model_arch (string, required): CNN model architecture that this data will be used for. Used to resize images as detailed above. default='alexnet' .</p>
</li>
<li>
<p>type (string, required): type of classifier that will be used. please refer to classifier object type. default='nn_classifier'.</p>
</li>
<li>
<p>device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'.</p>
</li>
<li>
<p>transformations (list, optional): list of pytorch transformations to be applied to all datasets. By default, the images are resized, channels added up to 3 and greyscaled. default='default'.</p>
</li>
<li>
<p>extra_transformations (list, optional): list of pytorch transformations to be extra added to train dataset specifically. default=None.</p>
</li>
</ul>
<p><strong>Methods</strong></p>
<p><em>.classes()</em></p>
<ul>
<li>Returns dictionary of classes/class_idx in data.</li>
</ul>
<p><em>.info()</em></p>
<ul>
<li>Returns full information of the data processor object.</li>
</ul>
<p><em>.dataset_info(plot=True, figure_size=(500,300))</em></p>
<ul>
<li>
<p>Displays information of the data and class breakdown.</p>
</li>
<li>
<p>Parameters:</p>
<ul>
<li>
<p>plot (boolean, optional): True to display data as graph. False to display in table format. default=True</p>
</li>
<li>
<p>figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(500,300)</p>
</li>
</ul>
</li>
</ul>
<p><em>.sample(figure_size=(10,10), show_labels=True, show_file_name=False)</em></p>
<ul>
<li>
<p>Displays a sample from the training dataset. Number of images displayed is the same as batch size.</p>
</li>
<li>
<p>Parameters:</p>
<ul>
<li>
<p>figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(10,10)</p>
</li>
<li>
<p>show_label (boolean, optional): show labels above images. default=True</p>
</li>
<li>
<p>show_file_names (boolean, optional): show file path above image. default=False</p>
</li>
</ul>
</li>
</ul>
<p><em>.check_leak(show_file=False)</em></p>
<ul>
<li>
<p>Checks possible overlap between train and test dataset files.</p>
</li>
<li>
<p>Parameters:</p>
<ul>
<li>show_file (boolean, optional): display table of leaked/common files between train and test. default=False.</li>
</ul>
</li>
</ul>
<p><em>.export(output_path)</em></p>
<ul>
<li>
<p>Exports the Dtaprocessor object for future use.</p>
</li>
<li>
<p>Parameters:</p>
<ul>
<li>output_path (string, required): output file path.  </li>
</ul>
</li>
</ul>
</div>
<h3 id="feature_extractor">Feature_Extractor</h3>
<pre><code>core.Feature_Extractor(model_arch, dataloader,pre_trained=True, unfreeze=False,
                       device='auto',)
</code></pre>
<div class="admonition quote">
<p>Creates a feature extractor neural network using one of the famous CNN architectures and the data provided as dataloader from Data_Processor.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p>model_arch (string, required): CNN architecture to be utilized. To see list of supported architectures see settings.</p>
</li>
<li>
<p>pre_trained (boolean, optional): Initialize with ImageNet pretrained weights or not. default=True.</p>
</li>
<li>
<p>unfreeze (boolean, required): Unfreeze all layers of network for future retraining. default=False.</p>
</li>
<li>
<p>dataloader (pytorch dataloader object, required): the dataloader that will be used to supply data for feature extraction.</p>
</li>
<li>
<p>device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'.</p>
</li>
</ul>
<div align='center'>

<table>
<thead>
<tr>
<th>Model Architecture</th>
<th align="center">Default Input Image Size</th>
<th align="center">Output Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>vgg11</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>vgg13</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>vgg16</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>vgg19</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>vgg11_bn</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>vgg13_bn</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>vgg16_bn</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>vgg19_bn</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>resnet18</td>
<td align="center">224 x 224</td>
<td align="center">512</td>
</tr>
<tr>
<td>resnet34</td>
<td align="center">224 x 224</td>
<td align="center">512</td>
</tr>
<tr>
<td>resnet50</td>
<td align="center">224 x 224</td>
<td align="center">2048</td>
</tr>
<tr>
<td>resnet101</td>
<td align="center">224 x 224</td>
<td align="center">2048</td>
</tr>
<tr>
<td>resnet152</td>
<td align="center">224 x 224</td>
<td align="center">2048</td>
</tr>
<tr>
<td>wide_resnet50_2</td>
<td align="center">224 x 224</td>
<td align="center">2048</td>
</tr>
<tr>
<td>wide_resnet101_2</td>
<td align="center">224 x 224</td>
<td align="center">2048</td>
</tr>
<tr>
<td>alexnet</td>
<td align="center">256 x 256</td>
<td align="center">4096</td>
</tr>
</tbody>
</table>
</div>

<p><strong>Returns</strong></p>
<p>Pandas dataframe with extracted features.</p>
<p><strong>Methods</strong>  </p>
<p><em>.num_features()</em></p>
<ul>
<li>Returns the number of features to be extracted.</li>
</ul>
<p><em>.run()</em></p>
<ul>
<li>Runs the feature extraction process. Returns tuple of feature_table (dataframe which contains all features, labels and image file path), features (dataframe which contains features only), feature_names(list of feature names)</li>
</ul>
<p><em>.export_features(csv_path)</em></p>
<ul>
<li>
<p>Exports extracted features into csv file.</p>
</li>
<li>
<p>Parameters:</p>
<ul>
<li>csv_path (string, required): path to csv output.</li>
</ul>
</li>
</ul>
<p><em>.plot_extracted_features(num_features=100, num_images=100,image_path_column='IMAGE_PATH', image_label_column='IMAGE_LABEL')</em></p>
<ul>
<li>
<p>Plots Extracted Features in Heatmap</p>
</li>
<li>
<p>Parameters:</p>
<ul>
<li>
<p>num_features (integer, optional): number of features to display. default=100</p>
</li>
<li>
<p>num_images (integer, optional): number of images to display features for. default=100</p>
</li>
<li>
<p>image_path_column (string, required): name of column that has image names/path. default='IMAGE_PATH'</p>
</li>
<li>
<p>image_label_column (string, required): name of column that has image labels. default='IMAGE_LABEL'</p>
</li>
</ul>
</li>
</ul>
<p><em>.export(output_path)</em></p>
<ul>
<li>
<p>Exports the Feature Extractor object for future use.</p>
</li>
<li>
<p>Parameters:</p>
<ul>
<li>output_path (string, required): output file path.</li>
</ul>
</li>
</ul>
</div>
<h3 id="classifier">Classifier</h3>
<pre><code>core.Classifier(extracted_feature_dictionary, feature_table=None,
                image_label_column=None, image_path_column=None,
                test_percent=None, type='logistic_regression',
                interaction_terms=False, parameters={},
                cv=True, stratified=True, num_splits=5)
</code></pre>
<div class="admonition quote">
<p><strong>Description</strong></p>
<p>Image Classification Class. Performs Binary/Multiclass classification using features extracted via Feature Extractor or Supplied by user.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p>extracted_feature_dictionary (dictionary, required): Dictionary of features/labels datasets to be used for classification. This follows the following format :
{
    'train':
            {'features':dataframe, 'feature_names':list, 'labels': list}},
    'test':
            {'features':dataframe, 'feature_names':list, 'labels': list}},
}</p>
</li>
<li>
<p>feature_table (string, optional): path to csv table with user selected image paths, labels and features. default=None.</p>
</li>
<li>
<p>image_label_column (string, required if using feature_table): name of the column with images labels.default=None.</p>
</li>
<li>
<p>image_path_column (string, requried if using feature_table): name of column with images paths.default=None.</p>
</li>
<li>
<p>test_percent (float, required if using feature_table): percentage of data for testing.default=None.</p>
</li>
<li>
<p>type (string, required): type of classifier. For complete list refer to settings. default='logistic_regression'.</p>
</li>
<li>
<p>interaction_terms (boolean, optional): create interaction terms between different features and add them as new features to feature table. default=False.</p>
</li>
<li>
<p>cv (boolean, required): True for cross validation. default=True.</p>
</li>
<li>
<p>stratified (boolean, required): True for stratified cross validation. default=True.</p>
</li>
<li>
<p>num_splits (integer, required): Number of K-fold cross validation splits. default=5.</p>
</li>
<li>
<p>parameters (dictionary, optional): optional parameters passed to the classifier. Please refer to sci-kit learn documentation.</p>
</li>
</ul>
<p><strong>Methods</strong></p>
<p><em>.info()</em></p>
<ul>
<li>Returns table of different classifier parameters/properties.</li>
</ul>
<p><em>.run()</em></p>
<ul>
<li>Runs Image Classifier.</li>
</ul>
<p><em>.average_cv_accuracy()</em></p>
<ul>
<li>Returns average cross validation accuracy.</li>
</ul>
<p><em>.test_accuracy()</em></p>
<ul>
<li>Returns accuracy of trained classifier on test dataset.</li>
</ul>
<p><em>.confusion_matrix(title='Confusion Matrix',cmap=None,normalize=False,figure_size=(8,6))</em></p>
<ul>
<li>
<p>Displays confusion matrix using trained classifier and test dataset.</p>
</li>
<li>
<p>Parameters:</p>
<ul>
<li>
<p>title (string, optional): name to be displayed over confusion matrix.</p>
</li>
<li>
<p>cmap (string, optional): colormap of the displayed confusion matrix. This follows matplot color palletes. default=None.</p>
</li>
<li>
<p>normalize (boolean, optional): normalize values. default=False.</p>
</li>
<li>
<p>figure_size (tuple, optional): size of the figure as width, height. default=(8,6)</p>
</li>
</ul>
</li>
</ul>
<p><em>.roc()</em></p>
<ul>
<li>Display ROC and AUC of trained classifier and test dataset.</li>
</ul>
<p><em>.predict(input_image_path, all_predictions=False)</em></p>
<ul>
<li>
<p>Returns label prediction of a target image using a trained classifier. This works as part of pipeline only for now.</p>
</li>
<li>
<p>Parameters:</p>
<ul>
<li>
<p>input_image_path (string, required): path of target image.</p>
</li>
<li>
<p>all_predictions (boolean, optional): return a table of all predictions for all possible labels.</p>
</li>
</ul>
</li>
</ul>
<p><em>.export()</em></p>
<ul>
<li>Exports the Classifier object for future use. output_path (string, required): output file path.</li>
</ul>
<p><em>.export_trained_classifier()</em></p>
<ul>
<li>Exports only the trained classifier for future use. output_path (string, required): output file path.</li>
</ul>
</div>
<h3 id="nn_classifier">NN_Classifier</h3>
<pre><code>core.NN_Classifier(feature_extractor, data_processor, unfreeze=False,
                  learning_rate=0.0001, epochs=10, optimizer='Adam',
                  loss_function='CrossEntropyLoss', lr_scheduler=None,
                  batch_size=16, device='auto', custom_nn_classifier=None,
                  loss_function_parameters={}, optimizer_parameters={},)
</code></pre>
<div class="admonition quote">
<p><strong>Description</strong></p>
<p>Neural Network Classifier. This serves as extension of pytorch neural network modules e.g. VGG16, for fine tuning or transfer learning.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p>data_processor (radtorch.core.data_processor, required): data processor object from radtorch.core.Data_Processor.</p>
</li>
<li>
<p>feature_extractor (radtorch.core.feature_extractor, required): feature_extractor object from radtorch.core.Feature_Extractor.</p>
</li>
<li>
<p>unfreeze (boolean, optional): True to unfreeze the weights of all layers in the neural network model for model finetuning. False to just use unfreezed final layers for transfer learning. default=False.</p>
</li>
<li>
<p>learning_rate (float, required): Learning rate. default=0.0001.</p>
</li>
<li>
<p>epochs (integer, required): training epochs. default=10.</p>
</li>
<li>
<p>optimizer (string, required): neural network optimizer type. Please see radtorch.settings for list of approved optimizers. default='Adam'.</p>
</li>
<li>
<p>optimizer_parameters (dictionary, optional): optional extra parameters for optimizer as per pytorch documentation.</p>
</li>
<li>
<p>loss_function (string, required): neural network loss function. Please see radtorch.settings for list of approved loss functions. default='CrossEntropyLoss'.</p>
</li>
<li>
<p>loss_function_parameters (dictionary, optional): optional extra parameters for loss function as per pytorch documentation.</p>
</li>
<li>
<p>lr_scheduler (string, optional): learning rate scheduler - upcoming soon.</p>
</li>
<li>
<p>batch_size (integer, required): batch size. default=16</p>
</li>
<li>
<p>custom_nn_classifier (pytorch model, optional): Option to use a custom made neural network classifier that will be added after feature extracted layers. default=None.</p>
</li>
<li>
<p>device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'.</p>
</li>
</ul>
<p><strong>Methods</strong></p>
<p><em>.info()</em></p>
<ul>
<li>Returns table of different classifier parameters/properties.</li>
</ul>
<p><em>.run()</em></p>
<ul>
<li>Runs Image Classifier.</li>
</ul>
<p><em>.confusion_matrix(target_dataset=None, figure_size=(8,6), cmap=None)</em></p>
<ul>
<li>
<p>Displays confusion matrix for trained nn_classifier on test dataset.</p>
</li>
<li>
<p>Parameters:</p>
<ul>
<li>
<p>target_dataset (pytorch dataset, optional): this option can be used to test the trained model on an external test dataset. If set to None, the confusion matrix is generated using the test dataset initially specified in the data_processor. default=None.</p>
</li>
<li>
<p>figure_size (tuple, optional): size of the figure as width, height. default=(8,6)</p>
</li>
</ul>
</li>
</ul>
<p><em>.roc()</em></p>
<ul>
<li>Display ROC and AUC of trained model and test dataset.</li>
</ul>
<p><em>.metrics(figure_size=(700,400))</em></p>
<ul>
<li>Displays graphical representation of train/validation loss /accuracy.</li>
</ul>
<p><em>.predict(input_image_path, all_predictions=True)</em></p>
<ul>
<li>
<p>Displays classs prediction for a target image using a trained classifier.</p>
</li>
<li>
<p>Parameters:</p>
<ul>
<li>
<p>input_image_path (string, required): path to target image.</p>
</li>
<li>
<p>all_predictions (boolean, optional): True to display prediction percentage accuracies for all prediction classes. default=True.</p>
</li>
</ul>
</li>
</ul>
<p><em>.misclassified(num_of_images=4, figure_size=(5,5), table=False)</em></p>
<ul>
<li>
<p>Displays sample of images misclassified by the classifier from test dataset.</p>
</li>
<li>
<p>Parameters:</p>
<ul>
<li>
<p>num_of_images (integer, optional): number of images to be displayed. default=4.</p>
</li>
<li>
<p>figure_size (tuple, optional): size of the figure as width, height. default=(5,5).</p>
</li>
<li>
<p>table (boolean, optional): True to display a table of all misclassified images including image path, true label and predicted label.</p>
</li>
</ul>
</li>
</ul>
</div>
<h2 id="generative-adversarial-networks">Generative Adversarial Networks</h2>
<h3 id="dcgan_discriminator">DCGAN_Discriminator</h3>
<pre><code>core.DCGAN_Discriminator(num_input_channels, kernel_size, num_discriminator_features,
                         input_image_size, device='auto')
</code></pre>
<div class="admonition quote">
<p><strong>Description</strong></p>
<p>Core Deep Convolutional GAN Discriminator Network.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p>kernel_size (integer, required): size of kernel/filter to be used for convolution.</p>
</li>
<li>
<p>num_discriminator_features (integer, required): number of features/convolutions for discriminator network.</p>
</li>
<li>
<p>num_input_channels (integer, required): number of channels for input image.</p>
</li>
<li>
<p>input_image_size (integer, required): size of input image.</p>
</li>
<li>
<p>device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'.</p>
</li>
</ul>
</div>
<h3 id="dcgan_generator">DCGAN_Generator</h3>
<pre><code>core.DCGAN_Generator(noise_size, num_generator_features, num_output_channels,
                    target_image_size, device='auto')
</code></pre>
<div class="admonition quote">
<p><strong>Description</strong></p>
<p>Core Deep Convolutional GAN Generator Network.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p>noise_size (integer, required): size of the noise sample to be generated.</p>
</li>
<li>
<p>num_generator_features (integer, required): number of features/convolutions for generator network.</p>
</li>
<li>
<p>num_output_channels (integer, required): number of channels for output image.</p>
</li>
<li>
<p>target_image_size (integer, required): size of output image.</p>
</li>
<li>
<p>device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'.</p>
</li>
</ul>
</div>
<h3 id="gan_discriminator">GAN_Discriminator</h3>
<pre><code>  core.GAN_Discriminator(input_image_size, intput_num_channels, device='auto')
</code></pre>
<div class="admonition quote">
<p><strong>Description</strong></p>
<p>Core Vanilla GAN Discriminator Network.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p>num_input_channels (integer, required): number of channels for input image.</p>
</li>
<li>
<p>input_image_size (integer, required): size of input image.</p>
</li>
<li>
<p>device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'.</p>
</li>
</ul>
</div>
<h3 id="gan_generator">GAN_Generator</h3>
<pre><code>  core.GAN_Generator(noise_size, target_image_size, output_num_channels, device='auto')
</code></pre>
<div class="admonition quote">
<p><strong>Description</strong></p>
<p>Core Vanilla Convolutional GAN Generator Network.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p>noise_size (integer, required): size of the noise sample to be generated.</p>
</li>
<li>
<p>num_output_channels (integer, required): number of channels for output image.</p>
</li>
<li>
<p>target_image_size (integer, required): size of output image.</p>
</li>
<li>
<p>device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'.</p>
</li>
</ul>
</div>
<p><small> Documentation Update: 5/14/2020 </small></p>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="Footer">
        
          <a href="../install/" title="Installation" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Installation
              </div>
            </div>
          </a>
        
        
          <a href="../pipeline/" title="radtorch.pipeline" class="md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                radtorch.pipeline
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4,11V13H16L10.5,18.5L11.92,19.92L19.84,12L11.92,4.08L10.5,5.5L16,11H4Z" /></svg>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/vendor.36cbf620.min.js"></script>
      <script src="../assets/javascripts/bundle.00c583dd.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents"}</script>
      
      <script>
        app = initialize({
          base: "..",
          features: ["instant"],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.7f7c8775.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
    
  </body>
</html>