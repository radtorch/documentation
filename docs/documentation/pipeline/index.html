


<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="shortcut icon" href="../img/radtorch_icon.ico">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-5.5.0">
    
    
      
        <title>Pipeline - RADTorch - API Documentation</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.b5d04df8.min.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/palette.9ab2c1f8.min.css">
      
      
        
        
        <meta name="theme-color" content="">
      
    
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    
      
        
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-116382803-2","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}),document.addEventListener("DOMContentSwitch",function(){ga("send","pageview",document.location.pathname)})</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
  
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="black" data-md-color-accent="deep-orange">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#pipeline-module-radtorchpipeline" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href=".." title="RADTorch - API Documentation" class="md-header-nav__button md-logo" aria-label="RADTorch - API Documentation">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 22a2 2 0 002-2V4a2 2 0 00-2-2h-6v7L9.5 7.5 7 9V2H6a2 2 0 00-2 2v16a2 2 0 002 2h12z"/></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            RADTorch - API Documentation
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              Pipeline
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/radtorch/radtorch/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    radtorch/radtorch
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
        
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="RADTorch - API Documentation" class="md-nav__button md-logo" aria-label="RADTorch - API Documentation">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 22a2 2 0 002-2V4a2 2 0 00-2-2h-6v7L9.5 7.5 7 9V2H6a2 2 0 00-2 2v16a2 2 0 002 2h12z"/></svg>

    </a>
    RADTorch - API Documentation
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/radtorch/radtorch/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    radtorch/radtorch
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../install/" title="Installation" class="md-nav__link">
      Installation
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../start/" title="Getting Started" class="md-nav__link">
      Getting Started
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Pipeline
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 9h14V7H3v2m0 4h14v-2H3v2m0 4h14v-2H3v2m16 0h2v-2h-2v2m0-10v2h2V7h-2m0 6h2v-2h-2v2z"/></svg>
        </span>
      </label>
    
    <a href="./" title="Pipeline" class="md-nav__link md-nav__link--active">
      Pipeline
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#import" class="md-nav__link">
    Import
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-classification" class="md-nav__link">
    Image Classification
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hybrid-image-classification" class="md-nav__link">
    Hybrid Image Classification
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#feature-extraction" class="md-nav__link">
    Feature Extraction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generative-adversarial-networks" class="md-nav__link">
    Generative Adversarial Networks
  </a>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../core/" title="Core" class="md-nav__link">
      Core
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../involve/" title="Get Involved" class="md-nav__link">
      Get Involved
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../copyright/" title="Copyrights" class="md-nav__link">
      Copyrights
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../license/" title="License" class="md-nav__link">
      License
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#import" class="md-nav__link">
    Import
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-classification" class="md-nav__link">
    Image Classification
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hybrid-image-classification" class="md-nav__link">
    Hybrid Image Classification
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#feature-extraction" class="md-nav__link">
    Feature Extraction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generative-adversarial-networks" class="md-nav__link">
    Generative Adversarial Networks
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  
                
                
                <h1 id="pipeline-module-radtorchpipeline">Pipeline Module <small> (radtorch.pipeline) </small></h1>
<p style='text-align: justify;'>
Pipelines are the most exciting feature of RADTorch Framework. With only few lines of code, pipelines allow you to run state-of-the-art machine learning algorithms and much more.
</p>

<p style='text-align: justify;'>
RADTorch follows principles of <b>object-oriented-programming</b> (OOP) in the sense that RADTorch pipelines are made of <a href='../core/'>core building blocks</a> and each of these blocks has specific functions/methods that can be accessed accordingly.
</p>

<p style='text-align: justify;'>

For example,
</p>

<pre><code>pipeline.Image_Classification.data_processor.dataset_info()
</code></pre>
<p style='text-align: justify;'>
can be used to access the dataset information for that particular Image Classification pipeline.
</p>

<h2 id="import">Import</h2>
<pre><code>from radtorch import pipeline
</code></pre>
<h2 id="image-classification">Image Classification</h2>
<div class="highlight"><pre><span></span><code>pipeline.Image_Classification(
              data_directory, name=None, table=None,
              image_path_column=&#39;IMAGE_PATH&#39;, image_label_column=&#39;IMAGE_LABEL&#39;,is_path=True,
              is_dicom=False, mode=&#39;RAW&#39;, wl=None, custom_resize=False,
              balance_class=False, balance_class_method=&#39;upsample&#39;,
              interaction_terms=False, normalize=((0,0,0), (1,1,1)),
              batch_size=16, num_workers=0,
              sampling=1.0, test_percent=0.2, valid_percent=0.2,
              model_arch=&#39;resnet50&#39;, pre_trained=True, unfreeze=False,
              type=&#39;nn_classifier&#39;, custom_nn_classifier=None,
              cv=True, stratified=True, num_splits=5, parameters={},
              learning_rate=0.0001, epochs=10, lr_scheduler=None,
              optimizer=&#39;Adam&#39;, loss_function=&#39;CrossEntropyLoss&#39;,
              loss_function_parameters={}, optimizer_parameters={},
              transformations=&#39;default&#39;, extra_transformations=None,
              device=&#39;auto&#39;,auto_save=False)
</code></pre></div>

<!-- !!! quote "" -->

<p><strong>Description</strong></p>
<p>Complete end-to-end image classification pipeline.</p>
<p><strong>Parameters</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Description</th>
<th>Default Value</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>General Parameters</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>name</td>
<td>string, optional</td>
<td>name to be given to this classifier pipeline</td>
<td></td>
</tr>
<tr>
<td>data_directory</td>
<td>string, required</td>
<td>path to target data directory/folder</td>
<td></td>
</tr>
<tr>
<td>is_dicom</td>
<td>boolean, optional</td>
<td>True if images are DICOM</td>
<td>False</td>
</tr>
<tr>
<td>table</td>
<td>string or pandas dataframe, optional</td>
<td>path to label table csv or name of pandas data table</td>
<td>None</td>
</tr>
<tr>
<td>image_path_column</td>
<td>string, optional</td>
<td>name of column that has image path/image file name.</td>
<td>'IMAGE_PATH'</td>
</tr>
<tr>
<td>image_label_column</td>
<td>string, optional</td>
<td>name of column that has image label.</td>
<td>'IMAGE_LABEL'</td>
</tr>
<tr>
<td>is_path</td>
<td>boolean, optional</td>
<td>True if file_path column in table is file path.  If False, this assumes that the column contains file names only and will append the data_directory to all files</td>
<td>True</td>
</tr>
<tr>
<td>mode</td>
<td>string, optional</td>
<td>mode of handling pixel values from DICOM to numpy array. Option={'RAW': raw pixel values, 'HU': converts pixel values to HU using slope and intercept, 'WIN':Applies a certain window/level to HU converted DICOM image, 'MWIN': converts DICOM image to 3 channel HU numpy array with each channel adjusted to certain window/level</td>
<td>'RAW'</td>
</tr>
<tr>
<td>wl</td>
<td>tuple or list of tuples, optional</td>
<td>value of Window/Level to be used. If mode is set to 'WIN' then wl takes the format (level, window). If mode is set to 'MWIN' then wl takes the format [(level1, window1), (level2, window2), (level3, window3)]</td>
<td></td>
</tr>
<tr>
<td>balance_class</td>
<td>boolean, optional</td>
<td>True to perform oversampling in the train dataset to solve class imbalance</td>
<td>True</td>
</tr>
<tr>
<td>balance_class_method</td>
<td>string, optional</td>
<td>methodology used to balance classes. Options={'upsample', 'downsample'}</td>
<td>'upsample'</td>
</tr>
<tr>
<td>interaction_terms</td>
<td>boolean, optional</td>
<td>create interaction terms between different features and add them as new features to feature table</td>
<td>False</td>
</tr>
<tr>
<td>normalize</td>
<td>boolean/False or Tuple, optional</td>
<td>normalizes all datasets by a specified mean and standard deviation. Since most of the used CNN architectures assumes 3 channel input, this follows the following format ((mean, mean, mean), (std, std, std))</td>
<td>((0,0,0), (1,1,1))</td>
</tr>
<tr>
<td>batch_size</td>
<td>integer, optional</td>
<td>batch size for dataloader</td>
<td>16</td>
</tr>
<tr>
<td>num_workers</td>
<td>integer, optional</td>
<td>number of CPU workers for dataloader</td>
<td>0</td>
</tr>
<tr>
<td>sampling</td>
<td>float, optional</td>
<td>fraction of the whole dataset to be used</td>
<td>1.0</td>
</tr>
<tr>
<td>test_percent</td>
<td>float, optional</td>
<td>percentage of data for testing</td>
<td>0.2</td>
</tr>
<tr>
<td>valid_percent</td>
<td>float, optional</td>
<td>percentage of data for validation</td>
<td>0.2</td>
</tr>
<tr>
<td>custom_resize</td>
<td>integer, optional</td>
<td>by default, the data processor resizes the image in dataset into the size expected bu the different CNN architectures. To override this and use a custom resize, set this to desired value</td>
<td>False</td>
</tr>
<tr>
<td>transformations</td>
<td>list, optional</td>
<td>list of pytorch transformations to be applied to all datasets. By default, the images are resized, channels added up to 3 and greyscaled</td>
<td>'default'</td>
</tr>
<tr>
<td>extra_transformations</td>
<td>list, optional</td>
<td>list of pytorch transformations to be extra added to train dataset specifically</td>
<td>None</td>
</tr>
<tr>
<td>model_arch</td>
<td>string, required</td>
<td>CNN model architecture that this data will be used for default image resize, feature extraction and model training (model training only if type is nn_classifier)</td>
<td>'resnet50'</td>
</tr>
<tr>
<td>pre_trained</td>
<td>boolean, optional</td>
<td>initialize CNN with ImageNet pretrained weights or not</td>
<td>True</td>
</tr>
<tr>
<td>unfreeze</td>
<td>boolean, required</td>
<td>unfreeze all layers of network for retraining</td>
<td>False</td>
</tr>
<tr>
<td>device</td>
<td>string, optional</td>
<td>device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}</td>
<td>'auto'</td>
</tr>
<tr>
<td>type</td>
<td>string, required</td>
<td>type of classifier. For complete list refer to settings</td>
<td>'logistic_regression'</td>
</tr>
<tr>
<td><strong>Classifier specific parameters</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>cv</td>
<td>boolean, required</td>
<td>True for cross validation</td>
<td>True</td>
</tr>
<tr>
<td>stratified</td>
<td>boolean, required</td>
<td>True for stratified cross validation</td>
<td>True</td>
</tr>
<tr>
<td>num_splits</td>
<td>integer, required</td>
<td>number of K-fold cross validation splits</td>
<td>5</td>
</tr>
<tr>
<td>parameters</td>
<td>dictionary, optional</td>
<td>optional user-specified parameters passed to the classifier. Please refer to sci-kit-learn documentation.</td>
<td></td>
</tr>
<tr>
<td><strong>NN_Classifier specific parameters</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>learning_rate</td>
<td>float, required</td>
<td>CNN learning rate</td>
<td>0.0001</td>
</tr>
<tr>
<td>epochs</td>
<td>integer, required</td>
<td>number of training epochs</td>
<td>10</td>
</tr>
<tr>
<td>optimizer</td>
<td>string, required</td>
<td>neural network optimizer type. Please see radtorch.settings for list of approved optimizers</td>
<td>'Adam'</td>
</tr>
<tr>
<td>optimizer_parameters</td>
<td>dictionary, optional</td>
<td>optional user-specific extra parameters for optimizer as per pytorch documentation.</td>
<td></td>
</tr>
<tr>
<td>loss_function</td>
<td>string, required</td>
<td>neural network loss function. Please see radtorch.settings for list of approved loss functions</td>
<td>'CrossEntropyLoss'</td>
</tr>
<tr>
<td>loss_function_parameters</td>
<td>dictionary, optional</td>
<td>optional user-specific extra parameters for loss function as per pytorch documentation.</td>
<td></td>
</tr>
<tr>
<td>custom_nn_classifier</td>
<td>pytorch model, optional</td>
<td>Option to use a custom made neural network classifier that will be added after feature extracted layers</td>
<td>None</td>
</tr>
<tr>
<td><strong>Beta</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>lr_scheduler</td>
<td>string, optional</td>
<td><strong><em>in progress</em></strong></td>
<td></td>
</tr>
<tr>
<td>auto_save</td>
<td>boolean, optional</td>
<td><strong><em>in progress</em></strong></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Methods</strong></p>
<p>In addition to <a href="../core/">core component methods</a>, image classification pipeline specific methods include:</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Description</th>
<th>Parameters</th>
<th>Default Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>.info()</td>
<td>show information of the image classification pipeline.</td>
<td></td>
<td></td>
</tr>
<tr>
<td>.run()</td>
<td>starts the image classification pipeline training.</td>
<td></td>
<td></td>
</tr>
<tr>
<td>.metrics(figure_size=(700, 350)</td>
<td>displays the training metrics of the image classification pipeline.</td>
<td>figure_size: Size of display figure</td>
<td>(700,350)</td>
</tr>
<tr>
<td>.export(output_path)</td>
<td>exports the pipeline to output path.</td>
<td>output_path: path to exported pipeline file</td>
<td></td>
</tr>
<tr>
<td>.cam(target_image_path, target_layer, type='scorecam', figure_size=(10,5), cmap='jet', alpha=0.5)</td>
<td>diplays class activation maps from a specific layer of the trained model on a target image</td>
<td>target_image_path: path to target image <br> <br> target_layer: target layer in trained model <br> <br> type: cam type . Options = 'cam', 'gradcam', 'gradcampp', 'smoothgradcampp', 'scorecam' <br> <br> figure_size: size of display figure <br> <br> cmap: color map <br> <br> alpha: overlay alpha</td>
<td><br> <br> <br>  <br> <br> <br>  'scorecam'  <br> <br> <br> <br>  <br><br> (10,5) <br><br><br> 'jet' <br><br> 0.5</td>
</tr>
<tr>
<td><strong>Beta</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>.deploy</td>
<td></td>
<td><strong><em>in progress</em></strong></td>
<td></td>
</tr>
</tbody>
</table>
<div class="admonition info">
<p class="admonition-title">Visualize Class Activatin Maps (CAM) for a trained image classification pipeline</p>
<p><strong>Requirements:</strong></p>
<ol>
<li>
<p>Trained image classification pipeline.</p>
</li>
<li>
<p>Select a target image.</p>
</li>
<li>
<p>Identify a target layer in the trained model. This is done using the <strong>show_model_layers</strong> method of the <a href="../core/#nn_classifier">nn_classifier core module</a>.</p>
</li>
<li>
<p>Use the <strong>.cam</strong> method.</p>
</li>
</ol>
<p><strong>Example</strong></p>
<p>Assume that your trained image classifier pipeline is <strong><em>clf</em></strong> and the target image is '/image/test.png'. To select the target layer use:</p>
<div class="highlight"><pre><span></span><code>clf.classifier.show_model_layers()
</code></pre></div>

<p>Then use the following method to show CAM :</p>
<p><div class="highlight"><pre><span></span><code>clf.cam(target_image_path=&#39;/image/test.png&#39;, target_layer=clf.trained_model.layer4[2].conv3)
</code></pre></div>
  <div style="text-align:center"><img src="../img/cam_example.png" /></div></p>
</div>
<h2 id="hybrid-image-classification">Hybrid Image Classification</h2>
<div class="highlight"><pre><span></span><code>pipeline.Hybrid_Image_Classification(
              data_directory, name=None, table=None,
              image_path_column=&#39;IMAGE_PATH&#39;, image_label_column=&#39;IMAGE_LABEL&#39;,is_path=True,
              is_dicom=False, mode=&#39;RAW&#39;, wl=None, custom_resize=False,
              balance_class=False, balance_class_method=&#39;upsample&#39;,
              interaction_terms=False, normalize=((0,0,0), (1,1,1)),
              batch_size=16, num_workers=0,
              sampling=1.0, test_percent=0.2, valid_percent=0.2,
              model_arch=&#39;resnet50&#39;, pre_trained=True, unfreeze=False,
              type=&#39;xgboost&#39;
              cv=True, stratified=True, num_splits=5, parameters={},
              transformations=&#39;default&#39;, extra_transformations=None,
              device=&#39;auto&#39;,auto_save=False)
</code></pre></div>

<p><strong>Description</strong></p>
<p>Complete end-to-end image classification pipeline that uses combination of automatically generated imaging features and user provided clinical/laboratory features.</p>
<p><strong>Parameters</strong></p>
<p>The hybrid image classification pipeline uses the same parameters as the <a href="#Image-classification">Image_Classification</a> pipeline.</p>
<div class="admonition warning">
<p class="admonition-title">Format of provided data table for hybrid model</p>
<p>The Hybrid Image Classification pipeline expects the data table to have the following headers :</p>
<ol>
<li>
<p>Column for Image Path (same as the one specified in 'image_label_path')</p>
</li>
<li>
<p>Column for Image Label (same as the one specified in 'image_label_column')</p>
</li>
<li>
<p>Columns for clinical/laboratory features.</p>
</li>
</ol>
</div>
<div class="admonition danger">
<p class="admonition-title">Supported classifiers</p>
<p>The Hybrid Image Classification pipeline uses CNNs for automatic imaging feature extraction. Training of CNNs (nn_classifier) using hybrid model is not yet supported.</p>
</div>
<div class="admonition info">
<p class="admonition-title">Handling Categorical and Numerical Clinical/Laboratory Data</p>
<p>The hybrid image classification pipeline is capable of automatically handling categorical and numerical clinical data provided by the user.</p>
<p>For example, if the data provided looks like that :</p>
<p><div style="text-align:center"><img src="/img/hybrid_1.png" /></div></p>
<p>the pipeline will automatically detect types of variables and modify accordingly to get this:</p>
<p><div style="text-align:center"><img src="/img/hybrid_2.png" /></div></p>
</div>
<p><strong>Methods</strong></p>
<p>Hybrid image classification pipeline follows the same methods as <a href="#Image-classification">Image_Classification</a> pipeline.</p>
<h2 id="feature-extraction">Feature Extraction</h2>
<div class="highlight"><pre><span></span><code>pipeline.Feature_Extraction(
                name=None, data_directory, table=None, is_dicom=True,
                normalize=((0, 0, 0), (1, 1, 1)),
                batch_size=16, num_workers=0,
                model_arch=&#39;resnet50&#39;, custom_resize=False,
                pre_trained=True, unfreeze=False,
                label_column=&#39;IMAGE_LABEL&#39;)
</code></pre></div>

<p><strong>Description</strong></p>
<p>Image Feature Extraction Pipeline.</p>
<p><strong>Parameters</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Description</th>
<th>Default Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>string, optional</td>
<td>name to be given to this classifier pipeline</td>
<td></td>
</tr>
<tr>
<td>data_directory</td>
<td>string, required</td>
<td>path to target data directory/folder</td>
<td></td>
</tr>
<tr>
<td>table</td>
<td>string or pandas dataframe, required</td>
<td>path to label table csv or name of pandas data table</td>
<td>None</td>
</tr>
<tr>
<td>is_dicom</td>
<td>boolean, required</td>
<td>True if images are DICOM</td>
<td>False</td>
</tr>
<tr>
<td>normalize</td>
<td>boolean/False or Tuple, optional</td>
<td>normalizes all datasets by a specified mean and standard deviation. Since most of the used CNN architectures assumes 3 channel input, this follows the following format ((mean, mean, mean), (std, std, std))</td>
<td>((0,0,0), (1,1,1))</td>
</tr>
<tr>
<td>batch_size</td>
<td>integer, optional</td>
<td>batch size for dataloader</td>
<td>16</td>
</tr>
<tr>
<td>num_workers</td>
<td>integer, optional</td>
<td>number of CPU workers for dataloader</td>
<td>0</td>
</tr>
<tr>
<td>model_arch</td>
<td>string, required</td>
<td>CNN model architecture that this data will be used for default image resize, feature extraction and model training (model training only if type is nn_classifier)</td>
<td>'resnet50'</td>
</tr>
<tr>
<td>custom_resize</td>
<td>integer, optional</td>
<td>by default, the data processor resizes the image in dataset into the size expected bu the different CNN architectures. To override this and use a custom resize, set this to desired value</td>
<td>False</td>
</tr>
<tr>
<td>pre_trained</td>
<td>boolean, optional</td>
<td>initialize CNN with ImageNet pretrained weights or not</td>
<td>True</td>
</tr>
<tr>
<td>unfreeze</td>
<td>boolean, required</td>
<td>unfreeze all layers of network for retraining</td>
<td>False</td>
</tr>
<tr>
<td>label_column</td>
<td>string, required</td>
<td>name of column that has image label.</td>
<td>'IMAGE_LABEL'</td>
</tr>
</tbody>
</table>
<p><strong>Methods</strong></p>
<p>In addition to <a href="../core/">core component methods</a>,  feature extraction pipeline specific methods include:</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Description</th>
<th>Parameters</th>
</tr>
</thead>
<tbody>
<tr>
<td>.info()</td>
<td>show information of the  pipeline.</td>
<td></td>
</tr>
<tr>
<td>.run()</td>
<td>starts the feature extraction pipeline.</td>
<td></td>
</tr>
<tr>
<td>.export(output_path)</td>
<td>exports the pipeline to output path.</td>
<td>output_path: path to exported pipeline file</td>
</tr>
</tbody>
</table>
<div class="admonition info">
<p class="admonition-title">Visualize Extracted Features</p>
<p>You can use the <strong>plot_extracted_features</strong> method of the <a href="../core/#feature_extractor">feature_extractor core module</a> to visualize the extracted features as below:</p>
<div class="highlight"><pre><span></span><code>Assume that your feature extraction pipeline is called X,

X.feature_extractor.plot_extracted_features(num_features=100, num_images=100)
</code></pre></div>

</div>
<h2 id="generative-adversarial-networks">Generative Adversarial Networks</h2>
<div class="highlight"><pre><span></span><code>core.GAN(
        data_directory, table=None, is_dicom=False, is_path=True,
        image_path_column=&#39;IMAGE_PATH&#39;, image_label_column=&#39;IMAGE_LABEL&#39;,
        mode=&#39;RAW&#39;, wl=None, batch_size=16, normalize=((0,0,0),(1,1,1)),
        num_workers=0, label_smooth=True, sampling=1.0, transformations=&#39;default&#39;,

        discriminator=&#39;dcgan&#39;, generator=&#39;dcgan&#39;,
        generator_noise_size=100, generator_noise_type=&#39;normal&#39;,
        discriminator_num_features=64, generator_num_features=64,
        image_size=128, image_channels=1,

        discrinimator_optimizer=&#39;Adam&#39;, generator_optimizer=&#39;Adam&#39;,
        discrinimator_optimizer_param={&#39;betas&#39;:(0.5,0.999)},
        generator_optimizer_param={&#39;betas&#39;:(0.5,0.999)},
        generator_learning_rate=0.0001, discriminator_learning_rate=0.0001,        

        epochs=10, device=&#39;auto&#39;)
</code></pre></div>

<p><strong>Description</strong></p>
<p>Generative Advarsarial Networks Pipeline.</p>
<p><strong>Parameters</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Description</th>
<th>Default Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><strong>General Parameters</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>data_directory</td>
<td>string, required</td>
<td>path to target data directory/folder</td>
<td></td>
</tr>
<tr>
<td>is_dicom</td>
<td>boolean, optional</td>
<td>True if images are DICOM</td>
<td>False</td>
</tr>
<tr>
<td>table</td>
<td>string or pandas   dataframe, optional</td>
<td>path to label table csv or name of pandas data table</td>
<td>None</td>
</tr>
<tr>
<td>image_path_column</td>
<td>string, optional</td>
<td>name of column that has image path/image file name.</td>
<td>'IMAGE_PATH'</td>
</tr>
<tr>
<td>image_label_column</td>
<td>string, optional</td>
<td>name of column that has image label.</td>
<td>'IMAGE_LABEL'</td>
</tr>
<tr>
<td>is_path</td>
<td>boolean, optional</td>
<td>True if file_path column in table is file path. If False, this assumes that the column contains file names only and will append the data_directory to all file names.</td>
<td>True</td>
</tr>
<tr>
<td>mode</td>
<td>string, optional</td>
<td>mode of handling pixel values from DICOM to numpy array. Option={'RAW': raw pixel values, 'HU':converts pixel values to HU using slope and intercept, 'WIN':Applies a   certain window/level to HU converted DICOM image, 'MWIN': converts DICOM image to 3 channel HU numpy array with each channel adjusted to certain   window/level</td>
<td>'RAW'</td>
</tr>
<tr>
<td>wl</td>
<td>tuple or list of   tuples, optional</td>
<td>value of Window/Level to be used. If mode is set to 'WIN' then wl takes the format (level, window).   If mode is set to 'MWIN' then wl takes the format [(level1, window1),   (level2, window2), (level3, window3)]</td>
<td></td>
</tr>
<tr>
<td>batch_size</td>
<td>integer, optional</td>
<td>batch size for dataloader</td>
<td>16</td>
</tr>
<tr>
<td>num_workers</td>
<td>integer, optional</td>
<td>number of CPU workers for dataloader</td>
<td>0</td>
</tr>
<tr>
<td>sampling</td>
<td>float, optional</td>
<td>fraction of the whole dataset to be used</td>
<td>1.0</td>
</tr>
<tr>
<td>transformations</td>
<td>list, optional</td>
<td>list of pytorch transformations to be applied to all datasets. By default, the images are resized, channels added up to 3 and greyscaled</td>
<td>'default'</td>
</tr>
<tr>
<td>normalize</td>
<td>boolean/False or Tuple,   optional</td>
<td>normalizes all datasets by a specified mean and standard deviation. Since most of the used CNN architectures assumes 3 channel input, this follows the following format   ((mean, mean, mean), (std, std, std))</td>
<td>((0,0,0), (1,1,1))</td>
</tr>
<tr>
<td>epochs</td>
<td>integer, required</td>
<td>number of training epochs</td>
<td>10</td>
</tr>
<tr>
<td>image_channels</td>
<td>integer, required</td>
<td>number of channels for discriminator input and generator output</td>
<td>1</td>
</tr>
<tr>
<td>image_size</td>
<td>integer, required</td>
<td>image size for discriminator input and generator output</td>
<td>128</td>
</tr>
<tr>
<td>device</td>
<td>string, optional</td>
<td>device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu,   'cuda': gpu}</td>
<td>'auto'</td>
</tr>
<tr>
<td><strong>Discriminator Parameters</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>discriminator</td>
<td>string, required</td>
<td>type of discriminator network. Options = {'dcgan', 'vanilla', 'wgan'}</td>
<td>'dcgan'</td>
</tr>
<tr>
<td>discriminator_num_features</td>
<td>integer, required</td>
<td>number of features/convolutions for discriminator network</td>
<td>64</td>
</tr>
<tr>
<td>label_smooth</td>
<td>boolean, optioanl</td>
<td>by default, labels for real images as assigned to 1. If label smoothing is set to True, lables of real images will be assigned to 0.9.   <br>(Source: <a href="https://github.com/soumith/ganhacks#6-use-soft-and-noisy-labels">https://github.com/soumith/ganhacks#6-use-soft-and-noisy-labels</a>)</td>
<td>True</td>
</tr>
<tr>
<td>discrinimator_optimizer</td>
<td>string, required</td>
<td>discriminator network optimizer type. Please see radtorch.settings for list of approved optimizers</td>
<td>'Adam'</td>
</tr>
<tr>
<td>discrinimator_optimizer_param</td>
<td>dictionary, optional</td>
<td>optional extra parameters for optimizer as per pytorch  documentation.</td>
<td>{'betas':(0.5,0.999)} <br> for Adam optimizer.</td>
</tr>
<tr>
<td>discriminator_learning_rate</td>
<td>float, required</td>
<td>discrinimator network learning rate</td>
<td>0.0001</td>
</tr>
<tr>
<td><strong>Generator Parameters</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>generator</td>
<td>string, required</td>
<td>type of generator network. Options = {'dcgan', 'vanilla',   'wgan'}</td>
<td>'dcgan'</td>
</tr>
<tr>
<td>generator_noise_type</td>
<td>string, optional</td>
<td>shape of noise to sample from. Options={'normal', 'gaussian'}   <br>(Source: (<a href="https://github.com/soumith/ganhacks#3-use-a-spherical-z">https://github.com/soumith/ganhacks#3-use-a-spherical-z</a>)</td>
<td>'normal'</td>
</tr>
<tr>
<td>generator_noise_size</td>
<td>integer, required</td>
<td>size of the noise sample to be generated</td>
<td>100</td>
</tr>
<tr>
<td>generator_num_features</td>
<td>integer, required</td>
<td>number of features/convolutions for generator network</td>
<td>64</td>
</tr>
<tr>
<td>generator_optimizer</td>
<td>string, required</td>
<td>generator network optimizer type. Please see radtorch.settings for list of approved optimizers</td>
<td>'Adam'</td>
</tr>
<tr>
<td>generator_optimizer_param</td>
<td>dictionary, optional</td>
<td>optional extra parameters for optimizer as per pytorch documentation</td>
<td>{'betas':(0.5,0.999)} <br> for Adam optimizer.</td>
</tr>
<tr>
<td>generator_learning_rate</td>
<td>float, required</td>
<td>generator network learning rate</td>
<td>0.0001</td>
</tr>
</tbody>
</table>
<p><strong>Methods</strong>
<div class="highlight"><pre><span></span><code>.run(self, verbose=&#39;batch&#39;, show_images=True, figure_size=(10,10))
</code></pre></div></p>
<ul>
<li>
<p>Runs the GAN training.</p>
</li>
<li>
<p>Parameters:</p>
<ul>
<li>
<p>verbose (string, required): amount of data output. Options {'batch': display info after each batch, 'epoch': display info after each epoch}.default='batch'</p>
</li>
<li>
<p>show_images (boolean, optional): True to show sample of generatot generated images after each epoch.</p>
</li>
<li>
<p>figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(10,10)</p>
</li>
</ul>
</li>
</ul>
<div class="highlight"><pre><span></span><code>.sample(figure_size=(10,10), show_labels=True)
</code></pre></div>

<ul>
<li>
<p>Displays a sample of real data.</p>
</li>
<li>
<p>Parameters:</p>
<ul>
<li>
<p>figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(10,10).</p>
</li>
<li>
<p>show_labels (boolean, optional): show labels on top of images. default=True.</p>
</li>
</ul>
</li>
</ul>
<div class="highlight"><pre><span></span><code>.info()
</code></pre></div>

<ul>
<li>Displays different parameters of the generative adversarial network.</li>
</ul>
<div class="highlight"><pre><span></span><code>.metrics(figure_size=(700,350))
</code></pre></div>

<ul>
<li>
<p>Displays training metrics for the GAN.</p>
</li>
<li>
<p>Explanation of metrics:</p>
<ul>
<li>
<p><em>D_loss</em>: Total loss of discriminator network on both real and fake images.</p>
</li>
<li>
<p><em>G_loss</em>: Loss of discriminator network on detecting fake images as real.</p>
</li>
<li>
<p><em>d_loss_real</em>: Loss of discriminator network on detecting real images as real.</p>
</li>
<li>
<p><em>d_loss_fake</em>: Loss of discriminator network on detecting fake images as fake.</p>
</li>
</ul>
</li>
<li>
<p>Parameters:</p>
<ul>
<li>figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(700,350).</li>
</ul>
</li>
</ul>
<p><small> Documentation Update: 08/01/2020 </small></p>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="Footer">
        
          <a href="../start/" title="Getting Started" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Getting Started
              </div>
            </div>
          </a>
        
        
          <a href="../core/" title="Core" class="md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Core
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/vendor.92ffa368.min.js"></script>
      <script src="../assets/javascripts/bundle.5123e3d4.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents"}</script>
      
      <script>
        app = initialize({
          base: "..",
          features: ["instant"],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.a68abb33.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
    
  </body>
</html>
