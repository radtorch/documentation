{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-116382803-2'); RADTorch The Radiology Machine Learning Framework RADTorch provides a framework of higher level classes and functions that aim at significantly reducing the time needed for implementation of different machine and deep learning algorithms on DICOM medical images. RADTorch was built by radiologists for radiologists so they can build, test and implement state-of-the-art machine learning algorithms in minutes . RADTorch was developed and is currently maintained by Mohamed Elbanan, MD : a Radiology Resident at Yale New Haven Health System, Clinical Research Affiliate at Yale School of Medicine and a Machine-learning enthusiast. RADTorch is built upon widely used machine learning and deep learning frameworks. These include: PyTorch for Deep Learning and Neural Networks. Scikit-learn for Data Management and Machine Learning Algorithms. PyDICOM for handling of DICOM data. Bokeh, Matplotlib and Seaborn for Data Visualization. Documentation Update: 5/14/2020","title":"Home"},{"location":"#radtorch-the-radiology-machine-learning-framework","text":"RADTorch provides a framework of higher level classes and functions that aim at significantly reducing the time needed for implementation of different machine and deep learning algorithms on DICOM medical images. RADTorch was built by radiologists for radiologists so they can build, test and implement state-of-the-art machine learning algorithms in minutes . RADTorch was developed and is currently maintained by Mohamed Elbanan, MD : a Radiology Resident at Yale New Haven Health System, Clinical Research Affiliate at Yale School of Medicine and a Machine-learning enthusiast. RADTorch is built upon widely used machine learning and deep learning frameworks. These include: PyTorch for Deep Learning and Neural Networks. Scikit-learn for Data Management and Machine Learning Algorithms. PyDICOM for handling of DICOM data. Bokeh, Matplotlib and Seaborn for Data Visualization. Documentation Update: 5/14/2020","title":"RADTorch   The Radiology Machine Learning Framework "},{"location":"copyright/","text":"Copyrights Copyrights are reserved to authors of all used open source packages and snippets of code. PyTorch https://github.com/pytorch/pytorch/blob/master/LICENSE From PyTorch Copyright \u00a9 2016- Facebook, Inc (Adam Paszke) Copyright \u00a9 2014- Facebook, Inc (Soumith Chintala) Copyright \u00a9 2011-2014 Idiap Research Institute (Ronan Collobert) Copyright \u00a9 2012-2014 Deepmind Technologies (Koray Kavukcuoglu) Copyright \u00a9 2011-2012 NEC Laboratories America (Koray Kavukcuoglu) Copyright \u00a9 2011-2013 NYU (Clement Farabet) Copyright \u00a9 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston) Copyright \u00a9 2006 Idiap Research Institute (Samy Bengio) Copyright \u00a9 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz) From Caffe2: Copyright \u00a9 2016-present, Facebook Inc. All rights reserved. All contributions by Facebook: Copyright \u00a9 2016 Facebook Inc. All contributions by Google: Copyright \u00a9 2015 Google Inc. All rights reserved. All contributions by Yangqing Jia: Copyright \u00a9 2015 Yangqing Jia All rights reserved. All contributions from Caffe: Copyright\u00a9 2013, 2014, 2015, the respective contributors All rights reserved. All other contributions: Copyright\u00a9 2015, 2016 the respective contributors All rights reserved. Caffe2 uses a copyright model similar to Caffe: each contributor holds copyright over their contributions to Caffe2. The project versioning records all such contribution and copyright details. If a contributor wants to further mark their specific copyright on a particular contribution, they should indicate their copyright solely in the commit message of the change when it is committed. Sklearn https://scikit-learn.org/stable/about.html#citing-scikit-learn Buitinck, L., Louppe, G., Blondel, M., Pedregosa, F., Mueller, A., Grisel, O., Niculae, V., Prettenhofer, P., Gramfort, A., Grobler, J. and Layton, R., 2013. API design for machine learning software: experiences from the scikit-learn project. arXiv preprint arXiv:1309.0238. Pydicom https://github.com/pydicom/pydicom/blob/master/LICENSE License file for pydicom, a pure-python DICOM library Copyright \u00a9 2008-2020 Darcy Mason and pydicom contributors Except for portions outlined below, pydicom is released under an MIT license: Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. Portions of pydicom (private dictionary file(s)) were generated from the private dictionary of the GDCM library, released under the following license: Program: GDCM (Grassroots DICOM). A DICOM library Module: http://gdcm.sourceforge.net/Copyright.html Copyright \u00a9 2006-2010 Mathieu Malaterre Copyright \u00a9 1993-2005 CREATIS (CREATIS = Centre de Recherche et d'Applications en Traitement de l'Image) All rights reserved. Matplotlib https://matplotlib.org/users/license.html Matplotlib only uses BSD compatible code, and its license is based on the PSF license. See the Open Source Initiative licenses page for details on individual licenses. Non-BSD compatible licenses (e.g., LGPL) are acceptable in matplotlib toolkits. For a discussion of the motivations behind the licensing choice, see Licenses. Bokeh https://github.com/bokeh/demo.bokeh.org/blob/master/LICENSE.txt Copyright \u00a9 2012 - 2018, Anaconda, Inc., and Bokeh Contributors All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of Anaconda nor the names of any contributors may be used to endorse or promote products derived from this software without specific prior written permission. Confusion Matrix Matplotlib Code snippet adapted with modification from : https://www.kaggle.com/grfiv4/plot-a-confusion-matrix Documentation Update: 5/14/2020","title":"Copyrights"},{"location":"copyright/#copyrights","text":"Copyrights are reserved to authors of all used open source packages and snippets of code.","title":"Copyrights"},{"location":"copyright/#pytorch","text":"https://github.com/pytorch/pytorch/blob/master/LICENSE From PyTorch Copyright \u00a9 2016- Facebook, Inc (Adam Paszke) Copyright \u00a9 2014- Facebook, Inc (Soumith Chintala) Copyright \u00a9 2011-2014 Idiap Research Institute (Ronan Collobert) Copyright \u00a9 2012-2014 Deepmind Technologies (Koray Kavukcuoglu) Copyright \u00a9 2011-2012 NEC Laboratories America (Koray Kavukcuoglu) Copyright \u00a9 2011-2013 NYU (Clement Farabet) Copyright \u00a9 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston) Copyright \u00a9 2006 Idiap Research Institute (Samy Bengio) Copyright \u00a9 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz) From Caffe2: Copyright \u00a9 2016-present, Facebook Inc. All rights reserved. All contributions by Facebook: Copyright \u00a9 2016 Facebook Inc. All contributions by Google: Copyright \u00a9 2015 Google Inc. All rights reserved. All contributions by Yangqing Jia: Copyright \u00a9 2015 Yangqing Jia All rights reserved. All contributions from Caffe: Copyright\u00a9 2013, 2014, 2015, the respective contributors All rights reserved. All other contributions: Copyright\u00a9 2015, 2016 the respective contributors All rights reserved. Caffe2 uses a copyright model similar to Caffe: each contributor holds copyright over their contributions to Caffe2. The project versioning records all such contribution and copyright details. If a contributor wants to further mark their specific copyright on a particular contribution, they should indicate their copyright solely in the commit message of the change when it is committed.","title":"PyTorch"},{"location":"copyright/#sklearn","text":"https://scikit-learn.org/stable/about.html#citing-scikit-learn Buitinck, L., Louppe, G., Blondel, M., Pedregosa, F., Mueller, A., Grisel, O., Niculae, V., Prettenhofer, P., Gramfort, A., Grobler, J. and Layton, R., 2013. API design for machine learning software: experiences from the scikit-learn project. arXiv preprint arXiv:1309.0238.","title":"Sklearn"},{"location":"copyright/#pydicom","text":"https://github.com/pydicom/pydicom/blob/master/LICENSE License file for pydicom, a pure-python DICOM library Copyright \u00a9 2008-2020 Darcy Mason and pydicom contributors Except for portions outlined below, pydicom is released under an MIT license: Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. Portions of pydicom (private dictionary file(s)) were generated from the private dictionary of the GDCM library, released under the following license: Program: GDCM (Grassroots DICOM). A DICOM library Module: http://gdcm.sourceforge.net/Copyright.html Copyright \u00a9 2006-2010 Mathieu Malaterre Copyright \u00a9 1993-2005 CREATIS (CREATIS = Centre de Recherche et d'Applications en Traitement de l'Image) All rights reserved.","title":"Pydicom"},{"location":"copyright/#matplotlib","text":"https://matplotlib.org/users/license.html Matplotlib only uses BSD compatible code, and its license is based on the PSF license. See the Open Source Initiative licenses page for details on individual licenses. Non-BSD compatible licenses (e.g., LGPL) are acceptable in matplotlib toolkits. For a discussion of the motivations behind the licensing choice, see Licenses.","title":"Matplotlib"},{"location":"copyright/#bokeh","text":"https://github.com/bokeh/demo.bokeh.org/blob/master/LICENSE.txt Copyright \u00a9 2012 - 2018, Anaconda, Inc., and Bokeh Contributors All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of Anaconda nor the names of any contributors may be used to endorse or promote products derived from this software without specific prior written permission.","title":"Bokeh"},{"location":"copyright/#confusion-matrix-matplotlib","text":"Code snippet adapted with modification from : https://www.kaggle.com/grfiv4/plot-a-confusion-matrix Documentation Update: 5/14/2020","title":"Confusion Matrix Matplotlib"},{"location":"core/","text":"Core Module radtorch.core from radtorch import core The core module has all the core functionalities of RADTorch framework. These include: RADTorch_Dataset Data_Processor Feature_Extractor Classifier NN_Classifier DCGAN_Discriminator DCGAN_Generator GAN_Discriminator GAN_Generator Feature_Selector (Coming Soon) Image classification RADTorch_Dataset core.RADTorch_Dataset(data_directory,transformations, table=None,is_dicom=False, mode='RAW', wl=None, image_path_column='IMAGE_PATH', image_label_column='IMAGE_LABEL', is_path=True, sampling=1.0) Description Core class for dataset. This is an extension of Pytorch dataset class with modifications. Parameters data_directory (string, required): path to target data directory/folder. is_dicom (bollean, optional): True if images are DICOM. default=False. table (string or pandas dataframe, optional): path to label table csv or name of pandas data table. default=None. image_path_column (string, optional): name of column that has image path/image file name. default='IMAGE_PATH'. image_label_column (string, optional): name of column that has image label. default='IMAGE_LABEL'. is_path (boolean, optional): True if file_path column in table is file path. If False, this assumes that the column contains file names only and will append the data_directory to all files. default=True. mode (string, optional): mode of handling pixel values from DICOM to numpy array. Option={'RAW': raw pixel values, 'HU': converts pixel values to HU using slope and intercept, 'WIN':Applies a certain window/level to HU converted DICOM image, 'MWIN': converts DICOM image to 3 channel HU numpy array with each channel adjusted to certain window/level. default='RAW'. wl (tuple or list of tuples, optional): value of Window/Levelto be used. If mode is set to 'WIN' then wl takes the format (level, window). If mode is set to 'MWIN' then wl takes the format [(level1, window1), (level2, window2), (level3, window3)]. default=None. sampling (float, optional): fraction of the whole dataset to be used. default=1.0. transformations (list, optional): list of pytorch transformations to be applied to all datasets. By default, the images are resized, channels added up to 3 and greyscaled. default='default'. Returns RADTorch dataset object. Methods info() Returns information of the dataset. .classes() Returns list of classes in dataset. .class_to_idx() Returns mapping of classes to class id (dictionary). .parameters() Returns all the parameter names of the dataset. .balance(method='upsample') Returns a balanced dataset. methods={'upsample', 'downsample'} .mean_std() calculates mean and standard deviation of dataset. Returns tuple of (mean, std) .normalize() Returns a normalized dataset with either mean/std of the dataset or a user specified mean/std in the form of ((mean, mean, mean), (std, std, std)). Data_Processor core.Data_Processor(data_directory,is_dicom=False,table=None, image_path_column='IMAGE_PATH', image_label_column='IMAGE_LABEL', is_path=True, mode='RAW', wl=None, balance_class=False, balance_class_method='upsample', normalize=((0,0,0), (1,1,1)), batch_size=16, num_workers=0, sampling=1.0, custom_resize=False, model_arch='alexnet', type='nn_classifier', transformations='default', extra_transformations=None, test_percent=0.2, valid_percent=0.2, device='auto') Description Class Data Processor. The core class for data preparation before feature extraction and classification. This class performs dataset creation, data splitting, sampling, balancing, normalization and transformations. Parameters data_directory (string, required): path to target data directory/folder. is_dicom (bollean, optional): True if images are DICOM. default=False. table (string or pandas dataframe, optional): path to label table csv or name of pandas data table. default=None. None means the Data_Processor will create the datasets and labels from folder structure as shown here . image_path_column (string, optional): name of column that has image path/image file name. default='IMAGE_PATH'. image_label_column (string, optional): name of column that has image label. default='IMAGE_LABEL'. is_path (boolean, optional): True if file_path column in table is file path. If False, this assumes that the column contains file names only and will append the data_directory to all files. default=False. mode (string, optional): mode of handling pixel values from DICOM to numpy array. Option={'RAW': raw pixel values, 'HU': converts pixel values to HU using slope and intercept, 'WIN':Applies a certain window/level to HU converted DICOM image, 'MWIN': converts DICOM image to 3 channel HU numpy array with each channel adjusted to certain window/level. default='RAW'. wl (tuple or list of tuples, optional): value of Window/Levelto be used. If mode is set to 'WIN' then wl takes the format (level, window). If mode is set to 'MWIN' then wl takes the format [(level1, window1), (level2, window2), (level3, window3)]. default=None. balance_class (bollean, optional): True to perform oversampling in the train dataset to solve class imbalance. default=False. balance_class_method (string, optional): methodology used to balance classes. Options={'upsample', 'downsample'}. default='upsample'. normalize (bollean, optional): Normalizes all datasets by a specified mean and standard deviation. Since most of the used CNN architectures assumes 3 channel input, this follows the following format ((mean, mean, mean), (std, std, std)). default=False. batch_size (integer, optional): Batch size for dataloader. defult=16. num_workers (integer, optional): Number of CPU workers for dataloader. default=0. sampling (float, optional): fraction of the whole dataset to be used. default=1.0. test_percent (float, optional): percentage of data for testing.default=0.2. valid_percent (float, optional): percentage of data for validation (ONLY with NN_Classifier) .default=0.2. custom_resize (integer, optional): By default, the data processor resizes the image in dataset into the size expected bu the different CNN architectures. To override this and use a custom resize, set this to desired value. default=False. model_arch (string, required): CNN model architecture that this data will be used for. Used to resize images as detailed above. default='alexnet' . type (string, required): type of classifier that will be used. please refer to classifier object type. default='nn_classifier'. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. transformations (list, optional): list of pytorch transformations to be applied to all datasets. By default, the images are resized, channels added up to 3 and greyscaled. default='default'. extra_transformations (list, optional): list of pytorch transformations to be extra added to train dataset specifically. default=None. Methods .classes() Returns dictionary of classes/class_idx in data. .info() Returns full information of the data processor object. .dataset_info(plot=True, figure_size=(500,300)) Displays information of the data and class breakdown. Parameters: plot (boolean, optional): True to display data as graph. False to display in table format. default=True figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(500,300) .sample(figure_size=(10,10), show_labels=True, show_file_name=False) Displays a sample from the training dataset. Number of images displayed is the same as batch size. Parameters: figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(10,10) show_label (boolean, optional): show labels above images. default=True show_file_names (boolean, optional): show file path above image. default=False .check_leak(show_file=False) Checks possible overlap between train and test dataset files. Parameters: show_file (boolean, optional): display table of leaked/common files between train and test. default=False. .export(output_path) Exports the Dtaprocessor object for future use. Parameters: output_path (string, required): output file path. Feature_Extractor core.Feature_Extractor(model_arch, dataloader,pre_trained=True, unfreeze=False, device='auto',) Creates a feature extractor neural network using one of the famous CNN architectures and the data provided as dataloader from Data_Processor. Parameters model_arch (string, required): CNN architecture to be utilized. To see list of supported architectures see settings. pre_trained (boolean, optional): Initialize with ImageNet pretrained weights or not. default=True. unfreeze (boolean, required): Unfreeze all layers of network for future retraining. default=False. dataloader (pytorch dataloader object, required): the dataloader that will be used to supply data for feature extraction. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. Model Architecture Default Input Image Size Output Features vgg11 224 x 224 4096 vgg13 224 x 224 4096 vgg16 224 x 224 4096 vgg19 224 x 224 4096 vgg11_bn 224 x 224 4096 vgg13_bn 224 x 224 4096 vgg16_bn 224 x 224 4096 vgg19_bn 224 x 224 4096 resnet18 224 x 224 512 resnet34 224 x 224 512 resnet50 224 x 224 2048 resnet101 224 x 224 2048 resnet152 224 x 224 2048 wide_resnet50_2 224 x 224 2048 wide_resnet101_2 224 x 224 2048 alexnet 256 x 256 4096 Returns Pandas dataframe with extracted features. Methods .num_features() Returns the number of features to be extracted. .run() Runs the feature extraction process. Returns tuple of feature_table (dataframe which contains all features, labels and image file path), features (dataframe which contains features only), feature_names(list of feature names) .export_features(csv_path) Exports extracted features into csv file. Parameters: csv_path (string, required): path to csv output. .plot_extracted_features(num_features=100, num_images=100,image_path_column='IMAGE_PATH', image_label_column='IMAGE_LABEL') Plots Extracted Features in Heatmap Parameters: num_features (integer, optional): number of features to display. default=100 num_images (integer, optional): number of images to display features for. default=100 image_path_column (string, required): name of column that has image names/path. default='IMAGE_PATH' image_label_column (string, required): name of column that has image labels. default='IMAGE_LABEL' .export(output_path) Exports the Feature Extractor object for future use. Parameters: output_path (string, required): output file path. Classifier core.Classifier(extracted_feature_dictionary, feature_table=None, image_label_column=None, image_path_column=None, test_percent=None, type='logistic_regression', interaction_terms=False, parameters={}, cv=True, stratified=True, num_splits=5) Description Image Classification Class. Performs Binary/Multiclass classification using features extracted via Feature Extractor or Supplied by user. Parameters extracted_feature_dictionary (dictionary, required): Dictionary of features/labels datasets to be used for classification. This follows the following format : { 'train': {'features':dataframe, 'feature_names':list, 'labels': list}}, 'test': {'features':dataframe, 'feature_names':list, 'labels': list}}, } feature_table (string, optional): path to csv table with user selected image paths, labels and features. default=None. image_label_column (string, required if using feature_table): name of the column with images labels.default=None. image_path_column (string, requried if using feature_table): name of column with images paths.default=None. test_percent (float, required if using feature_table): percentage of data for testing.default=None. type (string, required): type of classifier. For complete list refer to settings. default='logistic_regression'. interaction_terms (boolean, optional): create interaction terms between different features and add them as new features to feature table. default=False. cv (boolean, required): True for cross validation. default=True. stratified (boolean, required): True for stratified cross validation. default=True. num_splits (integer, required): Number of K-fold cross validation splits. default=5. parameters (dictionary, optional): optional parameters passed to the classifier. Please refer to sci-kit learn documentation. Methods .info() Returns table of different classifier parameters/properties. .run() Runs Image Classifier. .average_cv_accuracy() Returns average cross validation accuracy. .test_accuracy() Returns accuracy of trained classifier on test dataset. .confusion_matrix(title='Confusion Matrix',cmap=None,normalize=False,figure_size=(8,6)) Displays confusion matrix using trained classifier and test dataset. Parameters: title (string, optional): name to be displayed over confusion matrix. cmap (string, optional): colormap of the displayed confusion matrix. This follows matplot color palletes. default=None. normalize (boolean, optional): normalize values. default=False. figure_size (tuple, optional): size of the figure as width, height. default=(8,6) .roc() Display ROC and AUC of trained classifier and test dataset. .predict(input_image_path, all_predictions=False) Returns label prediction of a target image using a trained classifier. This works as part of pipeline only for now. Parameters: input_image_path (string, required): path of target image. all_predictions (boolean, optional): return a table of all predictions for all possible labels. .export() Exports the Classifier object for future use. output_path (string, required): output file path. .export_trained_classifier() Exports only the trained classifier for future use. output_path (string, required): output file path. NN_Classifier core.NN_Classifier(feature_extractor, data_processor, unfreeze=False, learning_rate=0.0001, epochs=10, optimizer='Adam', loss_function='CrossEntropyLoss', lr_scheduler=None, batch_size=16, device='auto', custom_nn_classifier=None, loss_function_parameters={}, optimizer_parameters={},) Description Neural Network Classifier. This serves as extension of pytorch neural network modules e.g. VGG16, for fine tuning or transfer learning. Parameters data_processor (radtorch.core.data_processor, required): data processor object from radtorch.core.Data_Processor. feature_extractor (radtorch.core.feature_extractor, required): feature_extractor object from radtorch.core.Feature_Extractor. unfreeze (boolean, optional): True to unfreeze the weights of all layers in the neural network model for model finetuning. False to just use unfreezed final layers for transfer learning. default=False. learning_rate (float, required): Learning rate. default=0.0001. epochs (integer, required): training epochs. default=10. optimizer (string, required): neural network optimizer type. Please see radtorch.settings for list of approved optimizers. default='Adam'. optimizer_parameters (dictionary, optional): optional extra parameters for optimizer as per pytorch documentation. loss_function (string, required): neural network loss function. Please see radtorch.settings for list of approved loss functions. default='CrossEntropyLoss'. loss_function_parameters (dictionary, optional): optional extra parameters for loss function as per pytorch documentation. lr_scheduler (string, optional): learning rate scheduler - upcoming soon. batch_size (integer, required): batch size. default=16 custom_nn_classifier (pytorch model, optional): Option to use a custom made neural network classifier that will be added after feature extracted layers. default=None. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. Methods .info() Returns table of different classifier parameters/properties. .run() Runs Image Classifier. .confusion_matrix(target_dataset=None, figure_size=(8,6), cmap=None) Displays confusion matrix for trained nn_classifier on test dataset. Parameters: target_dataset (pytorch dataset, optional): this option can be used to test the trained model on an external test dataset. If set to None, the confusion matrix is generated using the test dataset initially specified in the data_processor. default=None. figure_size (tuple, optional): size of the figure as width, height. default=(8,6) .roc() Display ROC and AUC of trained model and test dataset. .metrics(figure_size=(700,400)) Displays graphical representation of train/validation loss /accuracy. .predict(input_image_path, all_predictions=True) Displays classs prediction for a target image using a trained classifier. Parameters: input_image_path (string, required): path to target image. all_predictions (boolean, optional): True to display prediction percentage accuracies for all prediction classes. default=True. .misclassified(num_of_images=4, figure_size=(5,5), table=False) Displays sample of images misclassified by the classifier from test dataset. Parameters: num_of_images (integer, optional): number of images to be displayed. default=4. figure_size (tuple, optional): size of the figure as width, height. default=(5,5). table (boolean, optional): True to display a table of all misclassified images including image path, true label and predicted label. Generative Adversarial Networks DCGAN_Discriminator core.DCGAN_Discriminator(num_input_channels, kernel_size, num_discriminator_features, input_image_size, device='auto') Description Core Deep Convolutional GAN Discriminator Network. Parameters kernel_size (integer, required): size of kernel/filter to be used for convolution. num_discriminator_features (integer, required): number of features/convolutions for discriminator network. num_input_channels (integer, required): number of channels for input image. input_image_size (integer, required): size of input image. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. DCGAN_Generator core.DCGAN_Generator(noise_size, num_generator_features, num_output_channels, target_image_size, device='auto') Description Core Deep Convolutional GAN Generator Network. Parameters noise_size (integer, required): size of the noise sample to be generated. num_generator_features (integer, required): number of features/convolutions for generator network. num_output_channels (integer, required): number of channels for output image. target_image_size (integer, required): size of output image. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. GAN_Discriminator core.GAN_Discriminator(input_image_size, intput_num_channels, device='auto') Description Core Vanilla GAN Discriminator Network. Parameters num_input_channels (integer, required): number of channels for input image. input_image_size (integer, required): size of input image. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. GAN_Generator core.GAN_Generator(noise_size, target_image_size, output_num_channels, device='auto') Description Core Vanilla Convolutional GAN Generator Network. Parameters noise_size (integer, required): size of the noise sample to be generated. num_output_channels (integer, required): number of channels for output image. target_image_size (integer, required): size of output image. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. Documentation Update: 5/14/2020","title":"radtorch.core"},{"location":"core/#core-module-radtorchcore","text":"from radtorch import core The core module has all the core functionalities of RADTorch framework. These include: RADTorch_Dataset Data_Processor Feature_Extractor Classifier NN_Classifier DCGAN_Discriminator DCGAN_Generator GAN_Discriminator GAN_Generator Feature_Selector (Coming Soon)","title":"Core Module  radtorch.core "},{"location":"core/#image-classification","text":"","title":"Image classification"},{"location":"core/#radtorch_dataset","text":"core.RADTorch_Dataset(data_directory,transformations, table=None,is_dicom=False, mode='RAW', wl=None, image_path_column='IMAGE_PATH', image_label_column='IMAGE_LABEL', is_path=True, sampling=1.0) Description Core class for dataset. This is an extension of Pytorch dataset class with modifications. Parameters data_directory (string, required): path to target data directory/folder. is_dicom (bollean, optional): True if images are DICOM. default=False. table (string or pandas dataframe, optional): path to label table csv or name of pandas data table. default=None. image_path_column (string, optional): name of column that has image path/image file name. default='IMAGE_PATH'. image_label_column (string, optional): name of column that has image label. default='IMAGE_LABEL'. is_path (boolean, optional): True if file_path column in table is file path. If False, this assumes that the column contains file names only and will append the data_directory to all files. default=True. mode (string, optional): mode of handling pixel values from DICOM to numpy array. Option={'RAW': raw pixel values, 'HU': converts pixel values to HU using slope and intercept, 'WIN':Applies a certain window/level to HU converted DICOM image, 'MWIN': converts DICOM image to 3 channel HU numpy array with each channel adjusted to certain window/level. default='RAW'. wl (tuple or list of tuples, optional): value of Window/Levelto be used. If mode is set to 'WIN' then wl takes the format (level, window). If mode is set to 'MWIN' then wl takes the format [(level1, window1), (level2, window2), (level3, window3)]. default=None. sampling (float, optional): fraction of the whole dataset to be used. default=1.0. transformations (list, optional): list of pytorch transformations to be applied to all datasets. By default, the images are resized, channels added up to 3 and greyscaled. default='default'. Returns RADTorch dataset object. Methods info() Returns information of the dataset. .classes() Returns list of classes in dataset. .class_to_idx() Returns mapping of classes to class id (dictionary). .parameters() Returns all the parameter names of the dataset. .balance(method='upsample') Returns a balanced dataset. methods={'upsample', 'downsample'} .mean_std() calculates mean and standard deviation of dataset. Returns tuple of (mean, std) .normalize() Returns a normalized dataset with either mean/std of the dataset or a user specified mean/std in the form of ((mean, mean, mean), (std, std, std)).","title":"RADTorch_Dataset"},{"location":"core/#data_processor","text":"core.Data_Processor(data_directory,is_dicom=False,table=None, image_path_column='IMAGE_PATH', image_label_column='IMAGE_LABEL', is_path=True, mode='RAW', wl=None, balance_class=False, balance_class_method='upsample', normalize=((0,0,0), (1,1,1)), batch_size=16, num_workers=0, sampling=1.0, custom_resize=False, model_arch='alexnet', type='nn_classifier', transformations='default', extra_transformations=None, test_percent=0.2, valid_percent=0.2, device='auto') Description Class Data Processor. The core class for data preparation before feature extraction and classification. This class performs dataset creation, data splitting, sampling, balancing, normalization and transformations. Parameters data_directory (string, required): path to target data directory/folder. is_dicom (bollean, optional): True if images are DICOM. default=False. table (string or pandas dataframe, optional): path to label table csv or name of pandas data table. default=None. None means the Data_Processor will create the datasets and labels from folder structure as shown here . image_path_column (string, optional): name of column that has image path/image file name. default='IMAGE_PATH'. image_label_column (string, optional): name of column that has image label. default='IMAGE_LABEL'. is_path (boolean, optional): True if file_path column in table is file path. If False, this assumes that the column contains file names only and will append the data_directory to all files. default=False. mode (string, optional): mode of handling pixel values from DICOM to numpy array. Option={'RAW': raw pixel values, 'HU': converts pixel values to HU using slope and intercept, 'WIN':Applies a certain window/level to HU converted DICOM image, 'MWIN': converts DICOM image to 3 channel HU numpy array with each channel adjusted to certain window/level. default='RAW'. wl (tuple or list of tuples, optional): value of Window/Levelto be used. If mode is set to 'WIN' then wl takes the format (level, window). If mode is set to 'MWIN' then wl takes the format [(level1, window1), (level2, window2), (level3, window3)]. default=None. balance_class (bollean, optional): True to perform oversampling in the train dataset to solve class imbalance. default=False. balance_class_method (string, optional): methodology used to balance classes. Options={'upsample', 'downsample'}. default='upsample'. normalize (bollean, optional): Normalizes all datasets by a specified mean and standard deviation. Since most of the used CNN architectures assumes 3 channel input, this follows the following format ((mean, mean, mean), (std, std, std)). default=False. batch_size (integer, optional): Batch size for dataloader. defult=16. num_workers (integer, optional): Number of CPU workers for dataloader. default=0. sampling (float, optional): fraction of the whole dataset to be used. default=1.0. test_percent (float, optional): percentage of data for testing.default=0.2. valid_percent (float, optional): percentage of data for validation (ONLY with NN_Classifier) .default=0.2. custom_resize (integer, optional): By default, the data processor resizes the image in dataset into the size expected bu the different CNN architectures. To override this and use a custom resize, set this to desired value. default=False. model_arch (string, required): CNN model architecture that this data will be used for. Used to resize images as detailed above. default='alexnet' . type (string, required): type of classifier that will be used. please refer to classifier object type. default='nn_classifier'. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. transformations (list, optional): list of pytorch transformations to be applied to all datasets. By default, the images are resized, channels added up to 3 and greyscaled. default='default'. extra_transformations (list, optional): list of pytorch transformations to be extra added to train dataset specifically. default=None. Methods .classes() Returns dictionary of classes/class_idx in data. .info() Returns full information of the data processor object. .dataset_info(plot=True, figure_size=(500,300)) Displays information of the data and class breakdown. Parameters: plot (boolean, optional): True to display data as graph. False to display in table format. default=True figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(500,300) .sample(figure_size=(10,10), show_labels=True, show_file_name=False) Displays a sample from the training dataset. Number of images displayed is the same as batch size. Parameters: figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(10,10) show_label (boolean, optional): show labels above images. default=True show_file_names (boolean, optional): show file path above image. default=False .check_leak(show_file=False) Checks possible overlap between train and test dataset files. Parameters: show_file (boolean, optional): display table of leaked/common files between train and test. default=False. .export(output_path) Exports the Dtaprocessor object for future use. Parameters: output_path (string, required): output file path.","title":"Data_Processor"},{"location":"core/#feature_extractor","text":"core.Feature_Extractor(model_arch, dataloader,pre_trained=True, unfreeze=False, device='auto',) Creates a feature extractor neural network using one of the famous CNN architectures and the data provided as dataloader from Data_Processor. Parameters model_arch (string, required): CNN architecture to be utilized. To see list of supported architectures see settings. pre_trained (boolean, optional): Initialize with ImageNet pretrained weights or not. default=True. unfreeze (boolean, required): Unfreeze all layers of network for future retraining. default=False. dataloader (pytorch dataloader object, required): the dataloader that will be used to supply data for feature extraction. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. Model Architecture Default Input Image Size Output Features vgg11 224 x 224 4096 vgg13 224 x 224 4096 vgg16 224 x 224 4096 vgg19 224 x 224 4096 vgg11_bn 224 x 224 4096 vgg13_bn 224 x 224 4096 vgg16_bn 224 x 224 4096 vgg19_bn 224 x 224 4096 resnet18 224 x 224 512 resnet34 224 x 224 512 resnet50 224 x 224 2048 resnet101 224 x 224 2048 resnet152 224 x 224 2048 wide_resnet50_2 224 x 224 2048 wide_resnet101_2 224 x 224 2048 alexnet 256 x 256 4096 Returns Pandas dataframe with extracted features. Methods .num_features() Returns the number of features to be extracted. .run() Runs the feature extraction process. Returns tuple of feature_table (dataframe which contains all features, labels and image file path), features (dataframe which contains features only), feature_names(list of feature names) .export_features(csv_path) Exports extracted features into csv file. Parameters: csv_path (string, required): path to csv output. .plot_extracted_features(num_features=100, num_images=100,image_path_column='IMAGE_PATH', image_label_column='IMAGE_LABEL') Plots Extracted Features in Heatmap Parameters: num_features (integer, optional): number of features to display. default=100 num_images (integer, optional): number of images to display features for. default=100 image_path_column (string, required): name of column that has image names/path. default='IMAGE_PATH' image_label_column (string, required): name of column that has image labels. default='IMAGE_LABEL' .export(output_path) Exports the Feature Extractor object for future use. Parameters: output_path (string, required): output file path.","title":"Feature_Extractor"},{"location":"core/#classifier","text":"core.Classifier(extracted_feature_dictionary, feature_table=None, image_label_column=None, image_path_column=None, test_percent=None, type='logistic_regression', interaction_terms=False, parameters={}, cv=True, stratified=True, num_splits=5) Description Image Classification Class. Performs Binary/Multiclass classification using features extracted via Feature Extractor or Supplied by user. Parameters extracted_feature_dictionary (dictionary, required): Dictionary of features/labels datasets to be used for classification. This follows the following format : { 'train': {'features':dataframe, 'feature_names':list, 'labels': list}}, 'test': {'features':dataframe, 'feature_names':list, 'labels': list}}, } feature_table (string, optional): path to csv table with user selected image paths, labels and features. default=None. image_label_column (string, required if using feature_table): name of the column with images labels.default=None. image_path_column (string, requried if using feature_table): name of column with images paths.default=None. test_percent (float, required if using feature_table): percentage of data for testing.default=None. type (string, required): type of classifier. For complete list refer to settings. default='logistic_regression'. interaction_terms (boolean, optional): create interaction terms between different features and add them as new features to feature table. default=False. cv (boolean, required): True for cross validation. default=True. stratified (boolean, required): True for stratified cross validation. default=True. num_splits (integer, required): Number of K-fold cross validation splits. default=5. parameters (dictionary, optional): optional parameters passed to the classifier. Please refer to sci-kit learn documentation. Methods .info() Returns table of different classifier parameters/properties. .run() Runs Image Classifier. .average_cv_accuracy() Returns average cross validation accuracy. .test_accuracy() Returns accuracy of trained classifier on test dataset. .confusion_matrix(title='Confusion Matrix',cmap=None,normalize=False,figure_size=(8,6)) Displays confusion matrix using trained classifier and test dataset. Parameters: title (string, optional): name to be displayed over confusion matrix. cmap (string, optional): colormap of the displayed confusion matrix. This follows matplot color palletes. default=None. normalize (boolean, optional): normalize values. default=False. figure_size (tuple, optional): size of the figure as width, height. default=(8,6) .roc() Display ROC and AUC of trained classifier and test dataset. .predict(input_image_path, all_predictions=False) Returns label prediction of a target image using a trained classifier. This works as part of pipeline only for now. Parameters: input_image_path (string, required): path of target image. all_predictions (boolean, optional): return a table of all predictions for all possible labels. .export() Exports the Classifier object for future use. output_path (string, required): output file path. .export_trained_classifier() Exports only the trained classifier for future use. output_path (string, required): output file path.","title":"Classifier"},{"location":"core/#nn_classifier","text":"core.NN_Classifier(feature_extractor, data_processor, unfreeze=False, learning_rate=0.0001, epochs=10, optimizer='Adam', loss_function='CrossEntropyLoss', lr_scheduler=None, batch_size=16, device='auto', custom_nn_classifier=None, loss_function_parameters={}, optimizer_parameters={},) Description Neural Network Classifier. This serves as extension of pytorch neural network modules e.g. VGG16, for fine tuning or transfer learning. Parameters data_processor (radtorch.core.data_processor, required): data processor object from radtorch.core.Data_Processor. feature_extractor (radtorch.core.feature_extractor, required): feature_extractor object from radtorch.core.Feature_Extractor. unfreeze (boolean, optional): True to unfreeze the weights of all layers in the neural network model for model finetuning. False to just use unfreezed final layers for transfer learning. default=False. learning_rate (float, required): Learning rate. default=0.0001. epochs (integer, required): training epochs. default=10. optimizer (string, required): neural network optimizer type. Please see radtorch.settings for list of approved optimizers. default='Adam'. optimizer_parameters (dictionary, optional): optional extra parameters for optimizer as per pytorch documentation. loss_function (string, required): neural network loss function. Please see radtorch.settings for list of approved loss functions. default='CrossEntropyLoss'. loss_function_parameters (dictionary, optional): optional extra parameters for loss function as per pytorch documentation. lr_scheduler (string, optional): learning rate scheduler - upcoming soon. batch_size (integer, required): batch size. default=16 custom_nn_classifier (pytorch model, optional): Option to use a custom made neural network classifier that will be added after feature extracted layers. default=None. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. Methods .info() Returns table of different classifier parameters/properties. .run() Runs Image Classifier. .confusion_matrix(target_dataset=None, figure_size=(8,6), cmap=None) Displays confusion matrix for trained nn_classifier on test dataset. Parameters: target_dataset (pytorch dataset, optional): this option can be used to test the trained model on an external test dataset. If set to None, the confusion matrix is generated using the test dataset initially specified in the data_processor. default=None. figure_size (tuple, optional): size of the figure as width, height. default=(8,6) .roc() Display ROC and AUC of trained model and test dataset. .metrics(figure_size=(700,400)) Displays graphical representation of train/validation loss /accuracy. .predict(input_image_path, all_predictions=True) Displays classs prediction for a target image using a trained classifier. Parameters: input_image_path (string, required): path to target image. all_predictions (boolean, optional): True to display prediction percentage accuracies for all prediction classes. default=True. .misclassified(num_of_images=4, figure_size=(5,5), table=False) Displays sample of images misclassified by the classifier from test dataset. Parameters: num_of_images (integer, optional): number of images to be displayed. default=4. figure_size (tuple, optional): size of the figure as width, height. default=(5,5). table (boolean, optional): True to display a table of all misclassified images including image path, true label and predicted label.","title":"NN_Classifier"},{"location":"core/#generative-adversarial-networks","text":"","title":"Generative Adversarial Networks"},{"location":"core/#dcgan_discriminator","text":"core.DCGAN_Discriminator(num_input_channels, kernel_size, num_discriminator_features, input_image_size, device='auto') Description Core Deep Convolutional GAN Discriminator Network. Parameters kernel_size (integer, required): size of kernel/filter to be used for convolution. num_discriminator_features (integer, required): number of features/convolutions for discriminator network. num_input_channels (integer, required): number of channels for input image. input_image_size (integer, required): size of input image. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'.","title":"DCGAN_Discriminator"},{"location":"core/#dcgan_generator","text":"core.DCGAN_Generator(noise_size, num_generator_features, num_output_channels, target_image_size, device='auto') Description Core Deep Convolutional GAN Generator Network. Parameters noise_size (integer, required): size of the noise sample to be generated. num_generator_features (integer, required): number of features/convolutions for generator network. num_output_channels (integer, required): number of channels for output image. target_image_size (integer, required): size of output image. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'.","title":"DCGAN_Generator"},{"location":"core/#gan_discriminator","text":"core.GAN_Discriminator(input_image_size, intput_num_channels, device='auto') Description Core Vanilla GAN Discriminator Network. Parameters num_input_channels (integer, required): number of channels for input image. input_image_size (integer, required): size of input image. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'.","title":"GAN_Discriminator"},{"location":"core/#gan_generator","text":"core.GAN_Generator(noise_size, target_image_size, output_num_channels, device='auto') Description Core Vanilla Convolutional GAN Generator Network. Parameters noise_size (integer, required): size of the noise sample to be generated. num_output_channels (integer, required): number of channels for output image. target_image_size (integer, required): size of output image. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. Documentation Update: 5/14/2020","title":"GAN_Generator"},{"location":"install/","text":"Installation Latest stable release of RADTorch and its dependencies can be installed using the following terminal commands: pip3 install git+https://download.radtorch.com/ To uninstall simply use: pip3 uninstall radtorch Requirements Internet connection is required for installation and to download trained model weights. Python 3.5 or later. Dependencies RADTorch depends on the following packages which are installed automatically during the installation process. torch 1.4.0 torchvision 0.5.0 numpy 1.17.4 pandas 0.25.3 pydicom 1.3.0 matplotlib 3.1.3 pillow 7.0.0 tqdm 4.38.0 sklearn 0.22.1 Bokeh 2.0.0 seaborn 0.10.0 Documentation Update: 5/14/2020","title":"Installation"},{"location":"install/#installation","text":"Latest stable release of RADTorch and its dependencies can be installed using the following terminal commands: pip3 install git+https://download.radtorch.com/ To uninstall simply use: pip3 uninstall radtorch","title":"Installation"},{"location":"install/#requirements","text":"Internet connection is required for installation and to download trained model weights. Python 3.5 or later.","title":"Requirements"},{"location":"install/#dependencies","text":"RADTorch depends on the following packages which are installed automatically during the installation process. torch 1.4.0 torchvision 0.5.0 numpy 1.17.4 pandas 0.25.3 pydicom 1.3.0 matplotlib 3.1.3 pillow 7.0.0 tqdm 4.38.0 sklearn 0.22.1 Bokeh 2.0.0 seaborn 0.10.0 Documentation Update: 5/14/2020","title":"Dependencies"},{"location":"involve/","text":"Feature Requests Feature requests are more than welcomed on our discussion board HERE Contributing RadTorch is on GitHub . Bug reports and pull requests are welcome. Documentation Update: 5/14/2020","title":"Get Involved"},{"location":"involve/#feature-requests","text":"Feature requests are more than welcomed on our discussion board HERE","title":"Feature Requests"},{"location":"involve/#contributing","text":"RadTorch is on GitHub . Bug reports and pull requests are welcome. Documentation Update: 5/14/2020","title":"Contributing"},{"location":"license/","text":"GNU Affero General Public License v3.0 License Copyright \u00a9 2020 RADTorch, Mohamed Elbanan M.D. This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program. If not, see https://www.gnu.org/licenses/. Documentation Update: 5/14/2020","title":"License"},{"location":"license/#gnu-affero-general-public-license-v30-license","text":"Copyright \u00a9 2020 RADTorch, Mohamed Elbanan M.D. This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program. If not, see https://www.gnu.org/licenses/. Documentation Update: 5/14/2020","title":"GNU Affero General Public License v3.0 License"},{"location":"pipeline/","text":"Pipeline Module radtorch.pipeline Pipelines are probably the most exciting feature of RADTorch Framework. With only few lines of code, the pipeline module allows you to run state-of-the-art machine learning algorithms and much more. from radtorch import pipeline RADTorch follows principles of object-oriented-programming (OOP) in the sense that RADTorch pipelines are made of core building blocks and each of these blocks has specific functions/methods that can be accessed accordingly. For example, pipeline.Image_Classification.data_processor.dataset_info() can be used to access the dataset information for that particular Image Classification pipeline. Image_Classification pipeline.Image_Classification( data_directory, is_dicom=False, table=None, image_path_column='IMAGE_PATH', image_label_column='IMAGE_LABEL', is_path=True, mode='RAW', wl=None, balance_class=False, balance_class_method='upsample', interaction_terms=False, normalize=((0,0,0), (1,1,1)), batch_size=16, num_workers=0, sampling=1.0, test_percent=0.2, valid_percent=0.2, custom_resize=False, model_arch='alexnet', pre_trained=True, unfreeze=False, type='nn_classifier', cv=True, stratified=True, num_splits=5, parameters={}, learning_rate=0.0001, epochs=10, optimizer='Adam', loss_function='CrossEntropyLoss',lr_scheduler=None, custom_nn_classifier=None, loss_function_parameters={}, optimizer_parameters={}, transformations='default', extra_transformations=None, device='auto',) Description Complete end-to-end image classification pipeline. Parameters data_directory (string, required): path to target data directory/folder. is_dicom (bollean, optional): True if images are DICOM. default=False. table (string or pandas dataframe, optional): path to label table csv or name of pandas data table. default=None. image_path_column (string, optional): name of column that has image path/image file name. default='IMAGE_PATH'. image_label_column (string, optional): name of column that has image label. default='IMAGE_LABEL'. is_path (boolean, optional): True if file_path column in table is file path. If False, this assumes that the column contains file names only and will append the data_directory to all files. default=True. mode (string, optional): mode of handling pixel values from DICOM to numpy array. Option={'RAW': raw pixel values, 'HU': converts pixel values to HU using slope and intercept, 'WIN':Applies a certain window/level to HU converted DICOM image, 'MWIN': converts DICOM image to 3 channel HU numpy array with each channel adjusted to certain window/level. default='RAW'. wl (tuple or list of tuples, optional): value of Window/Levelto be used. If mode is set to 'WIN' then wl takes the format (level, window). If mode is set to 'MWIN' then wl takes the format [(level1, window1), (level2, window2), (level3, window3)]. default=None. balance_class (bollean, optional): True to perform oversampling in the train dataset to solve class imbalance. default=False. balance_class_method (string, optional): methodology used to balance classes. Options={'upsample', 'downsample'}. default='upsample'. interaction_terms (boolean, optional): create interaction terms between different features and add them as new features to feature table. default=False. normalize (bolean/False or Tuple, optional): Normalizes all datasets by a specified mean and standard deviation. Since most of the used CNN architectures assumes 3 channel input, this follows the following format ((mean, mean, mean), (std, std, std)). default=((0,0,0), (1,1,1)). batch_size (integer, optional): Batch size for dataloader. defult=16. num_workers (integer, optional): Number of CPU workers for dataloader. default=0. sampling (float, optional): fraction of the whole dataset to be used. default=1.0. test_percent (float, optional): percentage of data for testing.default=0.2. valid_percent (float, optional): percentage of data for validation (ONLY with NN_Classifier) .default=0.2. custom_resize (integer, optional): By default, the data processor resizes the image in dataset into the size expected bu the different CNN architectures. To override this and use a custom resize, set this to desired value. default=False. transformations (list, optional): list of pytorch transformations to be applied to all datasets. By default, the images are resized, channels added up to 3 and greyscaled. default='default'. extra_transformations (list, optional): list of pytorch transformations to be extra added to train dataset specifically. default=None. model_arch (string, required): CNN model architecture that this data will be used for. Used to resize images as detailed above. default='alexnet' . pre_trained (boolean, optional): Initialize with ImageNet pretrained weights or not. default=True. unfreeze (boolean, required): Unfreeze all layers of network for future retraining. default=False. type (string, required): type of classifier. For complete list refer to settings. default='logistic_regression'. Classifier specific parameters cv (boolean, required): True for cross validation. default=True. stratified (boolean, required): True for stratified cross validation. default=True. num_splits (integer, required): Number of K-fold cross validation splits. default=5. parameters (dictionary, optional): optional parameters passed to the classifier. Please refer to sci-kit learn documentaion. NN_Classifier specific parameters learning_rate (float, required): Learning rate. default=0.0001. epochs (integer, required): training epochs. default=10. optimizer (string, required): neural network optimizer type. Please see radtorch.settings for list of approved optimizers. default='Adam'. optimizer_parameters (dictionary, optional): optional extra parameters for optimizer as per pytorch documentation. loss_function (string, required): neural network loss function. Please see radtorch.settings for list of approved loss functions. default='CrossEntropyLoss'. loss_function_parameters (dictionary, optional): optional extra parameters for loss function as per pytorch documentation. lr_scheduler (string, optional): learning rate scheduler - upcoming soon. custom_nn_classifier (pytorch model, optional): Option to use a custom made neural network classifier that will be added after feature extracted layers. default=None. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. Methods In addition to core component methods , pipeline accessible methods include: .info() Displays information of the image classification pipeline. .run() Starts the image classification pipeline training. .metrics(figure_size=(700, 350)) Displays the training metrics of the image classification pipeline. .export(output_path): Exports the pipeline to output path. GAN core.GAN( data_directory, table=None, is_dicom=False, is_path=True, image_path_column='IMAGE_PATH', image_label_column='IMAGE_LABEL', mode='RAW', wl=None, batch_size=16, normalize=((0,0,0),(1,1,1)), num_workers=0, label_smooth=True, sampling=1.0, transformations='default', discriminator='dcgan', generator='dcgan', generator_noise_size=100, generator_noise_type='normal', discriminator_num_features=64, generator_num_features=64, image_size=128, image_channels=1, discrinimator_optimizer='Adam', generator_optimizer='Adam', discrinimator_optimizer_param={'betas':(0.5,0.999)}, generator_optimizer_param={'betas':(0.5,0.999)}, generator_learning_rate=0.0001, discriminator_learning_rate=0.0001, epochs=10, device='auto') Description Generative Advarsarial Networks Pipeline. Parameters data_directory (string, required): path to target data directory/folder. is_dicom (bollean, optional): True if images are DICOM. default=False. table (string or pandas dataframe, optional): path to label table csv or name of pandas data table. default=None. image_path_column (string, optional): name of column that has image path/image file name. default='IMAGE_PATH'. image_label_column (string, optional): name of column that has image label. default='IMAGE_LABEL'. is_path (boolean, optional): True if file_path column in table is file path. If False, this assumes that the column contains file names only and will append the data_directory to all files. default=True. mode (string, optional): mode of handling pixel values from DICOM to numpy array. Option={'RAW': raw pixel values, 'HU': converts pixel values to HU using slope and intercept, 'WIN':Applies a certain window/level to HU converted DICOM image, 'MWIN': converts DICOM image to 3 channel HU numpy array with each channel adjusted to certain window/level. default='RAW'. wl (tuple or list of tuples, optional): value of Window/Levelto be used. If mode is set to 'WIN' then wl takes the format (level, window). If mode is set to 'MWIN' then wl takes the format [(level1, window1), (level2, window2), (level3, window3)]. default=None. batch_size (integer, optional): Batch size for dataloader. defult=16. num_workers (integer, optional): Number of CPU workers for dataloader. default=0. sampling (float, optional): fraction of the whole dataset to be used. default=1.0. transformations (list, optional): list of pytorch transformations to be applied to all datasets. By default, the images are resized, channels added up to 3 and greyscaled. default='default'. normalize (bolean/False or Tuple, optional): Normalizes all datasets by a specified mean and standard deviation. Since most of the used CNN architectures assumes 3 channel input, this follows the following format ((mean, mean, mean), (std, std, std)). default=((0,0,0),(1,1,1)). label_smooth (boolean, optioanl): by default, labels for real images as assigned to 1. If label smoothing is set to True, lables of real images will be assigned to 0.9. default=True. (Source: https://github.com/soumith/ganhacks#6-use-soft-and-noisy-labels ) epochs (integer, required): training epochs. default=10. generator (string, required): type of generator network. Options = {'dcgan', 'vanilla'}. default='dcgan' discriminator (string, required): type of discriminator network. Options = {'dcgan', 'vanilla'}. default='dcgan' image_channels (integer, required): number of output channels for discriminator input and generator output. default=1 generator_noise_type (string, optional): shape of noise to sample from. Options={'normal', 'gaussian'}. default='normal'. ( https://github.com/soumith/ganhacks#3-use-a-spherical-z ) generator_noise_size (integer, required): size of the noise sample to be generated. default=100 generator_num_features (integer, required): number of features/convolutions for generator network. default=64 image_size (integer, required): iamge size for discriminator input and generator output.default=128 discriminator_num_features (integer, required): number of features/convolutions for discriminator network.default=64 generator_optimizer (string, required): generator network optimizer type. Please see radtorch.settings for list of approved optimizers. default='Adam'. generator_optimizer_param (dictionary, optional): optional extra parameters for optimizer as per pytorch documentation. default={'betas':(0.5,0.999)} for Adam optimizer. discrinimator_optimizer (string, required): discrinimator network optimizer type. Please see radtorch.settings for list of approved optimizers. default='Adam'. discrinimator_optimizer_param (dictionary, optional): optional extra parameters for optimizer as per pytorch documentation. default={'betas':(0.5,0.999)} for Adam optimizer. generator_learning_rate (float, required): generator network learning rate. default=0.0001. discriminator_learning_rate (float, required): discrinimator network learning rate. default=0.0001. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. Methods .run(self, verbose='batch', show_images=True, figure_size=(10,10)) Runs the GAN training. Parameters: verbose (string, required): amount of data output. Options {'batch': display info after each batch, 'epoch': display info after each epoch}.default='batch' show_images (boolean, optional): True to show sample of generatot generated images after each epoch. figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(10,10) .sample(figure_size=(10,10), show_labels=True) Displays a sample of real data. Parameters: figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(10,10). show_labels (boolean, optional): show labels on top of images. default=True. .info() Displays different parameters of the generative adversarial network. .metrics(figure_size=(700,350)) Displays training metrics for the GAN. Explanation of metrics: D_loss : Total loss of discriminator network on both real and fake images. G_loss : Loss of discriminator network on detecting fake images as real. d_loss_real : Loss of discriminator network on detecting real images as real. d_loss_fake : Loss of discriminator network on detecting fake images as fake. Parameters: figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(700,350). Documentation Update: 5/14/2020","title":"radtorch.pipeline"},{"location":"pipeline/#pipeline-module-radtorchpipeline","text":"Pipelines are probably the most exciting feature of RADTorch Framework. With only few lines of code, the pipeline module allows you to run state-of-the-art machine learning algorithms and much more. from radtorch import pipeline RADTorch follows principles of object-oriented-programming (OOP) in the sense that RADTorch pipelines are made of core building blocks and each of these blocks has specific functions/methods that can be accessed accordingly. For example, pipeline.Image_Classification.data_processor.dataset_info() can be used to access the dataset information for that particular Image Classification pipeline.","title":"Pipeline Module  radtorch.pipeline "},{"location":"pipeline/#image_classification","text":"pipeline.Image_Classification( data_directory, is_dicom=False, table=None, image_path_column='IMAGE_PATH', image_label_column='IMAGE_LABEL', is_path=True, mode='RAW', wl=None, balance_class=False, balance_class_method='upsample', interaction_terms=False, normalize=((0,0,0), (1,1,1)), batch_size=16, num_workers=0, sampling=1.0, test_percent=0.2, valid_percent=0.2, custom_resize=False, model_arch='alexnet', pre_trained=True, unfreeze=False, type='nn_classifier', cv=True, stratified=True, num_splits=5, parameters={}, learning_rate=0.0001, epochs=10, optimizer='Adam', loss_function='CrossEntropyLoss',lr_scheduler=None, custom_nn_classifier=None, loss_function_parameters={}, optimizer_parameters={}, transformations='default', extra_transformations=None, device='auto',) Description Complete end-to-end image classification pipeline. Parameters data_directory (string, required): path to target data directory/folder. is_dicom (bollean, optional): True if images are DICOM. default=False. table (string or pandas dataframe, optional): path to label table csv or name of pandas data table. default=None. image_path_column (string, optional): name of column that has image path/image file name. default='IMAGE_PATH'. image_label_column (string, optional): name of column that has image label. default='IMAGE_LABEL'. is_path (boolean, optional): True if file_path column in table is file path. If False, this assumes that the column contains file names only and will append the data_directory to all files. default=True. mode (string, optional): mode of handling pixel values from DICOM to numpy array. Option={'RAW': raw pixel values, 'HU': converts pixel values to HU using slope and intercept, 'WIN':Applies a certain window/level to HU converted DICOM image, 'MWIN': converts DICOM image to 3 channel HU numpy array with each channel adjusted to certain window/level. default='RAW'. wl (tuple or list of tuples, optional): value of Window/Levelto be used. If mode is set to 'WIN' then wl takes the format (level, window). If mode is set to 'MWIN' then wl takes the format [(level1, window1), (level2, window2), (level3, window3)]. default=None. balance_class (bollean, optional): True to perform oversampling in the train dataset to solve class imbalance. default=False. balance_class_method (string, optional): methodology used to balance classes. Options={'upsample', 'downsample'}. default='upsample'. interaction_terms (boolean, optional): create interaction terms between different features and add them as new features to feature table. default=False. normalize (bolean/False or Tuple, optional): Normalizes all datasets by a specified mean and standard deviation. Since most of the used CNN architectures assumes 3 channel input, this follows the following format ((mean, mean, mean), (std, std, std)). default=((0,0,0), (1,1,1)). batch_size (integer, optional): Batch size for dataloader. defult=16. num_workers (integer, optional): Number of CPU workers for dataloader. default=0. sampling (float, optional): fraction of the whole dataset to be used. default=1.0. test_percent (float, optional): percentage of data for testing.default=0.2. valid_percent (float, optional): percentage of data for validation (ONLY with NN_Classifier) .default=0.2. custom_resize (integer, optional): By default, the data processor resizes the image in dataset into the size expected bu the different CNN architectures. To override this and use a custom resize, set this to desired value. default=False. transformations (list, optional): list of pytorch transformations to be applied to all datasets. By default, the images are resized, channels added up to 3 and greyscaled. default='default'. extra_transformations (list, optional): list of pytorch transformations to be extra added to train dataset specifically. default=None. model_arch (string, required): CNN model architecture that this data will be used for. Used to resize images as detailed above. default='alexnet' . pre_trained (boolean, optional): Initialize with ImageNet pretrained weights or not. default=True. unfreeze (boolean, required): Unfreeze all layers of network for future retraining. default=False. type (string, required): type of classifier. For complete list refer to settings. default='logistic_regression'. Classifier specific parameters cv (boolean, required): True for cross validation. default=True. stratified (boolean, required): True for stratified cross validation. default=True. num_splits (integer, required): Number of K-fold cross validation splits. default=5. parameters (dictionary, optional): optional parameters passed to the classifier. Please refer to sci-kit learn documentaion. NN_Classifier specific parameters learning_rate (float, required): Learning rate. default=0.0001. epochs (integer, required): training epochs. default=10. optimizer (string, required): neural network optimizer type. Please see radtorch.settings for list of approved optimizers. default='Adam'. optimizer_parameters (dictionary, optional): optional extra parameters for optimizer as per pytorch documentation. loss_function (string, required): neural network loss function. Please see radtorch.settings for list of approved loss functions. default='CrossEntropyLoss'. loss_function_parameters (dictionary, optional): optional extra parameters for loss function as per pytorch documentation. lr_scheduler (string, optional): learning rate scheduler - upcoming soon. custom_nn_classifier (pytorch model, optional): Option to use a custom made neural network classifier that will be added after feature extracted layers. default=None. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. Methods In addition to core component methods , pipeline accessible methods include: .info() Displays information of the image classification pipeline. .run() Starts the image classification pipeline training. .metrics(figure_size=(700, 350)) Displays the training metrics of the image classification pipeline. .export(output_path): Exports the pipeline to output path.","title":"Image_Classification"},{"location":"pipeline/#gan","text":"core.GAN( data_directory, table=None, is_dicom=False, is_path=True, image_path_column='IMAGE_PATH', image_label_column='IMAGE_LABEL', mode='RAW', wl=None, batch_size=16, normalize=((0,0,0),(1,1,1)), num_workers=0, label_smooth=True, sampling=1.0, transformations='default', discriminator='dcgan', generator='dcgan', generator_noise_size=100, generator_noise_type='normal', discriminator_num_features=64, generator_num_features=64, image_size=128, image_channels=1, discrinimator_optimizer='Adam', generator_optimizer='Adam', discrinimator_optimizer_param={'betas':(0.5,0.999)}, generator_optimizer_param={'betas':(0.5,0.999)}, generator_learning_rate=0.0001, discriminator_learning_rate=0.0001, epochs=10, device='auto') Description Generative Advarsarial Networks Pipeline. Parameters data_directory (string, required): path to target data directory/folder. is_dicom (bollean, optional): True if images are DICOM. default=False. table (string or pandas dataframe, optional): path to label table csv or name of pandas data table. default=None. image_path_column (string, optional): name of column that has image path/image file name. default='IMAGE_PATH'. image_label_column (string, optional): name of column that has image label. default='IMAGE_LABEL'. is_path (boolean, optional): True if file_path column in table is file path. If False, this assumes that the column contains file names only and will append the data_directory to all files. default=True. mode (string, optional): mode of handling pixel values from DICOM to numpy array. Option={'RAW': raw pixel values, 'HU': converts pixel values to HU using slope and intercept, 'WIN':Applies a certain window/level to HU converted DICOM image, 'MWIN': converts DICOM image to 3 channel HU numpy array with each channel adjusted to certain window/level. default='RAW'. wl (tuple or list of tuples, optional): value of Window/Levelto be used. If mode is set to 'WIN' then wl takes the format (level, window). If mode is set to 'MWIN' then wl takes the format [(level1, window1), (level2, window2), (level3, window3)]. default=None. batch_size (integer, optional): Batch size for dataloader. defult=16. num_workers (integer, optional): Number of CPU workers for dataloader. default=0. sampling (float, optional): fraction of the whole dataset to be used. default=1.0. transformations (list, optional): list of pytorch transformations to be applied to all datasets. By default, the images are resized, channels added up to 3 and greyscaled. default='default'. normalize (bolean/False or Tuple, optional): Normalizes all datasets by a specified mean and standard deviation. Since most of the used CNN architectures assumes 3 channel input, this follows the following format ((mean, mean, mean), (std, std, std)). default=((0,0,0),(1,1,1)). label_smooth (boolean, optioanl): by default, labels for real images as assigned to 1. If label smoothing is set to True, lables of real images will be assigned to 0.9. default=True. (Source: https://github.com/soumith/ganhacks#6-use-soft-and-noisy-labels ) epochs (integer, required): training epochs. default=10. generator (string, required): type of generator network. Options = {'dcgan', 'vanilla'}. default='dcgan' discriminator (string, required): type of discriminator network. Options = {'dcgan', 'vanilla'}. default='dcgan' image_channels (integer, required): number of output channels for discriminator input and generator output. default=1 generator_noise_type (string, optional): shape of noise to sample from. Options={'normal', 'gaussian'}. default='normal'. ( https://github.com/soumith/ganhacks#3-use-a-spherical-z ) generator_noise_size (integer, required): size of the noise sample to be generated. default=100 generator_num_features (integer, required): number of features/convolutions for generator network. default=64 image_size (integer, required): iamge size for discriminator input and generator output.default=128 discriminator_num_features (integer, required): number of features/convolutions for discriminator network.default=64 generator_optimizer (string, required): generator network optimizer type. Please see radtorch.settings for list of approved optimizers. default='Adam'. generator_optimizer_param (dictionary, optional): optional extra parameters for optimizer as per pytorch documentation. default={'betas':(0.5,0.999)} for Adam optimizer. discrinimator_optimizer (string, required): discrinimator network optimizer type. Please see radtorch.settings for list of approved optimizers. default='Adam'. discrinimator_optimizer_param (dictionary, optional): optional extra parameters for optimizer as per pytorch documentation. default={'betas':(0.5,0.999)} for Adam optimizer. generator_learning_rate (float, required): generator network learning rate. default=0.0001. discriminator_learning_rate (float, required): discrinimator network learning rate. default=0.0001. device (string, optional): device to be used for training. Options{'auto': automatic detection of device type, 'cpu': cpu, 'cuda': gpu}. default='auto'. Methods .run(self, verbose='batch', show_images=True, figure_size=(10,10)) Runs the GAN training. Parameters: verbose (string, required): amount of data output. Options {'batch': display info after each batch, 'epoch': display info after each epoch}.default='batch' show_images (boolean, optional): True to show sample of generatot generated images after each epoch. figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(10,10) .sample(figure_size=(10,10), show_labels=True) Displays a sample of real data. Parameters: figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(10,10). show_labels (boolean, optional): show labels on top of images. default=True. .info() Displays different parameters of the generative adversarial network. .metrics(figure_size=(700,350)) Displays training metrics for the GAN. Explanation of metrics: D_loss : Total loss of discriminator network on both real and fake images. G_loss : Loss of discriminator network on detecting fake images as real. d_loss_real : Loss of discriminator network on detecting real images as real. d_loss_fake : Loss of discriminator network on detecting fake images as fake. Parameters: figure_size (tuple, optional): Tuple of width and length of figure plotted. default=(700,350). Documentation Update: 5/14/2020","title":"GAN"},{"location":"start/","text":"Running a state-of-the-art DICOM image classifier can be run using the Image Classification Pipeline using the commands: from radtorch import pipeline classifier = pipeline.Image_Classification(data_directory='path to data') classifier.run() The above 3 lines of code will run an image classifier pipeline using alexnet architecture with pre-trained weights as feature extractor and a logistic regression classifier using extracted features. Playground RADTorch playground for testing is provided on Google Colab . Documentation Update: 5/14/2020","title":"Getting Started"},{"location":"start/#playground","text":"RADTorch playground for testing is provided on Google Colab . Documentation Update: 5/14/2020","title":"Playground"}]}